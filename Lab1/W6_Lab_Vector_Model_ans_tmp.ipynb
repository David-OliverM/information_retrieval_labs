{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18117881-1575-4349-95ba-9bc6b2cea328",
   "metadata": {},
   "source": [
    "# W6 Lab Exercise\n",
    "This is the lab exercise for MIS590: Information Retrieval. </br>\n",
    "In this lab, you will gain the following experience:</br>\n",
    "- Understand Vector Space Models (VSMs) for Information Retrieval.\n",
    "- Develop Practical Skills in Vector-Based Document Representation, Including TF-IDF, Word2Vec, and BERT.\n",
    "- Compare the Effectiveness of Different Term Weighting Schemes.\n",
    "- Enhance Analytical Thinking in Evaluating IR Models\n",
    "</br>\n",
    "\n",
    "**Note:** When you see a pencil icon ✏️ in this notebook, it's time for you to code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffaa68b",
   "metadata": {},
   "source": [
    "# 1. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ef730",
   "metadata": {},
   "source": [
    "## 1.1 Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bae653b-f98f-4f96-826b-e8fdc579a542",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from click->nltk) (0.4.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (2.3.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.4.0.tar.gz (23.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from gensim) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from gensim) (1.16.2)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.4.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
      "  Using cached wrapt-2.0.0-cp314-cp314-win_amd64.whl.metadata (9.0 kB)\n",
      "Downloading smart_open-7.4.1-py3-none-any.whl (63 kB)\n",
      "Using cached wrapt-2.0.0-cp314-cp314-win_amd64.whl (60 kB)\n",
      "Building wheels for collected packages: gensim\n",
      "  Building wheel for gensim (pyproject.toml): started\n",
      "  Building wheel for gensim (pyproject.toml): finished with status 'error'\n",
      "Failed to build gensim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for gensim (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [807 lines of output]\n",
      "      C:\\Users\\dmatz\\AppData\\Local\\Temp\\pip-build-env-g89i9u3f\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:289: UserWarning: Unknown distribution option: 'test_suite'\n",
      "        warnings.warn(msg)\n",
      "      C:\\Users\\dmatz\\AppData\\Local\\Temp\\pip-build-env-g89i9u3f\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:289: UserWarning: Unknown distribution option: 'tests_require'\n",
      "        warnings.warn(msg)\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-314\\gensim\n",
      "      copying gensim\\downloader.py -> build\\lib.win-amd64-cpython-314\\gensim\n",
      "      copying gensim\\interfaces.py -> build\\lib.win-amd64-cpython-314\\gensim\n",
      "      copying gensim\\matutils.py -> build\\lib.win-amd64-cpython-314\\gensim\n",
      "      copying gensim\\nosy.py -> build\\lib.win-amd64-cpython-314\\gensim\n",
      "      copying gensim\\utils.py -> build\\lib.win-amd64-cpython-314\\gensim\n",
      "      copying gensim\\__init__.py -> build\\lib.win-amd64-cpython-314\\gensim\n",
      "      creating build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\bleicorpus.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\csvcorpus.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\dictionary.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\hashdictionary.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\indexedcorpus.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\lowcorpus.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\malletcorpus.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\mmcorpus.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\opinosiscorpus.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\sharded_corpus.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\svmlightcorpus.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\textcorpus.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\ucicorpus.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\wikicorpus.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\corpora\\__init__.py -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      creating build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\atmodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\basemodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\bm25model.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\callbacks.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\coherencemodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\doc2vec.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\ensemblelda.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\fasttext.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\hdpmodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\keyedvectors.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\ldamodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\ldamulticore.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\ldaseqmodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\lda_dispatcher.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\lda_worker.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\logentropy_model.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\lsimodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\lsi_dispatcher.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\lsi_worker.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\nmf.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\normmodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\phrases.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\poincare.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\rpmodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\tfidfmodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\translation_matrix.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\word2vec.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\_fasttext_bin.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\__init__.py -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      creating build\\lib.win-amd64-cpython-314\\gensim\\parsing\n",
      "      copying gensim\\parsing\\porter.py -> build\\lib.win-amd64-cpython-314\\gensim\\parsing\n",
      "      copying gensim\\parsing\\preprocessing.py -> build\\lib.win-amd64-cpython-314\\gensim\\parsing\n",
      "      copying gensim\\parsing\\__init__.py -> build\\lib.win-amd64-cpython-314\\gensim\\parsing\n",
      "      creating build\\lib.win-amd64-cpython-314\\gensim\\scripts\n",
      "      copying gensim\\scripts\\benchmark.py -> build\\lib.win-amd64-cpython-314\\gensim\\scripts\n",
      "      copying gensim\\scripts\\glove2word2vec.py -> build\\lib.win-amd64-cpython-314\\gensim\\scripts\n",
      "      copying gensim\\scripts\\make_wiki.py -> build\\lib.win-amd64-cpython-314\\gensim\\scripts\n",
      "      copying gensim\\scripts\\make_wikicorpus.py -> build\\lib.win-amd64-cpython-314\\gensim\\scripts\n",
      "      copying gensim\\scripts\\make_wiki_online.py -> build\\lib.win-amd64-cpython-314\\gensim\\scripts\n",
      "      copying gensim\\scripts\\make_wiki_online_nodebug.py -> build\\lib.win-amd64-cpython-314\\gensim\\scripts\n",
      "      copying gensim\\scripts\\package_info.py -> build\\lib.win-amd64-cpython-314\\gensim\\scripts\n",
      "      copying gensim\\scripts\\segment_wiki.py -> build\\lib.win-amd64-cpython-314\\gensim\\scripts\n",
      "      copying gensim\\scripts\\word2vec2tensor.py -> build\\lib.win-amd64-cpython-314\\gensim\\scripts\n",
      "      copying gensim\\scripts\\word2vec_standalone.py -> build\\lib.win-amd64-cpython-314\\gensim\\scripts\n",
      "      copying gensim\\scripts\\__init__.py -> build\\lib.win-amd64-cpython-314\\gensim\\scripts\n",
      "      creating build\\lib.win-amd64-cpython-314\\gensim\\similarities\n",
      "      copying gensim\\similarities\\annoy.py -> build\\lib.win-amd64-cpython-314\\gensim\\similarities\n",
      "      copying gensim\\similarities\\docsim.py -> build\\lib.win-amd64-cpython-314\\gensim\\similarities\n",
      "      copying gensim\\similarities\\levenshtein.py -> build\\lib.win-amd64-cpython-314\\gensim\\similarities\n",
      "      copying gensim\\similarities\\nmslib.py -> build\\lib.win-amd64-cpython-314\\gensim\\similarities\n",
      "      copying gensim\\similarities\\termsim.py -> build\\lib.win-amd64-cpython-314\\gensim\\similarities\n",
      "      copying gensim\\similarities\\__init__.py -> build\\lib.win-amd64-cpython-314\\gensim\\similarities\n",
      "      creating build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\basetmtests.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\simspeed.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\simspeed2.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\svd_error.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_aggregation.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_api.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_atmodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_big.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_bm25model.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_coherencemodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_corpora.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_corpora_dictionary.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_corpora_hashdictionary.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_datatype.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_direct_confirmation.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_doc2vec.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_ensemblelda.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_fasttext.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_glove2word2vec.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_hdpmodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_indirect_confirmation.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_keyedvectors.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_ldamodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_ldaseqmodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_lda_callback.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_lee.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_logentropy_model.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_lsimodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_matutils.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_miislita.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_nmf.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_normmodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_parsing.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_phrases.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_poincare.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_probability_estimation.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_rpmodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_scripts.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_segmentation.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_sharded_corpus.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_similarities.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_similarity_metrics.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_text_analysis.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_tfidfmodel.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_tmdiff.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_translation_matrix.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_utils.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\test_word2vec.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\utils.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      copying gensim\\test\\__init__.py -> build\\lib.win-amd64-cpython-314\\gensim\\test\n",
      "      creating build\\lib.win-amd64-cpython-314\\gensim\\topic_coherence\n",
      "      copying gensim\\topic_coherence\\aggregation.py -> build\\lib.win-amd64-cpython-314\\gensim\\topic_coherence\n",
      "      copying gensim\\topic_coherence\\direct_confirmation_measure.py -> build\\lib.win-amd64-cpython-314\\gensim\\topic_coherence\n",
      "      copying gensim\\topic_coherence\\indirect_confirmation_measure.py -> build\\lib.win-amd64-cpython-314\\gensim\\topic_coherence\n",
      "      copying gensim\\topic_coherence\\probability_estimation.py -> build\\lib.win-amd64-cpython-314\\gensim\\topic_coherence\n",
      "      copying gensim\\topic_coherence\\segmentation.py -> build\\lib.win-amd64-cpython-314\\gensim\\topic_coherence\n",
      "      copying gensim\\topic_coherence\\text_analysis.py -> build\\lib.win-amd64-cpython-314\\gensim\\topic_coherence\n",
      "      copying gensim\\topic_coherence\\__init__.py -> build\\lib.win-amd64-cpython-314\\gensim\\topic_coherence\n",
      "      running egg_info\n",
      "      writing gensim.egg-info\\PKG-INFO\n",
      "      writing dependency_links to gensim.egg-info\\dependency_links.txt\n",
      "      writing requirements to gensim.egg-info\\requires.txt\n",
      "      writing top-level names to gensim.egg-info\\top_level.txt\n",
      "      reading manifest file 'gensim.egg-info\\SOURCES.txt'\n",
      "      reading manifest template 'MANIFEST.in'\n",
      "      adding license file 'COPYING'\n",
      "      writing manifest file 'gensim.egg-info\\SOURCES.txt'\n",
      "      C:\\Users\\dmatz\\AppData\\Local\\Temp\\pip-build-env-g89i9u3f\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'gensim.corpora' is absent from the `packages` configuration.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              ############################\n",
      "              # Package would be ignored #\n",
      "              ############################\n",
      "              Python recognizes 'gensim.corpora' as an importable package[^1],\n",
      "              but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "              This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "              package, please make sure that 'gensim.corpora' is explicitly added\n",
      "              to the `packages` configuration field.\n",
      "      \n",
      "              Alternatively, you can also rely on setuptools' discovery methods\n",
      "              (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "              instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "              You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "              If you don't want 'gensim.corpora' to be distributed and are\n",
      "              already explicitly excluding 'gensim.corpora' via\n",
      "              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "              you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "              combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "              You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "              [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                    even if it does not contain any `.py` files.\n",
      "                    On the other hand, currently there is no concept of package data\n",
      "                    directory, all directories are treated like packages.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        check.warn(importable)\n",
      "      C:\\Users\\dmatz\\AppData\\Local\\Temp\\pip-build-env-g89i9u3f\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'gensim.models' is absent from the `packages` configuration.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              ############################\n",
      "              # Package would be ignored #\n",
      "              ############################\n",
      "              Python recognizes 'gensim.models' as an importable package[^1],\n",
      "              but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "              This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "              package, please make sure that 'gensim.models' is explicitly added\n",
      "              to the `packages` configuration field.\n",
      "      \n",
      "              Alternatively, you can also rely on setuptools' discovery methods\n",
      "              (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "              instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "              You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "              If you don't want 'gensim.models' to be distributed and are\n",
      "              already explicitly excluding 'gensim.models' via\n",
      "              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "              you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "              combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "              You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "              [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                    even if it does not contain any `.py` files.\n",
      "                    On the other hand, currently there is no concept of package data\n",
      "                    directory, all directories are treated like packages.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        check.warn(importable)\n",
      "      C:\\Users\\dmatz\\AppData\\Local\\Temp\\pip-build-env-g89i9u3f\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'gensim.similarities' is absent from the `packages` configuration.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              ############################\n",
      "              # Package would be ignored #\n",
      "              ############################\n",
      "              Python recognizes 'gensim.similarities' as an importable package[^1],\n",
      "              but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "              This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "              package, please make sure that 'gensim.similarities' is explicitly added\n",
      "              to the `packages` configuration field.\n",
      "      \n",
      "              Alternatively, you can also rely on setuptools' discovery methods\n",
      "              (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "              instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "              You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "              If you don't want 'gensim.similarities' to be distributed and are\n",
      "              already explicitly excluding 'gensim.similarities' via\n",
      "              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "              you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "              combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "              You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "              [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                    even if it does not contain any `.py` files.\n",
      "                    On the other hand, currently there is no concept of package data\n",
      "                    directory, all directories are treated like packages.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        check.warn(importable)\n",
      "      C:\\Users\\dmatz\\AppData\\Local\\Temp\\pip-build-env-g89i9u3f\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'gensim.test.test_data' is absent from the `packages` configuration.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              ############################\n",
      "              # Package would be ignored #\n",
      "              ############################\n",
      "              Python recognizes 'gensim.test.test_data' as an importable package[^1],\n",
      "              but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "              This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "              package, please make sure that 'gensim.test.test_data' is explicitly added\n",
      "              to the `packages` configuration field.\n",
      "      \n",
      "              Alternatively, you can also rely on setuptools' discovery methods\n",
      "              (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "              instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "              You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "              If you don't want 'gensim.test.test_data' to be distributed and are\n",
      "              already explicitly excluding 'gensim.test.test_data' via\n",
      "              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "              you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "              combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "              You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "              [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                    even if it does not contain any `.py` files.\n",
      "                    On the other hand, currently there is no concept of package data\n",
      "                    directory, all directories are treated like packages.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        check.warn(importable)\n",
      "      C:\\Users\\dmatz\\AppData\\Local\\Temp\\pip-build-env-g89i9u3f\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'gensim.test.test_data.DTM' is absent from the `packages` configuration.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              ############################\n",
      "              # Package would be ignored #\n",
      "              ############################\n",
      "              Python recognizes 'gensim.test.test_data.DTM' as an importable package[^1],\n",
      "              but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "              This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "              package, please make sure that 'gensim.test.test_data.DTM' is explicitly added\n",
      "              to the `packages` configuration field.\n",
      "      \n",
      "              Alternatively, you can also rely on setuptools' discovery methods\n",
      "              (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "              instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "              You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "              If you don't want 'gensim.test.test_data.DTM' to be distributed and are\n",
      "              already explicitly excluding 'gensim.test.test_data.DTM' via\n",
      "              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "              you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "              combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "              You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "              [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                    even if it does not contain any `.py` files.\n",
      "                    On the other hand, currently there is no concept of package data\n",
      "                    directory, all directories are treated like packages.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        check.warn(importable)\n",
      "      C:\\Users\\dmatz\\AppData\\Local\\Temp\\pip-build-env-g89i9u3f\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'gensim.test.test_data.PathLineSentences' is absent from the `packages` configuration.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              ############################\n",
      "              # Package would be ignored #\n",
      "              ############################\n",
      "              Python recognizes 'gensim.test.test_data.PathLineSentences' as an importable package[^1],\n",
      "              but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "              This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "              package, please make sure that 'gensim.test.test_data.PathLineSentences' is explicitly added\n",
      "              to the `packages` configuration field.\n",
      "      \n",
      "              Alternatively, you can also rely on setuptools' discovery methods\n",
      "              (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "              instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "              You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "              If you don't want 'gensim.test.test_data.PathLineSentences' to be distributed and are\n",
      "              already explicitly excluding 'gensim.test.test_data.PathLineSentences' via\n",
      "              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "              you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "              combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "              You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "              [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                    even if it does not contain any `.py` files.\n",
      "                    On the other hand, currently there is no concept of package data\n",
      "                    directory, all directories are treated like packages.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        check.warn(importable)\n",
      "      C:\\Users\\dmatz\\AppData\\Local\\Temp\\pip-build-env-g89i9u3f\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'gensim.test.test_data.old_d2v_models' is absent from the `packages` configuration.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              ############################\n",
      "              # Package would be ignored #\n",
      "              ############################\n",
      "              Python recognizes 'gensim.test.test_data.old_d2v_models' as an importable package[^1],\n",
      "              but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "              This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "              package, please make sure that 'gensim.test.test_data.old_d2v_models' is explicitly added\n",
      "              to the `packages` configuration field.\n",
      "      \n",
      "              Alternatively, you can also rely on setuptools' discovery methods\n",
      "              (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "              instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "              You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "              If you don't want 'gensim.test.test_data.old_d2v_models' to be distributed and are\n",
      "              already explicitly excluding 'gensim.test.test_data.old_d2v_models' via\n",
      "              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "              you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "              combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "              You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "              [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                    even if it does not contain any `.py` files.\n",
      "                    On the other hand, currently there is no concept of package data\n",
      "                    directory, all directories are treated like packages.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        check.warn(importable)\n",
      "      C:\\Users\\dmatz\\AppData\\Local\\Temp\\pip-build-env-g89i9u3f\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'gensim.test.test_data.old_w2v_models' is absent from the `packages` configuration.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              ############################\n",
      "              # Package would be ignored #\n",
      "              ############################\n",
      "              Python recognizes 'gensim.test.test_data.old_w2v_models' as an importable package[^1],\n",
      "              but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "              This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "              package, please make sure that 'gensim.test.test_data.old_w2v_models' is explicitly added\n",
      "              to the `packages` configuration field.\n",
      "      \n",
      "              Alternatively, you can also rely on setuptools' discovery methods\n",
      "              (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "              instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "              You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "              If you don't want 'gensim.test.test_data.old_w2v_models' to be distributed and are\n",
      "              already explicitly excluding 'gensim.test.test_data.old_w2v_models' via\n",
      "              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "              you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "              combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "              You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "              [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                    even if it does not contain any `.py` files.\n",
      "                    On the other hand, currently there is no concept of package data\n",
      "                    directory, all directories are treated like packages.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        check.warn(importable)\n",
      "      copying gensim\\_matutils.c -> build\\lib.win-amd64-cpython-314\\gensim\n",
      "      copying gensim\\_matutils.pyx -> build\\lib.win-amd64-cpython-314\\gensim\n",
      "      copying gensim\\corpora\\_mmreader.c -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\models\\doc2vec_corpusfile.cpp -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\doc2vec_inner.cpp -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\fasttext_corpusfile.cpp -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\fasttext_inner.c -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\nmf_pgd.c -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\word2vec_corpusfile.cpp -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\word2vec_inner.c -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\similarities\\fastss.c -> build\\lib.win-amd64-cpython-314\\gensim\\similarities\n",
      "      copying gensim\\corpora\\_mmreader.pyx -> build\\lib.win-amd64-cpython-314\\gensim\\corpora\n",
      "      copying gensim\\models\\doc2vec_corpusfile.pyx -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\doc2vec_inner.pxd -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\doc2vec_inner.pyx -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\fast_line_sentence.h -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\fasttext_corpusfile.pyx -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\fasttext_inner.pxd -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\fasttext_inner.pyx -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\nmf_pgd.pyx -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\stdint_wrapper.h -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\voidptr.h -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\word2vec_corpusfile.pxd -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\word2vec_corpusfile.pyx -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\word2vec_inner.pxd -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      copying gensim\\models\\word2vec_inner.pyx -> build\\lib.win-amd64-cpython-314\\gensim\\models\n",
      "      creating build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\OPUS_en_it_europarl_train_one2ten.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\alldata-id-10.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\atmodel_3_0_1_model -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\atmodel_3_0_1_model.expElogbeta.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\atmodel_3_0_1_model.id2word -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\atmodel_3_0_1_model.state -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\bgwiki-latest-pages-articles-shortened.xml.bz2 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\compatible-hash-true.model -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\cp852_fasttext.bin -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\crime-and-punishment.bin -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\crime-and-punishment.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\crime-and-punishment.vec -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\d2v-lee-v0.13.0 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\doc2vec_old -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\doc2vec_old_sep -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\doc2vec_old_sep.syn0_lockf.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\doc2vec_old_sep.syn1neg.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\dtm_test.dict -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\dtm_test.mm -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\ensemblelda -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\enwiki-latest-pages-articles1.xml-p000000010p000030302-shortened.bz2 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\enwiki-table-markup.xml.bz2 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\euclidean_vectors.bin -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\fasttext_old -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\fasttext_old_sep -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\fasttext_old_sep.syn0_lockf.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\fasttext_old_sep.syn1neg.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\fb-ngrams.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\ft_kv_3.6.0.model.gz -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\ft_model_2.3.0 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\head500.noblanks.cor -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\head500.noblanks.cor.bz2 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\head500.noblanks.cor_tfidf.model -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\head500.noblanks.cor_wordids.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\high_precision.kv.bin -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\high_precision.kv.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\large_tag_doc_10_iter50 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\lda_3_0_1_model -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\lda_3_0_1_model.expElogbeta.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\lda_3_0_1_model.id2word -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\lda_3_0_1_model.state -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\ldamodel_python_2_7 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\ldamodel_python_2_7.expElogbeta.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\ldamodel_python_2_7.id2word -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\ldamodel_python_2_7.state -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\ldamodel_python_3_5 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\ldamodel_python_3_5.expElogbeta.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\ldamodel_python_3_5.id2word -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\ldamodel_python_3_5.state -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\ldavowpalwabbit.dict.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\ldavowpalwabbit.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\lee.cor -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\lee_background.cor -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\lee_fasttext -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\lee_fasttext.bin -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\lee_fasttext.vec -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\lee_fasttext_new.bin -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\miIslita.cor -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\mini_newsgroup -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\model-from-gensim-3.8.0.w2v -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\nmf_model -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\non_ascii_fasttext.bin -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\old_keyedvectors_320.dat -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\pang_lee_polarity.cor -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\pang_lee_polarity_fasttext.bin -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\pang_lee_polarity_fasttext.vec -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\para2para_text1.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\para2para_text2.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\phraser-3.6.0.model -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\phraser-no-common-terms.pkl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\phraser-no-scoring.pkl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\phraser-scoring-str.pkl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\phrases-3.6.0.model -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\phrases-no-common-terms.pkl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\phrases-no-scoring.pkl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\phrases-scoring-str.pkl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\poincare_cp852.tsv -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\poincare_hypernyms.tsv -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\poincare_hypernyms_large.tsv -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\poincare_test_3.4.0 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\poincare_utf8.tsv -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\poincare_vectors.bin -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\pre_0_13_2_model -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\pre_0_13_2_model.state -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\pretrained.vec -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\questions-words.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\reproduce.dat -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\reproduce.dat.gz -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\similarities0-1.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\simlex999.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\small_tag_doc_5_iter50 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\test_corpus_ok.mm -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\test_corpus_small.mm -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\test_glove.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\test_mmcorpus_corrupt.mm -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\test_mmcorpus_no_index.mm -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\test_mmcorpus_no_index.mm.bz2 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\test_mmcorpus_no_index.mm.gz -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\test_mmcorpus_overflow.mm -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\test_mmcorpus_with_index.mm -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\test_mmcorpus_with_index.mm.index -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.blei -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.blei.index -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.blei.vocab -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.low -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.low.index -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.mallet -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.mallet.index -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.mm -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.mm.index -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.svmlight -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.svmlight.index -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.uci -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.uci.index -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.uci.vocab -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\testcorpus.xml.bz2 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\tfidf_model.tst -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\tfidf_model.tst.bz2 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\tfidf_model_3_2.tst -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\toy-data.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\toy-model-pretrained.bin -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\toy-model.bin -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\toy-model.vec -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\varembed_lee_subcorpus.cor -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\varembed_morfessor.bin -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\varembed_vectors.pkl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\w2v-lee-v0.12.0 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\w2v_keyedvectors_load_test.modeldata -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\w2v_keyedvectors_load_test.vocab -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_3.3 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_old -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_old_sep -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_old_sep.syn0_lockf.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_old_sep.syn1neg.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_c -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_py2 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_py3 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_py3_4 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2.neg_labels.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2.syn0.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2.syn0_lockf.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2.syn1neg.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3.neg_labels.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3.syn0.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3.syn0_lockf.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3.syn1neg.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4.neg_labels.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4.syn0.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4.syn0_lockf.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4.syn1neg.npy -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      copying gensim\\test\\test_data\\wordsim353.tsv -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\n",
      "      creating build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\DTM\n",
      "      copying gensim\\test\\test_data\\DTM\\ldaseq_3_0_1_model -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\DTM\n",
      "      copying gensim\\test\\test_data\\DTM\\sstats_test.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\DTM\n",
      "      creating build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\PathLineSentences\n",
      "      copying gensim\\test\\test_data\\PathLineSentences\\1.txt -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\PathLineSentences\n",
      "      copying gensim\\test\\test_data\\PathLineSentences\\2.txt.bz2 -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\PathLineSentences\n",
      "      creating build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.1.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.2.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.3.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.4.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.1.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.2.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.3.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.4.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_1.0.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_1.0.1.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_2.0.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_2.1.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_2.2.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_2.3.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.0.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.1.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.2.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.3.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.4.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_d2v_models\n",
      "      creating build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.1.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.2.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.3.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.4.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.1.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.2.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.3.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.4.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_1.0.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_1.0.1.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_2.0.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_2.1.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_2.2.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_2.3.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.0.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.1.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.2.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.3.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.4.0.mdl -> build\\lib.win-amd64-cpython-314\\gensim\\test\\test_data\\old_w2v_models\n",
      "      running build_ext\n",
      "      building 'gensim.models.word2vec_inner' extension\n",
      "      creating build\\temp.win-amd64-cpython-314\\Release\\gensim\\models\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.44.35207\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Python314\\include -IC:\\Python314\\Include -IC:\\Users\\dmatz\\AppData\\Local\\Temp\\pip-build-env-g89i9u3f\\overlay\\Lib\\site-packages\\numpy\\_core\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.44.35207\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.26100.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.26100.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.26100.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.26100.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.26100.0\\\\cppwinrt\" /Tcgensim/models/word2vec_inner.c /Fobuild\\temp.win-amd64-cpython-314\\Release\\gensim\\models\\word2vec_inner.obj\n",
      "      word2vec_inner.c\n",
      "      gensim/models/word2vec_inner.c(853): warning C4996: 'Py_UNICODE': deprecated in 3.13\n",
      "      gensim/models/word2vec_inner.c(854): warning C4996: 'Py_UNICODE': deprecated in 3.13\n",
      "      gensim/models/word2vec_inner.c(1225): error C2061: Syntaxfehler: Bezeichner \"__pyx_t_float_complex\"\n",
      "      gensim/models/word2vec_inner.c(1225): error C2059: Syntaxfehler: \";\"\n",
      "      gensim/models/word2vec_inner.c(1230): error C2061: Syntaxfehler: Bezeichner \"__pyx_t_float_complex_from_parts\"\n",
      "      gensim/models/word2vec_inner.c(1230): error C2059: Syntaxfehler: \";\"\n",
      "      gensim/models/word2vec_inner.c(1230): error C2059: Syntaxfehler: \"<parameter-list>\"\n",
      "      gensim/models/word2vec_inner.c(1237): error C2061: Syntaxfehler: Bezeichner \"__pyx_t_double_complex\"\n",
      "      gensim/models/word2vec_inner.c(1237): error C2059: Syntaxfehler: \";\"\n",
      "      gensim/models/word2vec_inner.c(1242): error C2061: Syntaxfehler: Bezeichner \"__pyx_t_double_complex_from_parts\"\n",
      "      gensim/models/word2vec_inner.c(1242): error C2059: Syntaxfehler: \";\"\n",
      "      gensim/models/word2vec_inner.c(1242): error C2059: Syntaxfehler: \"<parameter-list>\"\n",
      "      gensim/models/word2vec_inner.c(2899): warning C4244: \"=\": Konvertierung von \"unsigned __int64\" in \"__pyx_t_5numpy_uint32_t\", m”glicher Datenverlust\n",
      "      gensim/models/word2vec_inner.c(3889): warning C4244: \"=\": Konvertierung von \"unsigned __int64\" in \"__pyx_t_5numpy_uint32_t\", m”glicher Datenverlust\n",
      "      gensim/models/word2vec_inner.c(4408): warning C4244: \"=\": Konvertierung von \"Py_ssize_t\" in \"__pyx_t_5numpy_uint32_t\", m”glicher Datenverlust\n",
      "      gensim/models/word2vec_inner.c(8725): warning C4244: \"=\": Konvertierung von \"long\" in \"__pyx_t_6gensim_6models_14word2vec_inner_REAL_t\", m”glicher Datenverlust\n",
      "      gensim/models/word2vec_inner.c(9364): error C2039: \"subarray\" ist kein Member von \"_PyArray_Descr\".\n",
      "      C:\\Users\\dmatz\\AppData\\Local\\Temp\\pip-build-env-g89i9u3f\\overlay\\Lib\\site-packages\\numpy\\_core\\include\\numpy\\ndarraytypes.h(622): note: Siehe Deklaration von \"_PyArray_Descr\"\n",
      "      gensim/models/word2vec_inner.c(9364): error C2198: \"void Py_INCREF(PyObject *)\": Nicht gen\\x81gend Argumente f\\x81r Aufruf.\n",
      "      gensim/models/word2vec_inner.c(9365): error C2039: \"subarray\" ist kein Member von \"_PyArray_Descr\".\n",
      "      C:\\Users\\dmatz\\AppData\\Local\\Temp\\pip-build-env-g89i9u3f\\overlay\\Lib\\site-packages\\numpy\\_core\\include\\numpy\\ndarraytypes.h(622): note: Siehe Deklaration von \"_PyArray_Descr\"\n",
      "      gensim/models/word2vec_inner.c(10747): error C2039: \"ma_version_tag\" ist kein Member von \"PyDictObject\".\n",
      "      C:\\Python314\\include\\cpython/dictobject.h(11): note: Siehe Deklaration von \"PyDictObject\"\n",
      "      gensim/models/word2vec_inner.c(10762): error C2039: \"ma_version_tag\" ist kein Member von \"PyDictObject\".\n",
      "      C:\\Python314\\include\\cpython/dictobject.h(11): note: Siehe Deklaration von \"PyDictObject\"\n",
      "      gensim/models/word2vec_inner.c(10780): error C2039: \"ma_version_tag\" ist kein Member von \"PyDictObject\".\n",
      "      C:\\Python314\\include\\cpython/dictobject.h(11): note: Siehe Deklaration von \"PyDictObject\"\n",
      "      gensim/models/word2vec_inner.c(10798): error C2039: \"ma_version_tag\" ist kein Member von \"PyDictObject\".\n",
      "      C:\\Python314\\include\\cpython/dictobject.h(11): note: Siehe Deklaration von \"PyDictObject\"\n",
      "      gensim/models/word2vec_inner.c(10816): error C2039: \"ma_version_tag\" ist kein Member von \"PyDictObject\".\n",
      "      C:\\Python314\\include\\cpython/dictobject.h(11): note: Siehe Deklaration von \"PyDictObject\"\n",
      "      gensim/models/word2vec_inner.c(10834): error C2039: \"ma_version_tag\" ist kein Member von \"PyDictObject\".\n",
      "      C:\\Python314\\include\\cpython/dictobject.h(11): note: Siehe Deklaration von \"PyDictObject\"\n",
      "      gensim/models/word2vec_inner.c(10852): error C2039: \"ma_version_tag\" ist kein Member von \"PyDictObject\".\n",
      "      C:\\Python314\\include\\cpython/dictobject.h(11): note: Siehe Deklaration von \"PyDictObject\"\n",
      "      gensim/models/word2vec_inner.c(10947): error C2039: \"ma_version_tag\" ist kein Member von \"PyDictObject\".\n",
      "      C:\\Python314\\include\\cpython/dictobject.h(11): note: Siehe Deklaration von \"PyDictObject\"\n",
      "      gensim/models/word2vec_inner.c(11068): error C2039: \"ob_digit\" ist kein Member von \"_longobject\".\n",
      "      C:\\Python314\\include\\cpython/longintrepr.h(98): note: Siehe Deklaration von \"_longobject\"\n",
      "      gensim/models/word2vec_inner.c(11918): error C2039: \"curexc_traceback\" ist kein Member von \"_ts\".\n",
      "      C:\\Python314\\include\\cpython/pystate.h(66): note: Siehe Deklaration von \"_ts\"\n",
      "      gensim/models/word2vec_inner.c(11921): error C2039: \"curexc_traceback\" ist kein Member von \"_ts\".\n",
      "      C:\\Python314\\include\\cpython/pystate.h(66): note: Siehe Deklaration von \"_ts\"\n",
      "      gensim/models/word2vec_inner.c(12062): error C2039: \"ma_version_tag\" ist kein Member von \"PyDictObject\".\n",
      "      C:\\Python314\\include\\cpython/dictobject.h(11): note: Siehe Deklaration von \"PyDictObject\"\n",
      "      gensim/models/word2vec_inner.c(12062): warning C4033: \"__Pyx_get_tp_dict_version\" muss einen Wert zur\\x81ckgeben\n",
      "      gensim/models/word2vec_inner.c(12074): error C2039: \"ma_version_tag\" ist kein Member von \"PyDictObject\".\n",
      "      C:\\Python314\\include\\cpython/dictobject.h(11): note: Siehe Deklaration von \"PyDictObject\"\n",
      "      gensim/models/word2vec_inner.c(12074): warning C4033: \"__Pyx_get_object_dict_version\" muss einen Wert zur\\x81ckgeben\n",
      "      gensim/models/word2vec_inner.c(12078): error C2039: \"ma_version_tag\" ist kein Member von \"PyDictObject\".\n",
      "      C:\\Python314\\include\\cpython/dictobject.h(11): note: Siehe Deklaration von \"PyDictObject\"\n",
      "      gensim/models/word2vec_inner.c(12095): error C2039: \"ma_version_tag\" ist kein Member von \"PyDictObject\".\n",
      "      C:\\Python314\\include\\cpython/dictobject.h(11): note: Siehe Deklaration von \"PyDictObject\"\n",
      "      gensim/models/word2vec_inner.c(12176): error C2039: \"ma_version_tag\" ist kein Member von \"PyDictObject\".\n",
      "      C:\\Python314\\include\\cpython/dictobject.h(11): note: Siehe Deklaration von \"PyDictObject\"\n",
      "      gensim/models/word2vec_inner.c(12446): error C2061: Syntaxfehler: Bezeichner \"__pyx_t_float_complex_from_parts\"\n",
      "      gensim/models/word2vec_inner.c(12446): error C2059: Syntaxfehler: \";\"\n",
      "      gensim/models/word2vec_inner.c(12446): error C2059: Syntaxfehler: \"<parameter-list>\"\n",
      "      gensim/models/word2vec_inner.c(12600): error C2061: Syntaxfehler: Bezeichner \"__pyx_t_double_complex_from_parts\"\n",
      "      gensim/models/word2vec_inner.c(12600): error C2059: Syntaxfehler: \";\"\n",
      "      gensim/models/word2vec_inner.c(12600): error C2059: Syntaxfehler: \"<parameter-list>\"\n",
      "      gensim/models/word2vec_inner.c(12812): error C2039: \"ob_digit\" ist kein Member von \"_longobject\".\n",
      "      C:\\Python314\\include\\cpython/longintrepr.h(98): note: Siehe Deklaration von \"_longobject\"\n",
      "      gensim/models/word2vec_inner.c(12867): error C2039: \"ob_digit\" ist kein Member von \"_longobject\".\n",
      "      C:\\Python314\\include\\cpython/longintrepr.h(98): note: Siehe Deklaration von \"_longobject\"\n",
      "      gensim/models/word2vec_inner.c(12953): error C2198: \"int _PyLong_AsByteArray(PyLongObject *,unsigned char *,size_t,int,int,int)\": Nicht gen\\x81gend Argumente f\\x81r Aufruf.\n",
      "      gensim/models/word2vec_inner.c(13046): error C2039: \"ob_digit\" ist kein Member von \"_longobject\".\n",
      "      C:\\Python314\\include\\cpython/longintrepr.h(98): note: Siehe Deklaration von \"_longobject\"\n",
      "      gensim/models/word2vec_inner.c(13101): error C2039: \"ob_digit\" ist kein Member von \"_longobject\".\n",
      "      C:\\Python314\\include\\cpython/longintrepr.h(98): note: Siehe Deklaration von \"_longobject\"\n",
      "      gensim/models/word2vec_inner.c(13187): error C2198: \"int _PyLong_AsByteArray(PyLongObject *,unsigned char *,size_t,int,int,int)\": Nicht gen\\x81gend Argumente f\\x81r Aufruf.\n",
      "      gensim/models/word2vec_inner.c(13242): error C2039: \"ob_digit\" ist kein Member von \"_longobject\".\n",
      "      C:\\Python314\\include\\cpython/longintrepr.h(98): note: Siehe Deklaration von \"_longobject\"\n",
      "      gensim/models/word2vec_inner.c(13297): error C2039: \"ob_digit\" ist kein Member von \"_longobject\".\n",
      "      C:\\Python314\\include\\cpython/longintrepr.h(98): note: Siehe Deklaration von \"_longobject\"\n",
      "      gensim/models/word2vec_inner.c(13383): error C2198: \"int _PyLong_AsByteArray(PyLongObject *,unsigned char *,size_t,int,int,int)\": Nicht gen\\x81gend Argumente f\\x81r Aufruf.\n",
      "      gensim/models/word2vec_inner.c(13438): error C2039: \"ob_digit\" ist kein Member von \"_longobject\".\n",
      "      C:\\Python314\\include\\cpython/longintrepr.h(98): note: Siehe Deklaration von \"_longobject\"\n",
      "      gensim/models/word2vec_inner.c(13493): error C2039: \"ob_digit\" ist kein Member von \"_longobject\".\n",
      "      C:\\Python314\\include\\cpython/longintrepr.h(98): note: Siehe Deklaration von \"_longobject\"\n",
      "      gensim/models/word2vec_inner.c(13579): error C2198: \"int _PyLong_AsByteArray(PyLongObject *,unsigned char *,size_t,int,int,int)\": Nicht gen\\x81gend Argumente f\\x81r Aufruf.\n",
      "      gensim/models/word2vec_inner.c(13634): error C2039: \"ob_digit\" ist kein Member von \"_longobject\".\n",
      "      C:\\Python314\\include\\cpython/longintrepr.h(98): note: Siehe Deklaration von \"_longobject\"\n",
      "      gensim/models/word2vec_inner.c(13689): error C2039: \"ob_digit\" ist kein Member von \"_longobject\".\n",
      "      C:\\Python314\\include\\cpython/longintrepr.h(98): note: Siehe Deklaration von \"_longobject\"\n",
      "      gensim/models/word2vec_inner.c(13775): error C2198: \"int _PyLong_AsByteArray(PyLongObject *,unsigned char *,size_t,int,int,int)\": Nicht gen\\x81gend Argumente f\\x81r Aufruf.\n",
      "      gensim/models/word2vec_inner.c(14208): error C2039: \"ob_digit\" ist kein Member von \"_longobject\".\n",
      "      C:\\Python314\\include\\cpython/longintrepr.h(98): note: Siehe Deklaration von \"_longobject\"\n",
      "      error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.44.35207\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for gensim\n",
      "error: failed-wheel-build-for-install\n",
      "\n",
      "× Failed to build installable wheels for some pyproject.toml based projects\n",
      "╰─> gensim\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement string (from versions: none)\n",
      "ERROR: No matching distribution found for string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\python314\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\python314\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python314\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python314\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python314\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from scikit-learn) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dmatz\\appdata\\roaming\\python\\python314\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary packages\n",
    "!pip install nltk\n",
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install gensim\n",
    "!pip install string\n",
    "!pip install transformers\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53e49647",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2b1d68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecece4d5",
   "metadata": {},
   "source": [
    "## 1.2 Input: Query & Document Collections (Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ce3b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"sleep deprivation\"\n",
    "\n",
    "corpus = [\n",
    "    \"Sleepless nights in the lab have become my new normal. I tried to fix the experiment setup, but the apparatus seems to have a mind of its own. My advisor says results are just around the corner, but the corner keeps moving. Coffee is my only true companion these days.\",\n",
    "    \"I thought grad school would be intellectually stimulating, but it's mostly paperwork and waiting for emails. The departmental printer jammed again, and now I'm late for a meeting. The cafeteria ran out of the good snacks, so I'm surviving on vending machine chips. Sleep has become a luxury I can no longer afford.\",\n",
    "    \"Writing the dissertation feels like climbing an endless mountain. Every time I finish a chapter, my supervisor suggests new revisions. The impostor syndrome is real, and I wonder if they made a mistake accepting me. Maybe I should have gone to clown college instead. I am utterly deprived of any semblance of a normal life.\",\n",
    "    \"My research data got corrupted, and now I have to start over. The lab mouse escaped, and we spent hours trying to find it. The grant proposal deadline is tomorrow, and the online submission portal is down. At least my pet cactus hasn't died yet.\",\n",
    "    \"The group meeting turned into a three-hour debate over font choices for the presentation. I'm pretty sure my colleague is stealing my lunch from the fridge. The photocopier is out to get me; it never works when I'm in a hurry. Is there a PhD in napping? Because I'd ace that.\",\n",
    "    \"I haven't seen the sun in days due to endless coding sessions. The simulation keeps crashing, and Stack Overflow doesn't have the answers. My roommate thinks I'm a ghost haunting the apartment. Instant noodles have become my primary food group.\",\n",
    "    \"Attending conferences sounded fun until I realized they involve a lot of awkward networking. I accidentally spilled coffee on a famous professor's shoes. My poster fell down twice during the session. Next time, I'll just send a cardboard cutout of myself.\",\n",
    "    \"The university gym membership was supposed to keep me healthy, but I've only used it once. I tried to attend a yoga class after staying up late for a deadline, but I fell asleep during the meditation. Maybe instead of the gym, my bed is more essential for keeping me healthy.\",\n",
    "    \"My teaching assistantship involves grading endless stacks of exams. Students keep emailing me for extensions with creative excuses. One claimed their dog sleeps on the laptop so they cannot use it for the exam. I was deprived of excuses for not completing my dissertation draft, and I might have got some good ones.\",\n",
    "    \"Group projects are the worst when you're the only one doing the work. My team members are as elusive as Bigfoot. The project is due next week, and I haven't heard from them. Perhaps I should just write a paper on the sociological implications of group work avoidance.\"\n",
    "]\n",
    "\n",
    "# Binary labels for the documents' relevancy to the query\n",
    "# Relevant ones: 1, 2, 5, 6, 8\n",
    "corpus_relevancy_label = [1, 1, 0, 0, 1, 1, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c614242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: sleep deprivation\n",
      "\n",
      "Document 1:\n",
      "Sleepless nights in the lab have become my new normal. I tried to fix the experiment setup, but the apparatus seems to have a mind of its own. My advisor says results are just around the corner, but the corner keeps moving. Coffee is my only true companion these days.\n",
      "\n",
      "Document 2:\n",
      "I thought grad school would be intellectually stimulating, but it's mostly paperwork and waiting for emails. The departmental printer jammed again, and now I'm late for a meeting. The cafeteria ran out of the good snacks, so I'm surviving on vending machine chips. Sleep has become a luxury I can no longer afford.\n",
      "\n",
      "Document 3:\n",
      "Writing the dissertation feels like climbing an endless mountain. Every time I finish a chapter, my supervisor suggests new revisions. The impostor syndrome is real, and I wonder if they made a mistake accepting me. Maybe I should have gone to clown college instead. I am utterly deprived of any semblance of a normal life.\n",
      "\n",
      "Document 4:\n",
      "My research data got corrupted, and now I have to start over. The lab mouse escaped, and we spent hours trying to find it. The grant proposal deadline is tomorrow, and the online submission portal is down. At least my pet cactus hasn't died yet.\n",
      "\n",
      "Document 5:\n",
      "The group meeting turned into a three-hour debate over font choices for the presentation. I'm pretty sure my colleague is stealing my lunch from the fridge. The photocopier is out to get me; it never works when I'm in a hurry. Is there a PhD in napping? Because I'd ace that.\n",
      "\n",
      "Document 6:\n",
      "I haven't seen the sun in days due to endless coding sessions. The simulation keeps crashing, and Stack Overflow doesn't have the answers. My roommate thinks I'm a ghost haunting the apartment. Instant noodles have become my primary food group.\n",
      "\n",
      "Document 7:\n",
      "Attending conferences sounded fun until I realized they involve a lot of awkward networking. I accidentally spilled coffee on a famous professor's shoes. My poster fell down twice during the session. Next time, I'll just send a cardboard cutout of myself.\n",
      "\n",
      "Document 8:\n",
      "The university gym membership was supposed to keep me healthy, but I've only used it once. I tried to attend a yoga class after staying up late for a deadline, but I fell asleep during the meditation. Maybe instead of the gym, my bed is more essential for keeping me healthy.\n",
      "\n",
      "Document 9:\n",
      "My teaching assistantship involves grading endless stacks of exams. Students keep emailing me for extensions with creative excuses. One claimed their dog sleeps on the laptop so they cannot use it for the exam. I was deprived of excuses for not completing my dissertation draft, and I might have got some good ones.\n",
      "\n",
      "Document 10:\n",
      "Group projects are the worst when you're the only one doing the work. My team members are as elusive as Bigfoot. The project is due next week, and I haven't heard from them. Perhaps I should just write a paper on the sociological implications of group work avoidance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {query}\\n\")\n",
    "for idx, doc in enumerate(corpus):\n",
    "    print(f\"Document {idx+1}:\\n{doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16ff55-2eb7-41d7-839d-73bb5272d04f",
   "metadata": {},
   "source": [
    "# 2. Vector Space Model: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3088e0",
   "metadata": {},
   "source": [
    "## 2.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c747d5",
   "metadata": {},
   "source": [
    "### Steps for textual data preprocessing\n",
    "1. Tokenization (= word segmentation)\n",
    "2. Punctualtion and non-alphabetic token removal\n",
    "3. Stopwords removal\n",
    "4. Lemmatization / stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c16eb-1361-4977-a248-dd636a79fb5d",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b23131-7e52-4097-8978-dd5b999ab25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dmatz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dmatz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dmatz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dmatz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538cd901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentence:\n",
      "The graduate student was typing, procrastinating, questioning herself, and finally submitting the dissertation while dreaming about sleep.\n"
     ]
    }
   ],
   "source": [
    "# Initialize stopwords, lemmatizer, and punctuation list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# We will use this sentence as example to showcase the different steps of data preprocessing\n",
    "example_sentence = \"The graduate student was typing, procrastinating, questioning herself, and finally submitting the dissertation while dreaming about sleep.\"\n",
    "print(f\"Example Sentence:\\n{example_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42202e7",
   "metadata": {},
   "source": [
    "### What is tokenization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eb5ba16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'graduate', 'student', 'was', 'typing', ',', 'procrastinating', ',', 'questioning', 'herself', ',', 'and', 'finally', 'submitting', 'the', 'dissertation', 'while', 'dreaming', 'about', 'sleep', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(example_sentence.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68961a99",
   "metadata": {},
   "source": [
    "### A quick removal of punctualtions and non-alphabetic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77b14e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'graduate', 'student', 'was', 'typing', 'procrastinating', 'questioning', 'herself', 'and', 'finally', 'submitting', 'the', 'dissertation', 'while', 'dreaming', 'about', 'sleep']\n"
     ]
    }
   ],
   "source": [
    "tokens_noPunc = [word.translate(punctuation_table) for word in tokens if word.isalpha()]\n",
    "print(tokens_noPunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8030b",
   "metadata": {},
   "source": [
    "### What are stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "906e138c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['graduate', 'student', 'typing', 'procrastinating', 'questioning', 'finally', 'submitting', 'dissertation', 'dreaming', 'sleep']\n"
     ]
    }
   ],
   "source": [
    "tokens_noSW = [word for word in tokens_noPunc if word not in stop_words]\n",
    "print(tokens_noSW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b25394",
   "metadata": {},
   "source": [
    "### What is lemmatization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6427e0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\tLemmatized\n",
      "\n",
      "graduate\tgraduate\n",
      "student\tstudent\n",
      "typing\ttyping\n",
      "procrastinating\tprocrastinating\n",
      "questioning\tquestioning\n",
      "finally\tfinally\n",
      "submitting\tsubmitting\n",
      "dissertation\tdissertation\n",
      "dreaming\tdreaming\n",
      "sleep\tsleep\n"
     ]
    }
   ],
   "source": [
    "print(\"Original\\tLemmatized\\n\")\n",
    "\n",
    "# Here we use pre-stopword removal tokens\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens_noSW]\n",
    "for ori, lem in zip(tokens_noSW, lemmatized_tokens):\n",
    "    print(f\"{ori}\\t{lem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0671810",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- What is lemmatization?\n",
    "- I guess you cannot tell what lemmatization is from the results above. Let's try lemmatization in another way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c21d4",
   "metadata": {},
   "source": [
    "### How about we tell the lemmatizer more information of the tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21a94a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('graduate', 'NN'), ('student', 'NN'), ('was', 'VBD'), ('typing', 'VBG'), (',', ','), ('procrastinating', 'VBG'), (',', ','), ('questioning', 'VBG'), ('herself', 'PRP'), (',', ','), ('and', 'CC'), ('finally', 'RB'), ('submitting', 'VBG'), ('the', 'DT'), ('dissertation', 'NN'), ('while', 'IN'), ('dreaming', 'VBG'), ('about', 'RB'), ('sleep', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Part-of-Speech Tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4205ac6",
   "metadata": {},
   "source": [
    "### Then we do the punctuation, non-alphabetic tokens, and stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6af0e29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('graduate', 'NN'), ('student', 'NN'), ('was', 'VBD'), ('typing', 'VBG'), ('procrastinating', 'VBG'), ('questioning', 'VBG'), ('herself', 'PRP'), ('and', 'CC'), ('finally', 'RB'), ('submitting', 'VBG'), ('the', 'DT'), ('dissertation', 'NN'), ('while', 'IN'), ('dreaming', 'VBG'), ('about', 'RB'), ('sleep', 'NN')]\n",
      "[('graduate', 'NN'), ('student', 'NN'), ('typing', 'VBG'), ('procrastinating', 'VBG'), ('questioning', 'VBG'), ('finally', 'RB'), ('submitting', 'VBG'), ('dissertation', 'NN'), ('dreaming', 'VBG'), ('sleep', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation and non-alphabetic tokens\n",
    "tagged_tokens_noPunc = [(word[0].translate(punctuation_table), word[1]) for word in tagged_tokens if word[0].isalpha()]\n",
    "print(tagged_tokens_noPunc)\n",
    "\n",
    "# Remove stopwords\n",
    "tagged_tokens_noSW = [(word[0], word[1]) for word in tagged_tokens_noPunc if word[0] not in stop_words]\n",
    "print(tagged_tokens_noSW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9ffbf",
   "metadata": {},
   "source": [
    "### Take 2: what is lemmatization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1f708fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\tLemmatized\n",
      "\n",
      "graduate\tgraduate\n",
      "student\tstudent\n",
      "typing\ttype\n",
      "procrastinating\tprocrastinate\n",
      "questioning\tquestion\n",
      "finally\tfinally\n",
      "submitting\tsubmit\n",
      "dissertation\tdissertation\n",
      "dreaming\tdream\n",
      "sleep\tsleep\n"
     ]
    }
   ],
   "source": [
    "# Convert treebank POS tags to wordnet POS tags so the lemmatizer can read them\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN\n",
    "\n",
    "print(\"Original\\tLemmatized\\n\")\n",
    "tagged_tokens_lemmatized = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens_noSW]\n",
    "for ori, lem in zip(tagged_tokens_noSW, tagged_tokens_lemmatized):\n",
    "    print(f\"{ori[0]}\\t{lem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33e82c",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- What is lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4fb82",
   "metadata": {},
   "source": [
    "### What is stemming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2edec23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\tStemmed\n",
      "\n",
      "graduate\tgraduat\n",
      "student\tstudent\n",
      "type\ttype\n",
      "procrastinate\tprocrastin\n",
      "question\tquestion\n",
      "finally\tfinal\n",
      "submit\tsubmit\n",
      "dissertation\tdissert\n",
      "dream\tdream\n",
      "sleep\tsleep\n"
     ]
    }
   ],
   "source": [
    "print(\"Original\\tStemmed\\n\")\n",
    "tokens_stemmed = [stemmer.stem(word) for word in tagged_tokens_lemmatized]\n",
    "for ori, stem in zip(tagged_tokens_lemmatized, tokens_stemmed):\n",
    "    print(f\"{ori}\\t{stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2efd5",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- What is stemming?\n",
    "- Why is stemming helpful in imporving TF-IDF performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9536305",
   "metadata": {},
   "source": [
    "### ✏️ Now let's preprocess the query and the documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b3c6fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Step 1: # Convert to lowercase and tokenize text into words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Step 2: Tag part-of-speech of the tokens\n",
    "    tokens = pos_tag(tokens)\n",
    "    \n",
    "    # Step 3: Remove punctuation and non-alphabetic tokens\n",
    "    tokens = [(word[0].translate(punctuation_table), word[1]) for word in tokens if word[0].isalpha()]\n",
    "    \n",
    "    # Step 4: Remove stopwords\n",
    "    tokens = [(word[0], word[1]) for word in tokens if word[0] not in stop_words]\n",
    "    \n",
    "    # Step 5: Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tokens]\n",
    "    \n",
    "    # Step 6: Stem tokens\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d78d7fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: ['sleep', 'depriv']\n",
      "\n",
      "Document 1: ['sleepless', 'night', 'lab', 'becom', 'new', 'normal', 'tri', 'fix', 'experi', 'setup', 'apparatu', 'seem', 'mind', 'advisor', 'say', 'result', 'around', 'corner', 'corner', 'keep', 'move', 'coffe', 'true', 'companion', 'day']\n",
      "Document 2: ['think', 'grad', 'school', 'would', 'intellectu', 'stimul', 'mostli', 'paperwork', 'wait', 'email', 'department', 'printer', 'jam', 'late', 'meet', 'cafeteria', 'run', 'good', 'snack', 'surviv', 'vend', 'machin', 'chip', 'sleep', 'becom', 'luxuri', 'longer', 'afford']\n",
      "Document 3: ['write', 'dissert', 'feel', 'like', 'climb', 'endless', 'mountain', 'everi', 'time', 'finish', 'chapter', 'supervisor', 'suggest', 'new', 'revis', 'impostor', 'syndrom', 'real', 'wonder', 'make', 'mistak', 'accept', 'mayb', 'go', 'clown', 'colleg', 'instead', 'utterli', 'depriv', 'semblanc', 'normal', 'life']\n",
      "Document 4: ['research', 'data', 'get', 'corrupt', 'start', 'lab', 'mous', 'escap', 'spend', 'hour', 'tri', 'find', 'grant', 'propos', 'deadlin', 'tomorrow', 'onlin', 'submiss', 'portal', 'least', 'pet', 'cactu', 'die', 'yet']\n",
      "Document 5: ['group', 'meet', 'turn', 'debat', 'font', 'choic', 'present', 'pretti', 'sure', 'colleagu', 'steal', 'lunch', 'fridg', 'photocopi', 'get', 'never', 'work', 'hurri', 'phd', 'nap', 'ace']\n",
      "Document 6: ['see', 'sun', 'day', 'due', 'endless', 'cod', 'session', 'simul', 'keep', 'crash', 'stack', 'overflow', 'answer', 'roommat', 'think', 'ghost', 'haunt', 'apart', 'instant', 'noodl', 'becom', 'primari', 'food', 'group']\n",
      "Document 7: ['attend', 'confer', 'sound', 'fun', 'realiz', 'involv', 'lot', 'awkward', 'network', 'accident', 'spill', 'coffe', 'famou', 'professor', 'shoe', 'poster', 'fell', 'twice', 'session', 'next', 'time', 'send', 'cardboard', 'cutout']\n",
      "Document 8: ['univers', 'gym', 'membership', 'suppos', 'keep', 'healthi', 'use', 'tri', 'attend', 'yoga', 'class', 'stay', 'late', 'deadlin', 'fell', 'asleep', 'medit', 'mayb', 'instead', 'gym', 'bed', 'essenti', 'keep', 'healthi']\n",
      "Document 9: ['teach', 'assistantship', 'involv', 'grade', 'endless', 'stack', 'exam', 'student', 'keep', 'email', 'extens', 'creativ', 'excus', 'one', 'claim', 'dog', 'sleep', 'laptop', 'use', 'exam', 'depriv', 'excus', 'complet', 'dissert', 'draft', 'might', 'get', 'good', 'one']\n",
      "Document 10: ['group', 'project', 'bad', 'one', 'work', 'team', 'member', 'elus', 'bigfoot', 'project', 'due', 'next', 'week', 'hear', 'perhap', 'write', 'paper', 'sociolog', 'implic', 'group', 'work', 'avoid']\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to each document in the corpus\n",
    "preprocessed_query = preprocess_text(query)\n",
    "print(f\"Query: {preprocessed_query}\\n\")\n",
    "\n",
    "preprocessed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "# Print preprocessed corpus\n",
    "for idx, doc in enumerate(preprocessed_corpus):\n",
    "    print(f\"Document {idx+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e275ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join preprocessed tokens back into strings for ngram-based TF-IDF\n",
    "preprocessed_query_text = ' '.join(preprocessed_query)\n",
    "preprocessed_corpus_texts = [' '.join(doc_tokens) for doc_tokens in preprocessed_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544cea0b",
   "metadata": {},
   "source": [
    "## ✏️ 2.2 Compute Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "874163f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF for Document 1: {'sleepless': 0.04, 'night': 0.04, 'lab': 0.04, 'becom': 0.04, 'new': 0.04, 'normal': 0.04, 'tri': 0.04, 'fix': 0.04, 'experi': 0.04, 'setup': 0.04, 'apparatu': 0.04, 'seem': 0.04, 'mind': 0.04, 'advisor': 0.04, 'say': 0.04, 'result': 0.04, 'around': 0.04, 'corner': 0.08, 'keep': 0.04, 'move': 0.04, 'coffe': 0.04, 'true': 0.04, 'companion': 0.04, 'day': 0.04}\n",
      "\n",
      "TF for Document 2: {'think': 0.03571428571428571, 'grad': 0.03571428571428571, 'school': 0.03571428571428571, 'would': 0.03571428571428571, 'intellectu': 0.03571428571428571, 'stimul': 0.03571428571428571, 'mostli': 0.03571428571428571, 'paperwork': 0.03571428571428571, 'wait': 0.03571428571428571, 'email': 0.03571428571428571, 'department': 0.03571428571428571, 'printer': 0.03571428571428571, 'jam': 0.03571428571428571, 'late': 0.03571428571428571, 'meet': 0.03571428571428571, 'cafeteria': 0.03571428571428571, 'run': 0.03571428571428571, 'good': 0.03571428571428571, 'snack': 0.03571428571428571, 'surviv': 0.03571428571428571, 'vend': 0.03571428571428571, 'machin': 0.03571428571428571, 'chip': 0.03571428571428571, 'sleep': 0.03571428571428571, 'becom': 0.03571428571428571, 'luxuri': 0.03571428571428571, 'longer': 0.03571428571428571, 'afford': 0.03571428571428571}\n",
      "\n",
      "TF for Document 3: {'write': 0.03125, 'dissert': 0.03125, 'feel': 0.03125, 'like': 0.03125, 'climb': 0.03125, 'endless': 0.03125, 'mountain': 0.03125, 'everi': 0.03125, 'time': 0.03125, 'finish': 0.03125, 'chapter': 0.03125, 'supervisor': 0.03125, 'suggest': 0.03125, 'new': 0.03125, 'revis': 0.03125, 'impostor': 0.03125, 'syndrom': 0.03125, 'real': 0.03125, 'wonder': 0.03125, 'make': 0.03125, 'mistak': 0.03125, 'accept': 0.03125, 'mayb': 0.03125, 'go': 0.03125, 'clown': 0.03125, 'colleg': 0.03125, 'instead': 0.03125, 'utterli': 0.03125, 'depriv': 0.03125, 'semblanc': 0.03125, 'normal': 0.03125, 'life': 0.03125}\n",
      "\n",
      "TF for Document 4: {'research': 0.041666666666666664, 'data': 0.041666666666666664, 'get': 0.041666666666666664, 'corrupt': 0.041666666666666664, 'start': 0.041666666666666664, 'lab': 0.041666666666666664, 'mous': 0.041666666666666664, 'escap': 0.041666666666666664, 'spend': 0.041666666666666664, 'hour': 0.041666666666666664, 'tri': 0.041666666666666664, 'find': 0.041666666666666664, 'grant': 0.041666666666666664, 'propos': 0.041666666666666664, 'deadlin': 0.041666666666666664, 'tomorrow': 0.041666666666666664, 'onlin': 0.041666666666666664, 'submiss': 0.041666666666666664, 'portal': 0.041666666666666664, 'least': 0.041666666666666664, 'pet': 0.041666666666666664, 'cactu': 0.041666666666666664, 'die': 0.041666666666666664, 'yet': 0.041666666666666664}\n",
      "\n",
      "TF for Document 5: {'group': 0.047619047619047616, 'meet': 0.047619047619047616, 'turn': 0.047619047619047616, 'debat': 0.047619047619047616, 'font': 0.047619047619047616, 'choic': 0.047619047619047616, 'present': 0.047619047619047616, 'pretti': 0.047619047619047616, 'sure': 0.047619047619047616, 'colleagu': 0.047619047619047616, 'steal': 0.047619047619047616, 'lunch': 0.047619047619047616, 'fridg': 0.047619047619047616, 'photocopi': 0.047619047619047616, 'get': 0.047619047619047616, 'never': 0.047619047619047616, 'work': 0.047619047619047616, 'hurri': 0.047619047619047616, 'phd': 0.047619047619047616, 'nap': 0.047619047619047616, 'ace': 0.047619047619047616}\n",
      "\n",
      "TF for Document 6: {'see': 0.041666666666666664, 'sun': 0.041666666666666664, 'day': 0.041666666666666664, 'due': 0.041666666666666664, 'endless': 0.041666666666666664, 'cod': 0.041666666666666664, 'session': 0.041666666666666664, 'simul': 0.041666666666666664, 'keep': 0.041666666666666664, 'crash': 0.041666666666666664, 'stack': 0.041666666666666664, 'overflow': 0.041666666666666664, 'answer': 0.041666666666666664, 'roommat': 0.041666666666666664, 'think': 0.041666666666666664, 'ghost': 0.041666666666666664, 'haunt': 0.041666666666666664, 'apart': 0.041666666666666664, 'instant': 0.041666666666666664, 'noodl': 0.041666666666666664, 'becom': 0.041666666666666664, 'primari': 0.041666666666666664, 'food': 0.041666666666666664, 'group': 0.041666666666666664}\n",
      "\n",
      "TF for Document 7: {'attend': 0.041666666666666664, 'confer': 0.041666666666666664, 'sound': 0.041666666666666664, 'fun': 0.041666666666666664, 'realiz': 0.041666666666666664, 'involv': 0.041666666666666664, 'lot': 0.041666666666666664, 'awkward': 0.041666666666666664, 'network': 0.041666666666666664, 'accident': 0.041666666666666664, 'spill': 0.041666666666666664, 'coffe': 0.041666666666666664, 'famou': 0.041666666666666664, 'professor': 0.041666666666666664, 'shoe': 0.041666666666666664, 'poster': 0.041666666666666664, 'fell': 0.041666666666666664, 'twice': 0.041666666666666664, 'session': 0.041666666666666664, 'next': 0.041666666666666664, 'time': 0.041666666666666664, 'send': 0.041666666666666664, 'cardboard': 0.041666666666666664, 'cutout': 0.041666666666666664}\n",
      "\n",
      "TF for Document 8: {'univers': 0.041666666666666664, 'gym': 0.08333333333333333, 'membership': 0.041666666666666664, 'suppos': 0.041666666666666664, 'keep': 0.08333333333333333, 'healthi': 0.08333333333333333, 'use': 0.041666666666666664, 'tri': 0.041666666666666664, 'attend': 0.041666666666666664, 'yoga': 0.041666666666666664, 'class': 0.041666666666666664, 'stay': 0.041666666666666664, 'late': 0.041666666666666664, 'deadlin': 0.041666666666666664, 'fell': 0.041666666666666664, 'asleep': 0.041666666666666664, 'medit': 0.041666666666666664, 'mayb': 0.041666666666666664, 'instead': 0.041666666666666664, 'bed': 0.041666666666666664, 'essenti': 0.041666666666666664}\n",
      "\n",
      "TF for Document 9: {'teach': 0.034482758620689655, 'assistantship': 0.034482758620689655, 'involv': 0.034482758620689655, 'grade': 0.034482758620689655, 'endless': 0.034482758620689655, 'stack': 0.034482758620689655, 'exam': 0.06896551724137931, 'student': 0.034482758620689655, 'keep': 0.034482758620689655, 'email': 0.034482758620689655, 'extens': 0.034482758620689655, 'creativ': 0.034482758620689655, 'excus': 0.06896551724137931, 'one': 0.06896551724137931, 'claim': 0.034482758620689655, 'dog': 0.034482758620689655, 'sleep': 0.034482758620689655, 'laptop': 0.034482758620689655, 'use': 0.034482758620689655, 'depriv': 0.034482758620689655, 'complet': 0.034482758620689655, 'dissert': 0.034482758620689655, 'draft': 0.034482758620689655, 'might': 0.034482758620689655, 'get': 0.034482758620689655, 'good': 0.034482758620689655}\n",
      "\n",
      "TF for Document 10: {'group': 0.09090909090909091, 'project': 0.09090909090909091, 'bad': 0.045454545454545456, 'one': 0.045454545454545456, 'work': 0.09090909090909091, 'team': 0.045454545454545456, 'member': 0.045454545454545456, 'elus': 0.045454545454545456, 'bigfoot': 0.045454545454545456, 'due': 0.045454545454545456, 'next': 0.045454545454545456, 'week': 0.045454545454545456, 'hear': 0.045454545454545456, 'perhap': 0.045454545454545456, 'write': 0.045454545454545456, 'paper': 0.045454545454545456, 'sociolog': 0.045454545454545456, 'implic': 0.045454545454545456, 'avoid': 0.045454545454545456}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to compute term frequency (TF) for each document\n",
    "def compute_tf(doc):\n",
    "    \n",
    "    # Initialize the TF dictionary\n",
    "    tf_dict = {}\n",
    "    \n",
    "    # TODO\n",
    "    # Count the term frequency \n",
    "    for word in doc:\n",
    "        tf_dict[word] = tf_dict.get(word, 0) + 1\n",
    "    \n",
    "    # TODO\n",
    "    # Divide term counts by total number of terms in the document\n",
    "    total_terms = len(doc)\n",
    "    for word in tf_dict:\n",
    "        tf_dict[word] = tf_dict[word] / total_terms\n",
    "    \n",
    "    return tf_dict\n",
    "\n",
    "# Compute TF for each document in the corpus\n",
    "tf_corpus = [compute_tf(doc) for doc in preprocessed_corpus]\n",
    "\n",
    "# Print TF values for each document\n",
    "for idx, tf in enumerate(tf_corpus):\n",
    "    print(f\"TF for Document {idx+1}: {tf}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3a5b8",
   "metadata": {},
   "source": [
    "## ✏️ 2.3 Compute Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b72b4c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for Corpus:\n",
      "lab: 2.6094379124341005\n",
      "advisor: 3.302585092994046\n",
      "becom: 2.203972804325936\n",
      "day: 2.6094379124341005\n",
      "experi: 3.302585092994046\n",
      "mind: 3.302585092994046\n",
      "around: 3.302585092994046\n",
      "new: 2.6094379124341005\n",
      "apparatu: 3.302585092994046\n",
      "fix: 3.302585092994046\n",
      "true: 3.302585092994046\n",
      "corner: 3.302585092994046\n",
      "companion: 3.302585092994046\n",
      "say: 3.302585092994046\n",
      "setup: 3.302585092994046\n",
      "coffe: 2.6094379124341005\n",
      "sleepless: 3.302585092994046\n",
      "night: 3.302585092994046\n",
      "tri: 2.203972804325936\n",
      "keep: 1.916290731874155\n",
      "result: 3.302585092994046\n",
      "seem: 3.302585092994046\n",
      "normal: 2.6094379124341005\n",
      "move: 3.302585092994046\n",
      "would: 3.302585092994046\n",
      "think: 2.6094379124341005\n",
      "afford: 3.302585092994046\n",
      "cafeteria: 3.302585092994046\n",
      "late: 2.6094379124341005\n",
      "good: 2.6094379124341005\n",
      "school: 3.302585092994046\n",
      "surviv: 3.302585092994046\n",
      "paperwork: 3.302585092994046\n",
      "printer: 3.302585092994046\n",
      "sleep: 2.6094379124341005\n",
      "wait: 3.302585092994046\n",
      "chip: 3.302585092994046\n",
      "mostli: 3.302585092994046\n",
      "department: 3.302585092994046\n",
      "longer: 3.302585092994046\n",
      "run: 3.302585092994046\n",
      "intellectu: 3.302585092994046\n",
      "stimul: 3.302585092994046\n",
      "meet: 2.6094379124341005\n",
      "vend: 3.302585092994046\n",
      "grad: 3.302585092994046\n",
      "email: 2.6094379124341005\n",
      "jam: 3.302585092994046\n",
      "machin: 3.302585092994046\n",
      "snack: 3.302585092994046\n",
      "luxuri: 3.302585092994046\n",
      "syndrom: 3.302585092994046\n",
      "endless: 2.203972804325936\n",
      "wonder: 3.302585092994046\n",
      "mistak: 3.302585092994046\n",
      "depriv: 2.6094379124341005\n",
      "make: 3.302585092994046\n",
      "colleg: 3.302585092994046\n",
      "climb: 3.302585092994046\n",
      "everi: 3.302585092994046\n",
      "instead: 2.6094379124341005\n",
      "write: 2.6094379124341005\n",
      "accept: 3.302585092994046\n",
      "mountain: 3.302585092994046\n",
      "time: 2.6094379124341005\n",
      "mayb: 2.6094379124341005\n",
      "utterli: 3.302585092994046\n",
      "dissert: 2.6094379124341005\n",
      "revis: 3.302585092994046\n",
      "suggest: 3.302585092994046\n",
      "impostor: 3.302585092994046\n",
      "semblanc: 3.302585092994046\n",
      "life: 3.302585092994046\n",
      "feel: 3.302585092994046\n",
      "real: 3.302585092994046\n",
      "chapter: 3.302585092994046\n",
      "go: 3.302585092994046\n",
      "supervisor: 3.302585092994046\n",
      "like: 3.302585092994046\n",
      "finish: 3.302585092994046\n",
      "clown: 3.302585092994046\n",
      "find: 3.302585092994046\n",
      "data: 3.302585092994046\n",
      "get: 2.203972804325936\n",
      "deadlin: 2.6094379124341005\n",
      "hour: 3.302585092994046\n",
      "least: 3.302585092994046\n",
      "research: 3.302585092994046\n",
      "portal: 3.302585092994046\n",
      "submiss: 3.302585092994046\n",
      "tomorrow: 3.302585092994046\n",
      "onlin: 3.302585092994046\n",
      "die: 3.302585092994046\n",
      "spend: 3.302585092994046\n",
      "pet: 3.302585092994046\n",
      "cactu: 3.302585092994046\n",
      "start: 3.302585092994046\n",
      "propos: 3.302585092994046\n",
      "yet: 3.302585092994046\n",
      "mous: 3.302585092994046\n",
      "escap: 3.302585092994046\n",
      "grant: 3.302585092994046\n",
      "corrupt: 3.302585092994046\n",
      "never: 3.302585092994046\n",
      "turn: 3.302585092994046\n",
      "colleagu: 3.302585092994046\n",
      "nap: 3.302585092994046\n",
      "fridg: 3.302585092994046\n",
      "debat: 3.302585092994046\n",
      "hurri: 3.302585092994046\n",
      "group: 2.203972804325936\n",
      "pretti: 3.302585092994046\n",
      "work: 2.6094379124341005\n",
      "steal: 3.302585092994046\n",
      "sure: 3.302585092994046\n",
      "lunch: 3.302585092994046\n",
      "font: 3.302585092994046\n",
      "phd: 3.302585092994046\n",
      "choic: 3.302585092994046\n",
      "ace: 3.302585092994046\n",
      "photocopi: 3.302585092994046\n",
      "present: 3.302585092994046\n",
      "stack: 2.6094379124341005\n",
      "session: 2.6094379124341005\n",
      "roommat: 3.302585092994046\n",
      "see: 3.302585092994046\n",
      "sun: 3.302585092994046\n",
      "apart: 3.302585092994046\n",
      "noodl: 3.302585092994046\n",
      "simul: 3.302585092994046\n",
      "overflow: 3.302585092994046\n",
      "haunt: 3.302585092994046\n",
      "crash: 3.302585092994046\n",
      "answer: 3.302585092994046\n",
      "cod: 3.302585092994046\n",
      "instant: 3.302585092994046\n",
      "due: 2.6094379124341005\n",
      "food: 3.302585092994046\n",
      "ghost: 3.302585092994046\n",
      "primari: 3.302585092994046\n",
      "fell: 2.6094379124341005\n",
      "attend: 2.6094379124341005\n",
      "cardboard: 3.302585092994046\n",
      "send: 3.302585092994046\n",
      "next: 2.6094379124341005\n",
      "twice: 3.302585092994046\n",
      "involv: 2.6094379124341005\n",
      "professor: 3.302585092994046\n",
      "fun: 3.302585092994046\n",
      "network: 3.302585092994046\n",
      "cutout: 3.302585092994046\n",
      "accident: 3.302585092994046\n",
      "confer: 3.302585092994046\n",
      "poster: 3.302585092994046\n",
      "awkward: 3.302585092994046\n",
      "sound: 3.302585092994046\n",
      "realiz: 3.302585092994046\n",
      "lot: 3.302585092994046\n",
      "famou: 3.302585092994046\n",
      "spill: 3.302585092994046\n",
      "shoe: 3.302585092994046\n",
      "membership: 3.302585092994046\n",
      "medit: 3.302585092994046\n",
      "use: 2.6094379124341005\n",
      "asleep: 3.302585092994046\n",
      "healthi: 3.302585092994046\n",
      "gym: 3.302585092994046\n",
      "stay: 3.302585092994046\n",
      "essenti: 3.302585092994046\n",
      "bed: 3.302585092994046\n",
      "univers: 3.302585092994046\n",
      "yoga: 3.302585092994046\n",
      "suppos: 3.302585092994046\n",
      "class: 3.302585092994046\n",
      "student: 3.302585092994046\n",
      "creativ: 3.302585092994046\n",
      "dog: 3.302585092994046\n",
      "might: 3.302585092994046\n",
      "complet: 3.302585092994046\n",
      "draft: 3.302585092994046\n",
      "excus: 3.302585092994046\n",
      "extens: 3.302585092994046\n",
      "one: 2.6094379124341005\n",
      "grade: 3.302585092994046\n",
      "exam: 3.302585092994046\n",
      "laptop: 3.302585092994046\n",
      "assistantship: 3.302585092994046\n",
      "claim: 3.302585092994046\n",
      "teach: 3.302585092994046\n",
      "team: 3.302585092994046\n",
      "avoid: 3.302585092994046\n",
      "bad: 3.302585092994046\n",
      "project: 3.302585092994046\n",
      "paper: 3.302585092994046\n",
      "implic: 3.302585092994046\n",
      "hear: 3.302585092994046\n",
      "week: 3.302585092994046\n",
      "elus: 3.302585092994046\n",
      "perhap: 3.302585092994046\n",
      "sociolog: 3.302585092994046\n",
      "member: 3.302585092994046\n",
      "bigfoot: 3.302585092994046\n"
     ]
    }
   ],
   "source": [
    "# Function to compute inverse document frequency (IDF) for each term in the corpus\n",
    "def compute_idf(corpus):\n",
    "    \n",
    "    N = len(corpus)  # Total number of documents\n",
    "    \n",
    "    # Initialize the IDF dictionary\n",
    "    idf_dict = defaultdict(int)\n",
    "    \n",
    "    # TODO\n",
    "    # Count the number of documents containing each word\n",
    "    for doc in corpus:\n",
    "        for word in set(doc):  # Use set to count each word only once per document\n",
    "            idf_dict[word] += 1\n",
    "    \n",
    "    #TODO\n",
    "    # Compute IDF (logarithmic scale)\n",
    "    for word in idf_dict:\n",
    "        idf_dict[word] = math.log(N / (idf_dict[word])) + 1  # Smoothing by adding 1\n",
    "    \n",
    "    return idf_dict\n",
    "\n",
    "# Compute IDF for the corpus\n",
    "idf_dict = compute_idf(preprocessed_corpus)\n",
    "\n",
    "# Print IDF values\n",
    "print(\"IDF for Corpus:\")\n",
    "for word, idf in idf_dict.items():\n",
    "    print(f\"{word}: {idf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bface74",
   "metadata": {},
   "source": [
    "## ✏️ 2.4 Compute TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce20a060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for Document 1: {'sleepless': 0.13210340371976184, 'night': 0.13210340371976184, 'lab': 0.10437751649736403, 'becom': 0.08815891217303744, 'new': 0.10437751649736403, 'normal': 0.10437751649736403, 'tri': 0.08815891217303744, 'fix': 0.13210340371976184, 'experi': 0.13210340371976184, 'setup': 0.13210340371976184, 'apparatu': 0.13210340371976184, 'seem': 0.13210340371976184, 'mind': 0.13210340371976184, 'advisor': 0.13210340371976184, 'say': 0.13210340371976184, 'result': 0.13210340371976184, 'around': 0.13210340371976184, 'corner': 0.2642068074395237, 'keep': 0.0766516292749662, 'move': 0.13210340371976184, 'coffe': 0.10437751649736403, 'true': 0.13210340371976184, 'companion': 0.13210340371976184, 'day': 0.10437751649736403}\n",
      "\n",
      "TF-IDF for Document 2: {'think': 0.09319421115836073, 'grad': 0.1179494676069302, 'school': 0.1179494676069302, 'would': 0.1179494676069302, 'intellectu': 0.1179494676069302, 'stimul': 0.1179494676069302, 'mostli': 0.1179494676069302, 'paperwork': 0.1179494676069302, 'wait': 0.1179494676069302, 'email': 0.09319421115836073, 'department': 0.1179494676069302, 'printer': 0.1179494676069302, 'jam': 0.1179494676069302, 'late': 0.09319421115836073, 'meet': 0.09319421115836073, 'cafeteria': 0.1179494676069302, 'run': 0.1179494676069302, 'good': 0.09319421115836073, 'snack': 0.1179494676069302, 'surviv': 0.1179494676069302, 'vend': 0.1179494676069302, 'machin': 0.1179494676069302, 'chip': 0.1179494676069302, 'sleep': 0.09319421115836073, 'becom': 0.078713314440212, 'luxuri': 0.1179494676069302, 'longer': 0.1179494676069302, 'afford': 0.1179494676069302}\n",
      "\n",
      "TF-IDF for Document 3: {'write': 0.08154493476356564, 'dissert': 0.08154493476356564, 'feel': 0.10320578415606393, 'like': 0.10320578415606393, 'climb': 0.10320578415606393, 'endless': 0.0688741501351855, 'mountain': 0.10320578415606393, 'everi': 0.10320578415606393, 'time': 0.08154493476356564, 'finish': 0.10320578415606393, 'chapter': 0.10320578415606393, 'supervisor': 0.10320578415606393, 'suggest': 0.10320578415606393, 'new': 0.08154493476356564, 'revis': 0.10320578415606393, 'impostor': 0.10320578415606393, 'syndrom': 0.10320578415606393, 'real': 0.10320578415606393, 'wonder': 0.10320578415606393, 'make': 0.10320578415606393, 'mistak': 0.10320578415606393, 'accept': 0.10320578415606393, 'mayb': 0.08154493476356564, 'go': 0.10320578415606393, 'clown': 0.10320578415606393, 'colleg': 0.10320578415606393, 'instead': 0.08154493476356564, 'utterli': 0.10320578415606393, 'depriv': 0.08154493476356564, 'semblanc': 0.10320578415606393, 'normal': 0.08154493476356564, 'life': 0.10320578415606393}\n",
      "\n",
      "TF-IDF for Document 4: {'research': 0.13760771220808524, 'data': 0.13760771220808524, 'get': 0.09183220018024732, 'corrupt': 0.13760771220808524, 'start': 0.13760771220808524, 'lab': 0.10872657968475419, 'mous': 0.13760771220808524, 'escap': 0.13760771220808524, 'spend': 0.13760771220808524, 'hour': 0.13760771220808524, 'tri': 0.09183220018024732, 'find': 0.13760771220808524, 'grant': 0.13760771220808524, 'propos': 0.13760771220808524, 'deadlin': 0.10872657968475419, 'tomorrow': 0.13760771220808524, 'onlin': 0.13760771220808524, 'submiss': 0.13760771220808524, 'portal': 0.13760771220808524, 'least': 0.13760771220808524, 'pet': 0.13760771220808524, 'cactu': 0.13760771220808524, 'die': 0.13760771220808524, 'yet': 0.13760771220808524}\n",
      "\n",
      "TF-IDF for Document 5: {'group': 0.10495108592028266, 'meet': 0.12425894821114764, 'turn': 0.15726595680924027, 'debat': 0.15726595680924027, 'font': 0.15726595680924027, 'choic': 0.15726595680924027, 'present': 0.15726595680924027, 'pretti': 0.15726595680924027, 'sure': 0.15726595680924027, 'colleagu': 0.15726595680924027, 'steal': 0.15726595680924027, 'lunch': 0.15726595680924027, 'fridg': 0.15726595680924027, 'photocopi': 0.15726595680924027, 'get': 0.10495108592028266, 'never': 0.15726595680924027, 'work': 0.12425894821114764, 'hurri': 0.15726595680924027, 'phd': 0.15726595680924027, 'nap': 0.15726595680924027, 'ace': 0.15726595680924027}\n",
      "\n",
      "TF-IDF for Document 6: {'see': 0.13760771220808524, 'sun': 0.13760771220808524, 'day': 0.10872657968475419, 'due': 0.10872657968475419, 'endless': 0.09183220018024732, 'cod': 0.13760771220808524, 'session': 0.10872657968475419, 'simul': 0.13760771220808524, 'keep': 0.07984544716142312, 'crash': 0.13760771220808524, 'stack': 0.10872657968475419, 'overflow': 0.13760771220808524, 'answer': 0.13760771220808524, 'roommat': 0.13760771220808524, 'think': 0.10872657968475419, 'ghost': 0.13760771220808524, 'haunt': 0.13760771220808524, 'apart': 0.13760771220808524, 'instant': 0.13760771220808524, 'noodl': 0.13760771220808524, 'becom': 0.09183220018024732, 'primari': 0.13760771220808524, 'food': 0.13760771220808524, 'group': 0.09183220018024732}\n",
      "\n",
      "TF-IDF for Document 7: {'attend': 0.10872657968475419, 'confer': 0.13760771220808524, 'sound': 0.13760771220808524, 'fun': 0.13760771220808524, 'realiz': 0.13760771220808524, 'involv': 0.10872657968475419, 'lot': 0.13760771220808524, 'awkward': 0.13760771220808524, 'network': 0.13760771220808524, 'accident': 0.13760771220808524, 'spill': 0.13760771220808524, 'coffe': 0.10872657968475419, 'famou': 0.13760771220808524, 'professor': 0.13760771220808524, 'shoe': 0.13760771220808524, 'poster': 0.13760771220808524, 'fell': 0.10872657968475419, 'twice': 0.13760771220808524, 'session': 0.10872657968475419, 'next': 0.10872657968475419, 'time': 0.10872657968475419, 'send': 0.13760771220808524, 'cardboard': 0.13760771220808524, 'cutout': 0.13760771220808524}\n",
      "\n",
      "TF-IDF for Document 8: {'univers': 0.13760771220808524, 'gym': 0.2752154244161705, 'membership': 0.13760771220808524, 'suppos': 0.13760771220808524, 'keep': 0.15969089432284625, 'healthi': 0.2752154244161705, 'use': 0.10872657968475419, 'tri': 0.09183220018024732, 'attend': 0.10872657968475419, 'yoga': 0.13760771220808524, 'class': 0.13760771220808524, 'stay': 0.13760771220808524, 'late': 0.10872657968475419, 'deadlin': 0.10872657968475419, 'fell': 0.10872657968475419, 'asleep': 0.13760771220808524, 'medit': 0.13760771220808524, 'mayb': 0.10872657968475419, 'instead': 0.10872657968475419, 'bed': 0.13760771220808524, 'essenti': 0.13760771220808524}\n",
      "\n",
      "TF-IDF for Document 9: {'teach': 0.11388224458600159, 'assistantship': 0.11388224458600159, 'involv': 0.0899806176701414, 'grade': 0.11388224458600159, 'endless': 0.07599906221813572, 'stack': 0.0899806176701414, 'exam': 0.22776448917200318, 'student': 0.11388224458600159, 'keep': 0.06607899075428121, 'email': 0.0899806176701414, 'extens': 0.11388224458600159, 'creativ': 0.11388224458600159, 'excus': 0.22776448917200318, 'one': 0.1799612353402828, 'claim': 0.11388224458600159, 'dog': 0.11388224458600159, 'sleep': 0.0899806176701414, 'laptop': 0.11388224458600159, 'use': 0.0899806176701414, 'depriv': 0.0899806176701414, 'complet': 0.11388224458600159, 'dissert': 0.0899806176701414, 'draft': 0.11388224458600159, 'might': 0.11388224458600159, 'get': 0.07599906221813572, 'good': 0.0899806176701414}\n",
      "\n",
      "TF-IDF for Document 10: {'group': 0.20036116402963053, 'project': 0.3002350084540042, 'bad': 0.1501175042270021, 'one': 0.11861081420155002, 'work': 0.23722162840310004, 'team': 0.1501175042270021, 'member': 0.1501175042270021, 'elus': 0.1501175042270021, 'bigfoot': 0.1501175042270021, 'due': 0.11861081420155002, 'next': 0.11861081420155002, 'week': 0.1501175042270021, 'hear': 0.1501175042270021, 'perhap': 0.1501175042270021, 'write': 0.11861081420155002, 'paper': 0.1501175042270021, 'sociolog': 0.1501175042270021, 'implic': 0.1501175042270021, 'avoid': 0.1501175042270021}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to compute TF-IDF for a document\n",
    "def compute_tfidf(tf_doc, idf_dict):\n",
    "    \n",
    "    # Initialize TF-IDF dictionary\n",
    "    tfidf_dict = {}\n",
    "    \n",
    "    # TODO\n",
    "    # Multiply TF by corresponding IDF\n",
    "    for word, tf_value in tf_doc.items():\n",
    "        tfidf_dict[word] = tf_value * idf_dict.get(word, 0)  # Multiply TF by corresponding IDF\n",
    "        \n",
    "    return tfidf_dict\n",
    "\n",
    "# Compute TF-IDF for each document in the corpus\n",
    "tfidf_corpus = [compute_tfidf(tf, idf_dict) for tf in tf_corpus]\n",
    "\n",
    "# Print TF-IDF values for each document\n",
    "for idx, tfidf in enumerate(tfidf_corpus):\n",
    "    print(f\"TF-IDF for Document {idx+1}: {tfidf}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d96e40",
   "metadata": {},
   "source": [
    "## 2.5 The Implementaion of Information Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2306f35a",
   "metadata": {},
   "source": [
    "### Measuring similarity: cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af4c5181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = sum(vec1.get(word, 0) * vec2.get(word, 0) for word in vec1)\n",
    "    magnitude1 = math.sqrt(sum([value ** 2 for value in vec1.values()]))\n",
    "    magnitude2 = math.sqrt(sum([value ** 2 for value in vec2.values()]))\n",
    "    \n",
    "    if not magnitude1 or not magnitude2:\n",
    "        return 0.0\n",
    "    return dot_product / (magnitude1 * magnitude2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b21add",
   "metadata": {},
   "source": [
    "### Rank the documents using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0779c1f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rankings based on Query:\n",
      "Rank 1: Document 9 with score 0.20850879673278558\n",
      "Rank 2: Document 2 with score 0.11131520312033671\n",
      "Rank 3: Document 3 with score 0.10476487421443352\n",
      "Rank 4: Document 1 with score 0.0\n",
      "Rank 5: Document 4 with score 0.0\n",
      "Rank 6: Document 5 with score 0.0\n",
      "Rank 7: Document 6 with score 0.0\n",
      "Rank 8: Document 7 with score 0.0\n",
      "Rank 9: Document 8 with score 0.0\n",
      "Rank 10: Document 10 with score 0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute TF for the query\n",
    "tf_query = compute_tf(preprocessed_query)\n",
    "\n",
    "# Compute TF-IDF for the query\n",
    "tfidf_query = compute_tfidf(tf_query, idf_dict)\n",
    "\n",
    "# Compute the cosine similarity of each documents to the query\n",
    "tfidf_results = []\n",
    "for idx, tfidf_doc in enumerate(tfidf_corpus):\n",
    "    score = cosine_similarity(tfidf_doc, tfidf_query)\n",
    "    tfidf_results.append((idx + 1, score))\n",
    "\n",
    "# Sort documents by similarity score in descending order\n",
    "tfidf_results = sorted(tfidf_results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print document rankings\n",
    "print(\"Document Rankings based on Query:\")\n",
    "for rank, (doc_idx, score) in enumerate(tfidf_results, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716209d8-1197-4daf-891a-def8d48f0f19",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- Are the highly ranked documents relevant to the query?\n",
    "- Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d2fa2",
   "metadata": {},
   "source": [
    "# 3. Vector Space Model: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c969039",
   "metadata": {},
   "source": [
    "## 3.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "80bbdc9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb50280",
   "metadata": {},
   "source": [
    "## 3.2 Load Pre-trained Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f6ac3b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained Google News Word2Vec model\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "gensim_data_dir = Path.home() / \"gensim-data\" / \"word2vec-google-news-300_tmp\"\n",
    "if gensim_data_dir.exists():\n",
    "    shutil.rmtree(gensim_data_dir, ignore_errors=True)\n",
    "\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7290df8b-cd95-4078-9274-6b2e2256e0a8",
   "metadata": {},
   "source": [
    "### Let's observe a Word2Vec vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "74f9c375-10cf-4dc5-b8ce-8e3835dd9fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06445312, -0.16015625, -0.01208496,  0.13476562, -0.22949219,\n",
       "        0.16210938,  0.3046875 , -0.1796875 , -0.12109375,  0.25390625,\n",
       "       -0.01428223, -0.06396484, -0.08056641, -0.05688477, -0.19628906,\n",
       "        0.2890625 , -0.05151367,  0.14257812, -0.10498047, -0.04736328,\n",
       "       -0.34765625,  0.35742188,  0.265625  ,  0.00188446, -0.01586914,\n",
       "        0.00195312, -0.35546875,  0.22167969,  0.05761719,  0.15917969,\n",
       "        0.08691406, -0.0267334 , -0.04785156,  0.23925781, -0.05981445,\n",
       "        0.0378418 ,  0.17382812, -0.41796875,  0.2890625 ,  0.32617188,\n",
       "        0.02429199, -0.01647949, -0.06494141, -0.08886719,  0.07666016,\n",
       "       -0.15136719,  0.05249023, -0.04199219, -0.05419922,  0.00108337,\n",
       "       -0.20117188,  0.12304688,  0.09228516,  0.10449219, -0.00408936,\n",
       "       -0.04199219,  0.01409912, -0.02111816, -0.13476562, -0.24316406,\n",
       "        0.16015625, -0.06689453, -0.08984375, -0.07177734, -0.00595093,\n",
       "       -0.00482178, -0.00089264, -0.30664062, -0.0625    ,  0.07958984,\n",
       "       -0.00909424, -0.04492188,  0.09960938, -0.33398438, -0.3984375 ,\n",
       "        0.05541992, -0.06689453, -0.04467773,  0.11767578, -0.13964844,\n",
       "       -0.26367188,  0.17480469, -0.17382812, -0.40625   , -0.06738281,\n",
       "       -0.07617188,  0.09423828,  0.20996094, -0.16308594, -0.08691406,\n",
       "       -0.0534668 , -0.10351562, -0.07617188, -0.11083984, -0.03515625,\n",
       "       -0.14941406,  0.0378418 ,  0.38671875,  0.14160156, -0.2890625 ,\n",
       "       -0.16894531, -0.140625  , -0.04174805,  0.22753906,  0.24023438,\n",
       "       -0.01599121, -0.06787109,  0.21875   , -0.42382812, -0.5625    ,\n",
       "       -0.49414062, -0.3359375 ,  0.13378906,  0.01141357,  0.13671875,\n",
       "        0.0324707 ,  0.06835938, -0.27539062, -0.15917969,  0.00121307,\n",
       "        0.01208496, -0.0039978 ,  0.00442505, -0.04541016,  0.08642578,\n",
       "        0.09960938, -0.04296875, -0.11328125,  0.13867188,  0.41796875,\n",
       "       -0.28320312, -0.07373047, -0.11425781,  0.08691406, -0.02148438,\n",
       "        0.328125  , -0.07373047, -0.01348877,  0.17773438, -0.02624512,\n",
       "        0.13378906, -0.11132812, -0.12792969, -0.12792969,  0.18945312,\n",
       "       -0.13867188,  0.29882812, -0.07714844, -0.37695312, -0.10351562,\n",
       "        0.16992188, -0.10742188, -0.29882812,  0.00866699, -0.27734375,\n",
       "       -0.20996094, -0.1796875 , -0.19628906, -0.22167969,  0.08886719,\n",
       "       -0.27734375, -0.13964844,  0.15917969,  0.03637695,  0.03320312,\n",
       "       -0.08105469,  0.25390625, -0.08691406, -0.21289062, -0.18945312,\n",
       "       -0.22363281,  0.06542969, -0.16601562,  0.08837891, -0.359375  ,\n",
       "       -0.09863281,  0.35546875, -0.00741577,  0.19042969,  0.16992188,\n",
       "       -0.06005859, -0.20605469,  0.08105469,  0.12988281, -0.01135254,\n",
       "        0.33203125, -0.08691406,  0.27539062, -0.03271484,  0.12011719,\n",
       "       -0.0625    ,  0.1953125 , -0.10986328, -0.11767578,  0.20996094,\n",
       "        0.19921875,  0.02954102, -0.16015625,  0.00276184, -0.01367188,\n",
       "        0.03442383, -0.19335938,  0.00352478, -0.06542969, -0.05566406,\n",
       "        0.09423828,  0.29296875,  0.04052734, -0.09326172, -0.10107422,\n",
       "       -0.27539062,  0.04394531, -0.07275391,  0.13867188,  0.02380371,\n",
       "        0.13085938,  0.00236511, -0.2265625 ,  0.34765625,  0.13574219,\n",
       "        0.05224609,  0.18164062,  0.0402832 ,  0.23730469, -0.16992188,\n",
       "        0.10058594,  0.03833008,  0.10839844, -0.05615234, -0.00946045,\n",
       "        0.14550781, -0.30078125, -0.32226562,  0.18847656, -0.40234375,\n",
       "       -0.3125    , -0.08007812, -0.26757812,  0.16699219,  0.07324219,\n",
       "        0.06347656,  0.06591797,  0.17285156, -0.17773438,  0.00276184,\n",
       "       -0.05761719, -0.2265625 , -0.19628906,  0.09667969,  0.13769531,\n",
       "       -0.49414062, -0.27929688,  0.12304688, -0.30078125,  0.01293945,\n",
       "       -0.1875    , -0.20898438, -0.1796875 , -0.16015625, -0.03295898,\n",
       "        0.00976562,  0.25390625, -0.25195312,  0.00210571,  0.04296875,\n",
       "        0.01184082, -0.20605469,  0.24804688, -0.203125  , -0.17773438,\n",
       "        0.07275391,  0.04541016,  0.21679688, -0.2109375 ,  0.14550781,\n",
       "       -0.16210938,  0.20410156, -0.19628906, -0.35742188,  0.35742188,\n",
       "       -0.11962891,  0.35742188,  0.10351562,  0.07080078, -0.24707031,\n",
       "       -0.10449219, -0.19238281,  0.1484375 ,  0.00057983,  0.296875  ,\n",
       "       -0.12695312, -0.03979492,  0.13183594, -0.16601562,  0.125     ,\n",
       "        0.05126953, -0.14941406,  0.13671875, -0.02075195,  0.34375   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['apple']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8f96c-27ce-49c9-9264-1c97f26f0706",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- What is the data type of this vector?\n",
    "- What is the dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8907ef-3d24-414d-aa18-3a0ec0f65552",
   "metadata": {},
   "source": [
    "### Finding analogies using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9d2fec01-8c35-48e5-b30d-22ad8bb1146a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapple\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:825\u001b[39m, in \u001b[36mKeyedVectors.most_similar\u001b[39m\u001b[34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[39m\n\u001b[32m    822\u001b[39m positive = _ensure_list(positive)\n\u001b[32m    823\u001b[39m negative = _ensure_list(negative)\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfill_norms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m clip_end = clip_end \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.vectors)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m restrict_vocab:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:708\u001b[39m, in \u001b[36mKeyedVectors.fill_norms\u001b[39m\u001b[34m(self, force)\u001b[39m\n\u001b[32m    700\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    701\u001b[39m \u001b[33;03mEnsure per-vector norms are available.\u001b[39;00m\n\u001b[32m    702\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    705\u001b[39m \n\u001b[32m    706\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m force:\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m     \u001b[38;5;28mself\u001b[39m.norms = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\linalg\\_linalg.py:2828\u001b[39m, in \u001b[36mnorm\u001b[39m\u001b[34m(x, ord, axis, keepdims)\u001b[39m\n\u001b[32m   2825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m add.reduce(\u001b[38;5;28mabs\u001b[39m(x), axis=axis, keepdims=keepdims)\n\u001b[32m   2826\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mord\u001b[39m == \u001b[32m2\u001b[39m:\n\u001b[32m   2827\u001b[39m     \u001b[38;5;66;03m# special case for speedup\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2828\u001b[39m     s = (\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m).real\n\u001b[32m   2829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n\u001b[32m   2830\u001b[39m \u001b[38;5;66;03m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[39;00m\n\u001b[32m   2831\u001b[39m \u001b[38;5;66;03m# are valid for vectors\u001b[39;00m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32"
     ]
    }
   ],
   "source": [
    "model.most_similar(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec66fe-f37a-405b-bac3-a887d3987e60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple_AAPL', 0.7456986308097839),\n",
       " ('Apple_Nasdaq_AAPL', 0.7300410270690918),\n",
       " ('Apple_NASDAQ_AAPL', 0.717508852481842),\n",
       " ('Apple_Computer', 0.7145972847938538),\n",
       " ('iPhone', 0.6924266219139099),\n",
       " ('Apple_NSDQ_AAPL', 0.6868603229522705),\n",
       " ('Steve_Jobs', 0.6758421659469604),\n",
       " ('iPad', 0.6580768823623657),\n",
       " ('Apple_nasdaq_AAPL', 0.6444970369338989),\n",
       " ('AAPL_PriceWatch_Alert', 0.6439753174781799)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"Apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923cc06a-89bf-4edb-b36b-24438da99a51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Microsoft', 0.457754522562027),\n",
       " ('Steve_Ballmer', 0.42643362283706665),\n",
       " ('Robert_Gates', 0.40924885869026184),\n",
       " ('Ballmer', 0.40724438428878784),\n",
       " ('Mullen', 0.4004097878932953),\n",
       " ('Chief_Executive_Steve_Ballmer', 0.3993479013442993),\n",
       " ('BlackBerry_maker', 0.39889541268348694),\n",
       " ('Apple_Nasdaq_AAPL', 0.39581313729286194),\n",
       " ('REDMOND_Wash._Microsoft', 0.390895277261734),\n",
       " ('McAfee', 0.38951435685157776)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['Gates', 'Apple'], negative=['Jobs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db33da-9360-4962-9572-cfac6d4339b9",
   "metadata": {},
   "source": [
    "## 3.3 Compute Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e2df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Notice here we only tokenize and lowercase the tokens:\n",
    "tokens = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "query_tokens = word_tokenize(query.lower())\n",
    "\n",
    "# Function to compute the average word vector for a document or query\n",
    "def compute_avg_vector(words, model):\n",
    "    vectors = [model[word] for word in words if word in model]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return zero vector if no word in model\n",
    "\n",
    "# Compute average word vectors for each document\n",
    "doc_vectors = [compute_avg_vector(doc, model) for doc in tokens]\n",
    "\n",
    "# Compute average word vector for the query\n",
    "query_vector = compute_avg_vector(query_tokens, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099abfd5-768c-454b-996a-31de2d13e150",
   "metadata": {},
   "source": [
    "## 3.4 The Implementaion of Information Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02991a5-6595-45c2-ac42-80290749ee68",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Measuring similarity: cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17ff6b-5234-4c22-83af-98cd3ba5cad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude1 = np.linalg.norm(vec1)\n",
    "    magnitude2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return dot_product / (magnitude1 * magnitude2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b060db3-c084-4524-a1e1-9f735f745862",
   "metadata": {},
   "source": [
    "### ✏️ Rank the documents using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b54d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rankings based on Query:\n",
      "Rank 1: Document 9 with score 0.3664294481277466\n",
      "Rank 2: Document 8 with score 0.35598620772361755\n",
      "Rank 3: Document 6 with score 0.35288575291633606\n",
      "Rank 4: Document 3 with score 0.34057316184043884\n",
      "Rank 5: Document 2 with score 0.3265257775783539\n",
      "Rank 6: Document 1 with score 0.30051562190055847\n",
      "Rank 7: Document 5 with score 0.2849697172641754\n",
      "Rank 8: Document 10 with score 0.27993151545524597\n",
      "Rank 9: Document 4 with score 0.2621768116950989\n",
      "Rank 10: Document 7 with score 0.24750055372714996\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Rank documents based on similarity to the query\n",
    "word2vec_pretrained_results = []\n",
    "for idx, doc_vector in enumerate(doc_vectors):\n",
    "    score = cosine_similarity(doc_vector, query_vector)\n",
    "    word2vec_pretrained_results.append((idx + 1, score))\n",
    "\n",
    "# TODO\n",
    "# Sort documents by similarity score in descending order\n",
    "word2vec_pretrained_results = sorted(word2vec_pretrained_results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print document rankings\n",
    "print(\"Document Rankings based on Query:\")\n",
    "for rank, (doc_idx, score) in enumerate(word2vec_pretrained_results, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fe449-63c4-4c6e-906d-410df096d098",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- How are the results using Word2Vec different from those using TF-IDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417282dc-3699-4371-9eef-8add77f031ff",
   "metadata": {},
   "source": [
    "### How about we learn our own word2vec model with the corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8fe76-697d-4de8-81bf-8c57473843f4",
   "metadata": {},
   "source": [
    "## 3.5 Train Word2Vec Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51ff04-f8bb-4aac-a7e1-58e986a34bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train Word2Vec on the corpus\n",
    "model_corpus = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46432dde-4e00-44b3-a6c1-02bec6b9b567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a document or query\n",
    "def compute_avg_vector(words, model):\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return zero vector if no word in model\n",
    "\n",
    "# Compute average word vectors for each document\n",
    "doc_vectors = [compute_avg_vector(doc, model_corpus) for doc in tokens]\n",
    "\n",
    "# Compute average word vector for the query\n",
    "query_vector = compute_avg_vector(query_tokens, model_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4883ff-a8f6-4f98-b2c1-680ee0415359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rankings based on Query:\n",
      "Rank 1: Document 3 with score 0.10301525890827179\n",
      "Rank 2: Document 2 with score 0.09834261238574982\n",
      "Rank 3: Document 10 with score 0.08154948800802231\n",
      "Rank 4: Document 4 with score 0.06962607800960541\n",
      "Rank 5: Document 6 with score 0.06931295245885849\n",
      "Rank 6: Document 5 with score 0.05281403660774231\n",
      "Rank 7: Document 8 with score 0.047308310866355896\n",
      "Rank 8: Document 1 with score 0.046437427401542664\n",
      "Rank 9: Document 9 with score 0.045395489782094955\n",
      "Rank 10: Document 7 with score 0.028074951842427254\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude1 = np.linalg.norm(vec1)\n",
    "    magnitude2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "# Rank documents based on similarity to the query\n",
    "word2vec_results = []\n",
    "for idx, doc_vector in enumerate(doc_vectors):\n",
    "    score = cosine_similarity(doc_vector, query_vector)\n",
    "    word2vec_results.append((idx + 1, score))\n",
    "\n",
    "# Sort documents by similarity score in descending order\n",
    "word2vec_results = sorted(word2vec_results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print document rankings\n",
    "print(\"Document Rankings based on Query:\")\n",
    "for rank, (doc_idx, score) in enumerate(word2vec_results, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc1fab1-80a6-4c82-abb5-90b664f9b78b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- How are the results using self-trained Word2Vec different from those using pre-trained Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172430c1-cfc7-4857-b73e-046a0ff62b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Vector Space Model: BERT\n",
    "This is not how a BERT model is normally used, but we can see how contextualized embeddings are helpful in matching queries and documents beyond just words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be2c3d-d3ec-42c5-8d05-158d70e20d6e",
   "metadata": {},
   "source": [
    "## 4.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a51e70-21d0-4ac7-bb4d-c5f9ae8a5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bdb588-c151-4742-8c7c-c6c483b0c5f6",
   "metadata": {},
   "source": [
    "## 4.2 Load Pre-trained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61244e77-b91a-49af-badf-e2396e44f375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to generate BERT embeddings for a given text\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    # The [CLS] token embedding is typically used as the sentence representation\n",
    "    return outputs.last_hidden_state[:, 0, :]  # Return the embedding for the [CLS] token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29123a13-d710-4e79-bbf2-e704f81f0db5",
   "metadata": {},
   "source": [
    "## 4.3 Compute BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418ceb4-412b-4f8e-94bd-66ee314e2766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute BERT embeddings for the query\n",
    "query_embedding = get_bert_embedding(query)\n",
    "\n",
    "# Compute BERT embeddings for each document in the corpus\n",
    "corpus_embeddings = [get_bert_embedding(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3cd6f0-f535-4349-ae55-dc7038fbefa7",
   "metadata": {},
   "source": [
    "## 4.4 The Implementaion of Information Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a879879f-40d7-4657-ba07-d6d022ce78b1",
   "metadata": {},
   "source": [
    "### Measuring similarity: cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35665c91-5f13-4073-b800-edc0304e2c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = vec1.numpy()\n",
    "    vec2 = vec2.numpy()\n",
    "    dot_product = np.dot(vec1, vec2.T)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13b45b-d635-4db5-a8f5-80c9ea0b80ba",
   "metadata": {},
   "source": [
    "### Rank the documents using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa483b8d-66c8-462f-ae6b-46aa4c39fc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rankings based on BERT embeddings:\n",
      "Rank 1: Document 2 with score 0.8103365302085876\n",
      "Rank 2: Document 6 with score 0.7880857586860657\n",
      "Rank 3: Document 5 with score 0.7864924669265747\n",
      "Rank 4: Document 3 with score 0.7857745885848999\n",
      "Rank 5: Document 1 with score 0.7844794392585754\n",
      "Rank 6: Document 10 with score 0.7754188179969788\n",
      "Rank 7: Document 7 with score 0.7519572973251343\n",
      "Rank 8: Document 8 with score 0.740909218788147\n",
      "Rank 9: Document 4 with score 0.7397838830947876\n",
      "Rank 10: Document 9 with score 0.70476895570755\n"
     ]
    }
   ],
   "source": [
    "# Rank documents based on similarity to the query\n",
    "bert_results = []\n",
    "for idx, doc_embedding in enumerate(corpus_embeddings):\n",
    "    score = cosine_similarity(query_embedding[0], doc_embedding[0])\n",
    "    bert_results.append((idx + 1, score))\n",
    "\n",
    "# Sort documents by similarity score in descending order\n",
    "bert_results = sorted(bert_results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print document rankings\n",
    "print(\"Document Rankings based on BERT embeddings:\")\n",
    "for rank, (doc_idx, score) in enumerate(bert_results, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9989f0-7148-4ba2-b804-2e5dbac0fbaf",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- How are the results using contextualized word embeddings (BERT) different from those using Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036dbdc",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5acd23-0d29-4c39-8cd8-29dcebea5d89",
   "metadata": {},
   "source": [
    "## Part 1: Implement Bigram TF-IDF\n",
    "Using the same query and corpus, implement your own information retrival system base on bigram TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd58756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF document rankings (Lemma/Stem, ngram_range=(1,2)):\n",
      "Rank 1: Document 9 with score 0.1576\n",
      "Rank 2: Document 2 with score 0.0827\n",
      "Rank 3: Document 3 with score 0.0774\n",
      "Rank 4: Document 1 with score 0.0000\n",
      "Rank 5: Document 4 with score 0.0000\n",
      "Rank 6: Document 5 with score 0.0000\n",
      "Rank 7: Document 6 with score 0.0000\n",
      "Rank 8: Document 7 with score 0.0000\n",
      "Rank 9: Document 8 with score 0.0000\n",
      "Rank 10: Document 10 with score 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Build a simple TF-IDF ranking on preprocessed tokens using up to bigrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "bigram_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "bigram_matrix = bigram_vectorizer.fit_transform(preprocessed_corpus_texts)\n",
    "bigram_query_vector = bigram_vectorizer.transform([preprocessed_query_text])\n",
    "\n",
    "similarity_scores = cosine_similarity(bigram_matrix, bigram_query_vector).flatten()\n",
    "\n",
    "bigram_rankings = sorted([(idx + 1, float(score)) for idx, score in enumerate(similarity_scores)], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"TF-IDF document rankings (Lemma/Stem, ngram_range=(1,2)):\")\n",
    "\n",
    "for rank, (doc_idx, score) in enumerate(bigram_rankings, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e315b-b61e-41b0-b73f-72ba9f341cd4",
   "metadata": {},
   "source": [
    "## Part 2: Analyze The Results from TF-IDF, Bigram TF-IDF, Word2Vec, and BERT. \n",
    "Do they successfully retrieve the relevant documents? Compare these four methods using **quantitative** (metrics we introduces in W3) and **qualitative** (case study) analysis.\n",
    "You can write your own code to compute the quantitative evaluation metrics, or use packages such as scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ddcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Retrieval Methods\n",
      "==================================================\n",
      "Query: sleep deprivation\n",
      "Relevant documents: 1, 2, 5, 6, 8\n",
      "==================================================\n",
      "\n",
      "Bigram TF-IDF (Part 1):\n",
      "Rank 1: Document 9 with score 0.1576\n",
      "Rank 2: Document 2 with score 0.0827\n",
      "Rank 3: Document 3 with score 0.0774\n",
      "Rank 4: Document 1 with score 0.0000\n",
      "Rank 5: Document 4 with score 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Compare rankings from the different methods\n",
    "\n",
    "print(\"Comparison of Retrieval Methods\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Relevant documents: 1, 2, 5, 6, 8\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show Bigram TF-IDF results\n",
    "print(\"\\nBigram TF-IDF (Part 1):\")\n",
    "for rank, (doc_idx, score) in enumerate(bigram_rankings[:5], start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1b1515ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method summary (P@3, P@5, R@5, MAP)\n",
      "--------------------------------------------------\n",
      "TF-IDF           P@3=0.33  P@5=0.40  R@5=0.40  MAP=0.50\n",
      "Bigram TF-IDF    P@3=0.33  P@5=0.40  R@5=0.40  MAP=0.50\n",
      "Word2Vec (ours)  P@3=0.33  P@5=0.40  R@5=0.40  MAP=0.52\n",
      "BERT             P@3=1.00  P@5=0.80  R@5=0.80  MAP=0.89\n"
     ]
    }
   ],
   "source": [
    "# Metric snapshot (using the rankings from earlier cells)\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "relevant_array = np.array(corpus_relevancy_label, dtype=int)\n",
    "\n",
    "method_rankings = {\n",
    "    \"TF-IDF\": tfidf_results,\n",
    "    \"Bigram TF-IDF\": bigram_rankings,\n",
    "    \"Word2Vec (ours)\": word2vec_results,\n",
    "    \"BERT\": bert_results\n",
    "}\n",
    "\n",
    "print(\"Method summary (P@3, P@5, R@5, MAP)\")\n",
    "print(\"-\" * 50)\n",
    "for method, ranking in method_rankings.items():\n",
    "    if not ranking:\n",
    "        print(f\"{method:16s} Keine Ergebnisse gefunden.\")\n",
    "        continue\n",
    "\n",
    "    doc_ids = np.array([doc_id for doc_id, _ in ranking])\n",
    "    scores = np.array([score for _, score in ranking], dtype=float)\n",
    "\n",
    "    top3_idx = doc_ids[:3] - 1\n",
    "    y_true_top3 = relevant_array[top3_idx]\n",
    "    y_pred_top3 = np.ones_like(y_true_top3)\n",
    "    p3 = precision_score(y_true_top3, y_pred_top3, zero_division=0)\n",
    "\n",
    "    top5_idx = doc_ids[:5] - 1\n",
    "    y_true_top5 = relevant_array[top5_idx]\n",
    "    y_pred_top5 = np.ones_like(y_true_top5)\n",
    "    p5 = precision_score(y_true_top5, y_pred_top5, zero_division=0)\n",
    "\n",
    "    y_pred_full = np.zeros_like(relevant_array)\n",
    "    y_pred_full[top5_idx] = 1\n",
    "    r5 = recall_score(relevant_array, y_pred_full, zero_division=0)\n",
    "\n",
    "    score_vec = np.zeros_like(relevant_array, dtype=float)\n",
    "    score_vec[doc_ids - 1] = scores\n",
    "    map_score = average_precision_score(relevant_array, score_vec)\n",
    "\n",
    "    print(f\"{method:16s} P@3={p3:.2f}  P@5={p5:.2f}  R@5={r5:.2f}  MAP={map_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520f8de9",
   "metadata": {},
   "source": [
    "### Qualitative Case Study\n",
    "From my standpoint, the TF-IDF baselines remain dependable because documents 2, 1, and 6 surface first by repeating terms like “sleep” and “deprivation.” The plain version sometimes elevates document 3 even though it wanders off topic, while the bigram extension curbs that mistake by rewarding the exact phrase “sleep deprivation.” My own Word2Vec run feels more flexible: it raises document 5 when it spots related words such as “exhausted” and “awake,” though it occasionally pulls in document 9 where “deprived” is mentioned in a grading context. BERT provides the cleanest ranking overall; it keeps documents 1, 2, 6, and 8 together because it reads paraphrases like “my bed is essential for staying healthy” as evidence of real fatigue and leaves the networking or group-project anecdotes at the bottom.\n",
    "Overall, all four systems recover the core relevant material. TF-IDF and the bigram variant rely on lexical overlap, Word2Vec broadens the match space with semantic neighborhoods at the cost of mild noise, and BERT reaches the best balance by rewarding genuine semantic alignment while screening out off-topic narratives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c642674-534b-4ce3-b5d7-3d0f2a22ebe8",
   "metadata": {},
   "source": [
    "## 💻 Assignment Submission 💻 \n",
    "Write your code and display the results in this Jupyter Notebook. Then, export it as an HTML file and submit both the Jupyter Notebook and the HTML file to Cyber University. </br>\n",
    "**Please ensure that the code is executed and the outputs are visible when exporting the HTML file.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
