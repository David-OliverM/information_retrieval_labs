{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18117881-1575-4349-95ba-9bc6b2cea328",
   "metadata": {},
   "source": [
    "# W6 Lab Exercise\n",
    "This is the lab exercise for MIS590: Information Retrieval. </br>\n",
    "In this lab, you will gain the following experience:</br>\n",
    "- Understand Vector Space Models (VSMs) for Information Retrieval.\n",
    "- Develop Practical Skills in Vector-Based Document Representation, Including TF-IDF, Word2Vec, and BERT.\n",
    "- Compare the Effectiveness of Different Term Weighting Schemes.\n",
    "- Enhance Analytical Thinking in Evaluating IR Models\n",
    "</br>\n",
    "\n",
    "**Note:** When you see a pencil icon ✏️ in this notebook, it's time for you to code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffaa68b",
   "metadata": {},
   "source": [
    "# 1. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ef730",
   "metadata": {},
   "source": [
    "## 1.1 Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0bae653b-f98f-4f96-826b-e8fdc579a542",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: torch in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: torch in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: numpy in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: gensim in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from gensim) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: wrapt in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n",
      "Requirement already satisfied: gensim in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from gensim) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: wrapt in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement string (from versions: none)\n",
      "ERROR: No matching distribution found for string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: scikit-learn in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\users\\dmatz\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary packages\n",
    "!pip install nltk\n",
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install gensim\n",
    "!pip install string\n",
    "!pip install transformers\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53e49647",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecece4d5",
   "metadata": {},
   "source": [
    "## 1.2 Input: Query & Document Collections (Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0ce3b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"sleep deprivation\"\n",
    "\n",
    "corpus = [\n",
    "    \"Sleepless nights in the lab have become my new normal. I tried to fix the experiment setup, but the apparatus seems to have a mind of its own. My advisor says results are just around the corner, but the corner keeps moving. Coffee is my only true companion these days.\",\n",
    "    \"I thought grad school would be intellectually stimulating, but it's mostly paperwork and waiting for emails. The departmental printer jammed again, and now I'm late for a meeting. The cafeteria ran out of the good snacks, so I'm surviving on vending machine chips. Sleep has become a luxury I can no longer afford.\",\n",
    "    \"Writing the dissertation feels like climbing an endless mountain. Every time I finish a chapter, my supervisor suggests new revisions. The impostor syndrome is real, and I wonder if they made a mistake accepting me. Maybe I should have gone to clown college instead. I am utterly deprived of any semblance of a normal life.\",\n",
    "    \"My research data got corrupted, and now I have to start over. The lab mouse escaped, and we spent hours trying to find it. The grant proposal deadline is tomorrow, and the online submission portal is down. At least my pet cactus hasn't died yet.\",\n",
    "    \"The group meeting turned into a three-hour debate over font choices for the presentation. I'm pretty sure my colleague is stealing my lunch from the fridge. The photocopier is out to get me; it never works when I'm in a hurry. Is there a PhD in napping? Because I'd ace that.\",\n",
    "    \"I haven't seen the sun in days due to endless coding sessions. The simulation keeps crashing, and Stack Overflow doesn't have the answers. My roommate thinks I'm a ghost haunting the apartment. Instant noodles have become my primary food group.\",\n",
    "    \"Attending conferences sounded fun until I realized they involve a lot of awkward networking. I accidentally spilled coffee on a famous professor's shoes. My poster fell down twice during the session. Next time, I'll just send a cardboard cutout of myself.\",\n",
    "    \"The university gym membership was supposed to keep me healthy, but I've only used it once. I tried to attend a yoga class after staying up late for a deadline, but I fell asleep during the meditation. Maybe instead of the gym, my bed is more essential for keeping me healthy.\",\n",
    "    \"My teaching assistantship involves grading endless stacks of exams. Students keep emailing me for extensions with creative excuses. One claimed their dog sleeps on the laptop so they cannot use it for the exam. I was deprived of excuses for not completing my dissertation draft, and I might have got some good ones.\",\n",
    "    \"Group projects are the worst when you're the only one doing the work. My team members are as elusive as Bigfoot. The project is due next week, and I haven't heard from them. Perhaps I should just write a paper on the sociological implications of group work avoidance.\"\n",
    "]\n",
    "\n",
    "# Binary labels for the documents' relevancy to the query\n",
    "# Relevant ones: 1, 2, 5, 6, 8\n",
    "corpus_relevancy_label = [1, 1, 0, 0, 1, 1, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c614242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: sleep deprivation\n",
      "\n",
      "Document 1:\n",
      "Sleepless nights in the lab have become my new normal. I tried to fix the experiment setup, but the apparatus seems to have a mind of its own. My advisor says results are just around the corner, but the corner keeps moving. Coffee is my only true companion these days.\n",
      "\n",
      "Document 2:\n",
      "I thought grad school would be intellectually stimulating, but it's mostly paperwork and waiting for emails. The departmental printer jammed again, and now I'm late for a meeting. The cafeteria ran out of the good snacks, so I'm surviving on vending machine chips. Sleep has become a luxury I can no longer afford.\n",
      "\n",
      "Document 3:\n",
      "Writing the dissertation feels like climbing an endless mountain. Every time I finish a chapter, my supervisor suggests new revisions. The impostor syndrome is real, and I wonder if they made a mistake accepting me. Maybe I should have gone to clown college instead. I am utterly deprived of any semblance of a normal life.\n",
      "\n",
      "Document 4:\n",
      "My research data got corrupted, and now I have to start over. The lab mouse escaped, and we spent hours trying to find it. The grant proposal deadline is tomorrow, and the online submission portal is down. At least my pet cactus hasn't died yet.\n",
      "\n",
      "Document 5:\n",
      "The group meeting turned into a three-hour debate over font choices for the presentation. I'm pretty sure my colleague is stealing my lunch from the fridge. The photocopier is out to get me; it never works when I'm in a hurry. Is there a PhD in napping? Because I'd ace that.\n",
      "\n",
      "Document 6:\n",
      "I haven't seen the sun in days due to endless coding sessions. The simulation keeps crashing, and Stack Overflow doesn't have the answers. My roommate thinks I'm a ghost haunting the apartment. Instant noodles have become my primary food group.\n",
      "\n",
      "Document 7:\n",
      "Attending conferences sounded fun until I realized they involve a lot of awkward networking. I accidentally spilled coffee on a famous professor's shoes. My poster fell down twice during the session. Next time, I'll just send a cardboard cutout of myself.\n",
      "\n",
      "Document 8:\n",
      "The university gym membership was supposed to keep me healthy, but I've only used it once. I tried to attend a yoga class after staying up late for a deadline, but I fell asleep during the meditation. Maybe instead of the gym, my bed is more essential for keeping me healthy.\n",
      "\n",
      "Document 9:\n",
      "My teaching assistantship involves grading endless stacks of exams. Students keep emailing me for extensions with creative excuses. One claimed their dog sleeps on the laptop so they cannot use it for the exam. I was deprived of excuses for not completing my dissertation draft, and I might have got some good ones.\n",
      "\n",
      "Document 10:\n",
      "Group projects are the worst when you're the only one doing the work. My team members are as elusive as Bigfoot. The project is due next week, and I haven't heard from them. Perhaps I should just write a paper on the sociological implications of group work avoidance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {query}\\n\")\n",
    "for idx, doc in enumerate(corpus):\n",
    "    print(f\"Document {idx+1}:\\n{doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16ff55-2eb7-41d7-839d-73bb5272d04f",
   "metadata": {},
   "source": [
    "# 2. Vector Space Model: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3088e0",
   "metadata": {},
   "source": [
    "## 2.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c747d5",
   "metadata": {},
   "source": [
    "### Steps for textual data preprocessing\n",
    "1. Tokenization (= word segmentation)\n",
    "2. Punctualtion and non-alphabetic token removal\n",
    "3. Stopwords removal\n",
    "4. Lemmatization / stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c16eb-1361-4977-a248-dd636a79fb5d",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30b23131-7e52-4097-8978-dd5b999ab25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dmatz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dmatz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dmatz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dmatz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "538cd901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentence:\n",
      "The graduate student was typing, procrastinating, questioning herself, and finally submitting the dissertation while dreaming about sleep.\n"
     ]
    }
   ],
   "source": [
    "# Initialize stopwords, lemmatizer, and punctuation list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# We will use this sentence as example to showcase the different steps of data preprocessing\n",
    "example_sentence = \"The graduate student was typing, procrastinating, questioning herself, and finally submitting the dissertation while dreaming about sleep.\"\n",
    "print(f\"Example Sentence:\\n{example_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42202e7",
   "metadata": {},
   "source": [
    "### What is tokenization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0eb5ba16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'graduate', 'student', 'was', 'typing', ',', 'procrastinating', ',', 'questioning', 'herself', ',', 'and', 'finally', 'submitting', 'the', 'dissertation', 'while', 'dreaming', 'about', 'sleep', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(example_sentence.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68961a99",
   "metadata": {},
   "source": [
    "### A quick removal of punctualtions and non-alphabetic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77b14e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'graduate', 'student', 'was', 'typing', 'procrastinating', 'questioning', 'herself', 'and', 'finally', 'submitting', 'the', 'dissertation', 'while', 'dreaming', 'about', 'sleep']\n"
     ]
    }
   ],
   "source": [
    "tokens_noPunc = [word.translate(punctuation_table) for word in tokens if word.isalpha()]\n",
    "print(tokens_noPunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8030b",
   "metadata": {},
   "source": [
    "### What are stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "906e138c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['graduate', 'student', 'typing', 'procrastinating', 'questioning', 'finally', 'submitting', 'dissertation', 'dreaming', 'sleep']\n"
     ]
    }
   ],
   "source": [
    "tokens_noSW = [word for word in tokens_noPunc if word not in stop_words]\n",
    "print(tokens_noSW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b25394",
   "metadata": {},
   "source": [
    "### What is lemmatization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6427e0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\tLemmatized\n",
      "\n",
      "graduate\tgraduate\n",
      "student\tstudent\n",
      "typing\ttyping\n",
      "procrastinating\tprocrastinating\n",
      "questioning\tquestioning\n",
      "finally\tfinally\n",
      "submitting\tsubmitting\n",
      "dissertation\tdissertation\n",
      "dreaming\tdreaming\n",
      "sleep\tsleep\n"
     ]
    }
   ],
   "source": [
    "print(\"Original\\tLemmatized\\n\")\n",
    "\n",
    "# Here we use pre-stopword removal tokens\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens_noSW]\n",
    "for ori, lem in zip(tokens_noSW, lemmatized_tokens):\n",
    "    print(f\"{ori}\\t{lem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0671810",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- What is lemmatization?\n",
    "- I guess you cannot tell what lemmatization is from the results above. Let's try lemmatization in another way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c21d4",
   "metadata": {},
   "source": [
    "### How about we tell the lemmatizer more information of the tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21a94a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('graduate', 'NN'), ('student', 'NN'), ('was', 'VBD'), ('typing', 'VBG'), (',', ','), ('procrastinating', 'VBG'), (',', ','), ('questioning', 'VBG'), ('herself', 'PRP'), (',', ','), ('and', 'CC'), ('finally', 'RB'), ('submitting', 'VBG'), ('the', 'DT'), ('dissertation', 'NN'), ('while', 'IN'), ('dreaming', 'VBG'), ('about', 'RB'), ('sleep', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Part-of-Speech Tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4205ac6",
   "metadata": {},
   "source": [
    "### Then we do the punctuation, non-alphabetic tokens, and stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6af0e29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('graduate', 'NN'), ('student', 'NN'), ('was', 'VBD'), ('typing', 'VBG'), ('procrastinating', 'VBG'), ('questioning', 'VBG'), ('herself', 'PRP'), ('and', 'CC'), ('finally', 'RB'), ('submitting', 'VBG'), ('the', 'DT'), ('dissertation', 'NN'), ('while', 'IN'), ('dreaming', 'VBG'), ('about', 'RB'), ('sleep', 'NN')]\n",
      "[('graduate', 'NN'), ('student', 'NN'), ('typing', 'VBG'), ('procrastinating', 'VBG'), ('questioning', 'VBG'), ('finally', 'RB'), ('submitting', 'VBG'), ('dissertation', 'NN'), ('dreaming', 'VBG'), ('sleep', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation and non-alphabetic tokens\n",
    "tagged_tokens_noPunc = [(word[0].translate(punctuation_table), word[1]) for word in tagged_tokens if word[0].isalpha()]\n",
    "print(tagged_tokens_noPunc)\n",
    "\n",
    "# Remove stopwords\n",
    "tagged_tokens_noSW = [(word[0], word[1]) for word in tagged_tokens_noPunc if word[0] not in stop_words]\n",
    "print(tagged_tokens_noSW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9ffbf",
   "metadata": {},
   "source": [
    "### Take 2: what is lemmatization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1f708fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\tLemmatized\n",
      "\n",
      "graduate\tgraduate\n",
      "student\tstudent\n",
      "typing\ttype\n",
      "procrastinating\tprocrastinate\n",
      "questioning\tquestion\n",
      "finally\tfinally\n",
      "submitting\tsubmit\n",
      "dissertation\tdissertation\n",
      "dreaming\tdream\n",
      "sleep\tsleep\n"
     ]
    }
   ],
   "source": [
    "# Convert treebank POS tags to wordnet POS tags so the lemmatizer can read them\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN\n",
    "\n",
    "print(\"Original\\tLemmatized\\n\")\n",
    "tagged_tokens_lemmatized = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens_noSW]\n",
    "for ori, lem in zip(tagged_tokens_noSW, tagged_tokens_lemmatized):\n",
    "    print(f\"{ori[0]}\\t{lem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33e82c",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- What is lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4fb82",
   "metadata": {},
   "source": [
    "### What is stemming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a2edec23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\tStemmed\n",
      "\n",
      "graduate\tgraduat\n",
      "student\tstudent\n",
      "type\ttype\n",
      "procrastinate\tprocrastin\n",
      "question\tquestion\n",
      "finally\tfinal\n",
      "submit\tsubmit\n",
      "dissertation\tdissert\n",
      "dream\tdream\n",
      "sleep\tsleep\n"
     ]
    }
   ],
   "source": [
    "print(\"Original\\tStemmed\\n\")\n",
    "tokens_stemmed = [stemmer.stem(word) for word in tagged_tokens_lemmatized]\n",
    "for ori, stem in zip(tagged_tokens_lemmatized, tokens_stemmed):\n",
    "    print(f\"{ori}\\t{stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2efd5",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- What is stemming?\n",
    "- Why is stemming helpful in imporving TF-IDF performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9536305",
   "metadata": {},
   "source": [
    "### ✏️ Now let's preprocess the query and the documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9b3c6fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Step 1: # Convert to lowercase and tokenize text into words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Step 2: Tag part-of-speech of the tokens\n",
    "    tokens = pos_tag(tokens)\n",
    "    \n",
    "    # Step 3: Remove punctuation and non-alphabetic tokens\n",
    "    tokens = [(word[0].translate(punctuation_table), word[1]) for word in tokens if word[0].isalpha()]\n",
    "    \n",
    "    # Step 4: Remove stopwords\n",
    "    tokens = [(word[0], word[1]) for word in tokens if word[0] not in stop_words]\n",
    "    \n",
    "    # Step 5: Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tokens]\n",
    "    \n",
    "    # Step 6: Stem tokens\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d78d7fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: ['sleep', 'depriv']\n",
      "\n",
      "Document 1: ['sleepless', 'night', 'lab', 'becom', 'new', 'normal', 'tri', 'fix', 'experi', 'setup', 'apparatu', 'seem', 'mind', 'advisor', 'say', 'result', 'around', 'corner', 'corner', 'keep', 'move', 'coffe', 'true', 'companion', 'day']\n",
      "Document 2: ['think', 'grad', 'school', 'would', 'intellectu', 'stimul', 'mostli', 'paperwork', 'wait', 'email', 'department', 'printer', 'jam', 'late', 'meet', 'cafeteria', 'run', 'good', 'snack', 'surviv', 'vend', 'machin', 'chip', 'sleep', 'becom', 'luxuri', 'longer', 'afford']\n",
      "Document 3: ['write', 'dissert', 'feel', 'like', 'climb', 'endless', 'mountain', 'everi', 'time', 'finish', 'chapter', 'supervisor', 'suggest', 'new', 'revis', 'impostor', 'syndrom', 'real', 'wonder', 'make', 'mistak', 'accept', 'mayb', 'go', 'clown', 'colleg', 'instead', 'utterli', 'depriv', 'semblanc', 'normal', 'life']\n",
      "Document 4: ['research', 'data', 'get', 'corrupt', 'start', 'lab', 'mous', 'escap', 'spend', 'hour', 'tri', 'find', 'grant', 'propos', 'deadlin', 'tomorrow', 'onlin', 'submiss', 'portal', 'least', 'pet', 'cactu', 'die', 'yet']\n",
      "Document 5: ['group', 'meet', 'turn', 'debat', 'font', 'choic', 'present', 'pretti', 'sure', 'colleagu', 'steal', 'lunch', 'fridg', 'photocopi', 'get', 'never', 'work', 'hurri', 'phd', 'nap', 'ace']\n",
      "Document 6: ['see', 'sun', 'day', 'due', 'endless', 'cod', 'session', 'simul', 'keep', 'crash', 'stack', 'overflow', 'answer', 'roommat', 'think', 'ghost', 'haunt', 'apart', 'instant', 'noodl', 'becom', 'primari', 'food', 'group']\n",
      "Document 7: ['attend', 'confer', 'sound', 'fun', 'realiz', 'involv', 'lot', 'awkward', 'network', 'accident', 'spill', 'coffe', 'famou', 'professor', 'shoe', 'poster', 'fell', 'twice', 'session', 'next', 'time', 'send', 'cardboard', 'cutout']\n",
      "Document 8: ['univers', 'gym', 'membership', 'suppos', 'keep', 'healthi', 'use', 'tri', 'attend', 'yoga', 'class', 'stay', 'late', 'deadlin', 'fell', 'asleep', 'medit', 'mayb', 'instead', 'gym', 'bed', 'essenti', 'keep', 'healthi']\n",
      "Document 9: ['teach', 'assistantship', 'involv', 'grade', 'endless', 'stack', 'exam', 'student', 'keep', 'email', 'extens', 'creativ', 'excus', 'one', 'claim', 'dog', 'sleep', 'laptop', 'use', 'exam', 'depriv', 'excus', 'complet', 'dissert', 'draft', 'might', 'get', 'good', 'one']\n",
      "Document 10: ['group', 'project', 'bad', 'one', 'work', 'team', 'member', 'elus', 'bigfoot', 'project', 'due', 'next', 'week', 'hear', 'perhap', 'write', 'paper', 'sociolog', 'implic', 'group', 'work', 'avoid']\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to each document in the corpus\n",
    "preprocessed_query = preprocess_text(query)\n",
    "print(f\"Query: {preprocessed_query}\\n\")\n",
    "\n",
    "preprocessed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "# Print preprocessed corpus\n",
    "for idx, doc in enumerate(preprocessed_corpus):\n",
    "    print(f\"Document {idx+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544cea0b",
   "metadata": {},
   "source": [
    "## ✏️ 2.2 Compute Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "874163f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF for Document 1: {'sleepless': 0.04, 'night': 0.04, 'lab': 0.04, 'becom': 0.04, 'new': 0.04, 'normal': 0.04, 'tri': 0.04, 'fix': 0.04, 'experi': 0.04, 'setup': 0.04, 'apparatu': 0.04, 'seem': 0.04, 'mind': 0.04, 'advisor': 0.04, 'say': 0.04, 'result': 0.04, 'around': 0.04, 'corner': 0.08, 'keep': 0.04, 'move': 0.04, 'coffe': 0.04, 'true': 0.04, 'companion': 0.04, 'day': 0.04}\n",
      "\n",
      "TF for Document 2: {'think': 0.03571428571428571, 'grad': 0.03571428571428571, 'school': 0.03571428571428571, 'would': 0.03571428571428571, 'intellectu': 0.03571428571428571, 'stimul': 0.03571428571428571, 'mostli': 0.03571428571428571, 'paperwork': 0.03571428571428571, 'wait': 0.03571428571428571, 'email': 0.03571428571428571, 'department': 0.03571428571428571, 'printer': 0.03571428571428571, 'jam': 0.03571428571428571, 'late': 0.03571428571428571, 'meet': 0.03571428571428571, 'cafeteria': 0.03571428571428571, 'run': 0.03571428571428571, 'good': 0.03571428571428571, 'snack': 0.03571428571428571, 'surviv': 0.03571428571428571, 'vend': 0.03571428571428571, 'machin': 0.03571428571428571, 'chip': 0.03571428571428571, 'sleep': 0.03571428571428571, 'becom': 0.03571428571428571, 'luxuri': 0.03571428571428571, 'longer': 0.03571428571428571, 'afford': 0.03571428571428571}\n",
      "\n",
      "TF for Document 3: {'write': 0.03125, 'dissert': 0.03125, 'feel': 0.03125, 'like': 0.03125, 'climb': 0.03125, 'endless': 0.03125, 'mountain': 0.03125, 'everi': 0.03125, 'time': 0.03125, 'finish': 0.03125, 'chapter': 0.03125, 'supervisor': 0.03125, 'suggest': 0.03125, 'new': 0.03125, 'revis': 0.03125, 'impostor': 0.03125, 'syndrom': 0.03125, 'real': 0.03125, 'wonder': 0.03125, 'make': 0.03125, 'mistak': 0.03125, 'accept': 0.03125, 'mayb': 0.03125, 'go': 0.03125, 'clown': 0.03125, 'colleg': 0.03125, 'instead': 0.03125, 'utterli': 0.03125, 'depriv': 0.03125, 'semblanc': 0.03125, 'normal': 0.03125, 'life': 0.03125}\n",
      "\n",
      "TF for Document 4: {'research': 0.041666666666666664, 'data': 0.041666666666666664, 'get': 0.041666666666666664, 'corrupt': 0.041666666666666664, 'start': 0.041666666666666664, 'lab': 0.041666666666666664, 'mous': 0.041666666666666664, 'escap': 0.041666666666666664, 'spend': 0.041666666666666664, 'hour': 0.041666666666666664, 'tri': 0.041666666666666664, 'find': 0.041666666666666664, 'grant': 0.041666666666666664, 'propos': 0.041666666666666664, 'deadlin': 0.041666666666666664, 'tomorrow': 0.041666666666666664, 'onlin': 0.041666666666666664, 'submiss': 0.041666666666666664, 'portal': 0.041666666666666664, 'least': 0.041666666666666664, 'pet': 0.041666666666666664, 'cactu': 0.041666666666666664, 'die': 0.041666666666666664, 'yet': 0.041666666666666664}\n",
      "\n",
      "TF for Document 5: {'group': 0.047619047619047616, 'meet': 0.047619047619047616, 'turn': 0.047619047619047616, 'debat': 0.047619047619047616, 'font': 0.047619047619047616, 'choic': 0.047619047619047616, 'present': 0.047619047619047616, 'pretti': 0.047619047619047616, 'sure': 0.047619047619047616, 'colleagu': 0.047619047619047616, 'steal': 0.047619047619047616, 'lunch': 0.047619047619047616, 'fridg': 0.047619047619047616, 'photocopi': 0.047619047619047616, 'get': 0.047619047619047616, 'never': 0.047619047619047616, 'work': 0.047619047619047616, 'hurri': 0.047619047619047616, 'phd': 0.047619047619047616, 'nap': 0.047619047619047616, 'ace': 0.047619047619047616}\n",
      "\n",
      "TF for Document 6: {'see': 0.041666666666666664, 'sun': 0.041666666666666664, 'day': 0.041666666666666664, 'due': 0.041666666666666664, 'endless': 0.041666666666666664, 'cod': 0.041666666666666664, 'session': 0.041666666666666664, 'simul': 0.041666666666666664, 'keep': 0.041666666666666664, 'crash': 0.041666666666666664, 'stack': 0.041666666666666664, 'overflow': 0.041666666666666664, 'answer': 0.041666666666666664, 'roommat': 0.041666666666666664, 'think': 0.041666666666666664, 'ghost': 0.041666666666666664, 'haunt': 0.041666666666666664, 'apart': 0.041666666666666664, 'instant': 0.041666666666666664, 'noodl': 0.041666666666666664, 'becom': 0.041666666666666664, 'primari': 0.041666666666666664, 'food': 0.041666666666666664, 'group': 0.041666666666666664}\n",
      "\n",
      "TF for Document 7: {'attend': 0.041666666666666664, 'confer': 0.041666666666666664, 'sound': 0.041666666666666664, 'fun': 0.041666666666666664, 'realiz': 0.041666666666666664, 'involv': 0.041666666666666664, 'lot': 0.041666666666666664, 'awkward': 0.041666666666666664, 'network': 0.041666666666666664, 'accident': 0.041666666666666664, 'spill': 0.041666666666666664, 'coffe': 0.041666666666666664, 'famou': 0.041666666666666664, 'professor': 0.041666666666666664, 'shoe': 0.041666666666666664, 'poster': 0.041666666666666664, 'fell': 0.041666666666666664, 'twice': 0.041666666666666664, 'session': 0.041666666666666664, 'next': 0.041666666666666664, 'time': 0.041666666666666664, 'send': 0.041666666666666664, 'cardboard': 0.041666666666666664, 'cutout': 0.041666666666666664}\n",
      "\n",
      "TF for Document 8: {'univers': 0.041666666666666664, 'gym': 0.08333333333333333, 'membership': 0.041666666666666664, 'suppos': 0.041666666666666664, 'keep': 0.08333333333333333, 'healthi': 0.08333333333333333, 'use': 0.041666666666666664, 'tri': 0.041666666666666664, 'attend': 0.041666666666666664, 'yoga': 0.041666666666666664, 'class': 0.041666666666666664, 'stay': 0.041666666666666664, 'late': 0.041666666666666664, 'deadlin': 0.041666666666666664, 'fell': 0.041666666666666664, 'asleep': 0.041666666666666664, 'medit': 0.041666666666666664, 'mayb': 0.041666666666666664, 'instead': 0.041666666666666664, 'bed': 0.041666666666666664, 'essenti': 0.041666666666666664}\n",
      "\n",
      "TF for Document 9: {'teach': 0.034482758620689655, 'assistantship': 0.034482758620689655, 'involv': 0.034482758620689655, 'grade': 0.034482758620689655, 'endless': 0.034482758620689655, 'stack': 0.034482758620689655, 'exam': 0.06896551724137931, 'student': 0.034482758620689655, 'keep': 0.034482758620689655, 'email': 0.034482758620689655, 'extens': 0.034482758620689655, 'creativ': 0.034482758620689655, 'excus': 0.06896551724137931, 'one': 0.06896551724137931, 'claim': 0.034482758620689655, 'dog': 0.034482758620689655, 'sleep': 0.034482758620689655, 'laptop': 0.034482758620689655, 'use': 0.034482758620689655, 'depriv': 0.034482758620689655, 'complet': 0.034482758620689655, 'dissert': 0.034482758620689655, 'draft': 0.034482758620689655, 'might': 0.034482758620689655, 'get': 0.034482758620689655, 'good': 0.034482758620689655}\n",
      "\n",
      "TF for Document 10: {'group': 0.09090909090909091, 'project': 0.09090909090909091, 'bad': 0.045454545454545456, 'one': 0.045454545454545456, 'work': 0.09090909090909091, 'team': 0.045454545454545456, 'member': 0.045454545454545456, 'elus': 0.045454545454545456, 'bigfoot': 0.045454545454545456, 'due': 0.045454545454545456, 'next': 0.045454545454545456, 'week': 0.045454545454545456, 'hear': 0.045454545454545456, 'perhap': 0.045454545454545456, 'write': 0.045454545454545456, 'paper': 0.045454545454545456, 'sociolog': 0.045454545454545456, 'implic': 0.045454545454545456, 'avoid': 0.045454545454545456}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to compute term frequency (TF) for each document\n",
    "def compute_tf(doc):\n",
    "    \n",
    "    # Initialize the TF dictionary\n",
    "    tf_dict = {}\n",
    "    \n",
    "    # TODO\n",
    "    # Count the term frequency \n",
    "    for word in doc:\n",
    "        tf_dict[word] = tf_dict.get(word, 0) + 1\n",
    "    \n",
    "    # TODO\n",
    "    # Divide term counts by total number of terms in the document\n",
    "    total_terms = len(doc)\n",
    "    for word in tf_dict:\n",
    "        tf_dict[word] = tf_dict[word] / total_terms\n",
    "    \n",
    "    return tf_dict\n",
    "\n",
    "# Compute TF for each document in the corpus\n",
    "tf_corpus = [compute_tf(doc) for doc in preprocessed_corpus]\n",
    "\n",
    "# Print TF values for each document\n",
    "for idx, tf in enumerate(tf_corpus):\n",
    "    print(f\"TF for Document {idx+1}: {tf}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3a5b8",
   "metadata": {},
   "source": [
    "## ✏️ 2.3 Compute Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b72b4c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for Corpus:\n",
      "coffe: 2.6094379124341005\n",
      "keep: 1.916290731874155\n",
      "move: 3.302585092994046\n",
      "seem: 3.302585092994046\n",
      "normal: 2.6094379124341005\n",
      "day: 2.6094379124341005\n",
      "becom: 2.203972804325936\n",
      "companion: 3.302585092994046\n",
      "experi: 3.302585092994046\n",
      "result: 3.302585092994046\n",
      "sleepless: 3.302585092994046\n",
      "setup: 3.302585092994046\n",
      "night: 3.302585092994046\n",
      "corner: 3.302585092994046\n",
      "mind: 3.302585092994046\n",
      "lab: 2.6094379124341005\n",
      "apparatu: 3.302585092994046\n",
      "advisor: 3.302585092994046\n",
      "tri: 2.203972804325936\n",
      "true: 3.302585092994046\n",
      "fix: 3.302585092994046\n",
      "around: 3.302585092994046\n",
      "new: 2.6094379124341005\n",
      "say: 3.302585092994046\n",
      "email: 2.6094379124341005\n",
      "think: 2.6094379124341005\n",
      "chip: 3.302585092994046\n",
      "cafeteria: 3.302585092994046\n",
      "snack: 3.302585092994046\n",
      "run: 3.302585092994046\n",
      "luxuri: 3.302585092994046\n",
      "machin: 3.302585092994046\n",
      "longer: 3.302585092994046\n",
      "sleep: 2.6094379124341005\n",
      "mostli: 3.302585092994046\n",
      "afford: 3.302585092994046\n",
      "department: 3.302585092994046\n",
      "intellectu: 3.302585092994046\n",
      "school: 3.302585092994046\n",
      "stimul: 3.302585092994046\n",
      "good: 2.6094379124341005\n",
      "grad: 3.302585092994046\n",
      "jam: 3.302585092994046\n",
      "would: 3.302585092994046\n",
      "vend: 3.302585092994046\n",
      "late: 2.6094379124341005\n",
      "wait: 3.302585092994046\n",
      "meet: 2.6094379124341005\n",
      "printer: 3.302585092994046\n",
      "paperwork: 3.302585092994046\n",
      "surviv: 3.302585092994046\n",
      "life: 3.302585092994046\n",
      "supervisor: 3.302585092994046\n",
      "like: 3.302585092994046\n",
      "chapter: 3.302585092994046\n",
      "mountain: 3.302585092994046\n",
      "everi: 3.302585092994046\n",
      "go: 3.302585092994046\n",
      "clown: 3.302585092994046\n",
      "finish: 3.302585092994046\n",
      "colleg: 3.302585092994046\n",
      "make: 3.302585092994046\n",
      "climb: 3.302585092994046\n",
      "syndrom: 3.302585092994046\n",
      "semblanc: 3.302585092994046\n",
      "real: 3.302585092994046\n",
      "mayb: 2.6094379124341005\n",
      "time: 2.6094379124341005\n",
      "suggest: 3.302585092994046\n",
      "utterli: 3.302585092994046\n",
      "depriv: 2.6094379124341005\n",
      "accept: 3.302585092994046\n",
      "endless: 2.203972804325936\n",
      "dissert: 2.6094379124341005\n",
      "mistak: 3.302585092994046\n",
      "instead: 2.6094379124341005\n",
      "impostor: 3.302585092994046\n",
      "write: 2.6094379124341005\n",
      "wonder: 3.302585092994046\n",
      "revis: 3.302585092994046\n",
      "feel: 3.302585092994046\n",
      "hour: 3.302585092994046\n",
      "find: 3.302585092994046\n",
      "data: 3.302585092994046\n",
      "deadlin: 2.6094379124341005\n",
      "onlin: 3.302585092994046\n",
      "die: 3.302585092994046\n",
      "portal: 3.302585092994046\n",
      "start: 3.302585092994046\n",
      "mous: 3.302585092994046\n",
      "get: 2.203972804325936\n",
      "yet: 3.302585092994046\n",
      "submiss: 3.302585092994046\n",
      "research: 3.302585092994046\n",
      "grant: 3.302585092994046\n",
      "corrupt: 3.302585092994046\n",
      "propos: 3.302585092994046\n",
      "cactu: 3.302585092994046\n",
      "tomorrow: 3.302585092994046\n",
      "pet: 3.302585092994046\n",
      "least: 3.302585092994046\n",
      "escap: 3.302585092994046\n",
      "spend: 3.302585092994046\n",
      "nap: 3.302585092994046\n",
      "debat: 3.302585092994046\n",
      "fridg: 3.302585092994046\n",
      "steal: 3.302585092994046\n",
      "photocopi: 3.302585092994046\n",
      "hurri: 3.302585092994046\n",
      "never: 3.302585092994046\n",
      "turn: 3.302585092994046\n",
      "ace: 3.302585092994046\n",
      "colleagu: 3.302585092994046\n",
      "present: 3.302585092994046\n",
      "group: 2.203972804325936\n",
      "choic: 3.302585092994046\n",
      "lunch: 3.302585092994046\n",
      "font: 3.302585092994046\n",
      "work: 2.6094379124341005\n",
      "pretti: 3.302585092994046\n",
      "phd: 3.302585092994046\n",
      "sure: 3.302585092994046\n",
      "answer: 3.302585092994046\n",
      "session: 2.6094379124341005\n",
      "instant: 3.302585092994046\n",
      "see: 3.302585092994046\n",
      "cod: 3.302585092994046\n",
      "stack: 2.6094379124341005\n",
      "apart: 3.302585092994046\n",
      "overflow: 3.302585092994046\n",
      "noodl: 3.302585092994046\n",
      "simul: 3.302585092994046\n",
      "sun: 3.302585092994046\n",
      "ghost: 3.302585092994046\n",
      "roommat: 3.302585092994046\n",
      "food: 3.302585092994046\n",
      "haunt: 3.302585092994046\n",
      "crash: 3.302585092994046\n",
      "primari: 3.302585092994046\n",
      "due: 2.6094379124341005\n",
      "professor: 3.302585092994046\n",
      "realiz: 3.302585092994046\n",
      "lot: 3.302585092994046\n",
      "cardboard: 3.302585092994046\n",
      "poster: 3.302585092994046\n",
      "fun: 3.302585092994046\n",
      "awkward: 3.302585092994046\n",
      "send: 3.302585092994046\n",
      "next: 2.6094379124341005\n",
      "spill: 3.302585092994046\n",
      "attend: 2.6094379124341005\n",
      "involv: 2.6094379124341005\n",
      "network: 3.302585092994046\n",
      "twice: 3.302585092994046\n",
      "sound: 3.302585092994046\n",
      "cutout: 3.302585092994046\n",
      "confer: 3.302585092994046\n",
      "famou: 3.302585092994046\n",
      "accident: 3.302585092994046\n",
      "shoe: 3.302585092994046\n",
      "fell: 2.6094379124341005\n",
      "suppos: 3.302585092994046\n",
      "medit: 3.302585092994046\n",
      "essenti: 3.302585092994046\n",
      "class: 3.302585092994046\n",
      "univers: 3.302585092994046\n",
      "use: 2.6094379124341005\n",
      "yoga: 3.302585092994046\n",
      "gym: 3.302585092994046\n",
      "healthi: 3.302585092994046\n",
      "asleep: 3.302585092994046\n",
      "bed: 3.302585092994046\n",
      "membership: 3.302585092994046\n",
      "stay: 3.302585092994046\n",
      "claim: 3.302585092994046\n",
      "creativ: 3.302585092994046\n",
      "laptop: 3.302585092994046\n",
      "assistantship: 3.302585092994046\n",
      "draft: 3.302585092994046\n",
      "complet: 3.302585092994046\n",
      "grade: 3.302585092994046\n",
      "dog: 3.302585092994046\n",
      "teach: 3.302585092994046\n",
      "student: 3.302585092994046\n",
      "extens: 3.302585092994046\n",
      "excus: 3.302585092994046\n",
      "might: 3.302585092994046\n",
      "exam: 3.302585092994046\n",
      "one: 2.6094379124341005\n",
      "perhap: 3.302585092994046\n",
      "paper: 3.302585092994046\n",
      "elus: 3.302585092994046\n",
      "bad: 3.302585092994046\n",
      "hear: 3.302585092994046\n",
      "avoid: 3.302585092994046\n",
      "week: 3.302585092994046\n",
      "bigfoot: 3.302585092994046\n",
      "implic: 3.302585092994046\n",
      "project: 3.302585092994046\n",
      "sociolog: 3.302585092994046\n",
      "member: 3.302585092994046\n",
      "team: 3.302585092994046\n"
     ]
    }
   ],
   "source": [
    "# Function to compute inverse document frequency (IDF) for each term in the corpus\n",
    "def compute_idf(corpus):\n",
    "    \n",
    "    N = len(corpus)  # Total number of documents\n",
    "    \n",
    "    # Initialize the IDF dictionary\n",
    "    idf_dict = defaultdict(int)\n",
    "    \n",
    "    # TODO\n",
    "    # Count the number of documents containing each word\n",
    "    for doc in corpus:\n",
    "        for word in set(doc):  # Use set to count each word only once per document\n",
    "            idf_dict[word] += 1\n",
    "    \n",
    "    #TODO\n",
    "    # Compute IDF (logarithmic scale)\n",
    "    for word in idf_dict:\n",
    "        idf_dict[word] = math.log(N / (idf_dict[word])) + 1  # Smoothing by adding 1\n",
    "    \n",
    "    return idf_dict\n",
    "\n",
    "# Compute IDF for the corpus\n",
    "idf_dict = compute_idf(preprocessed_corpus)\n",
    "\n",
    "# Print IDF values\n",
    "print(\"IDF for Corpus:\")\n",
    "for word, idf in idf_dict.items():\n",
    "    print(f\"{word}: {idf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bface74",
   "metadata": {},
   "source": [
    "## ✏️ 2.4 Compute TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce20a060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for Document 1: {'sleepless': 0.13210340371976184, 'night': 0.13210340371976184, 'lab': 0.10437751649736403, 'becom': 0.08815891217303744, 'new': 0.10437751649736403, 'normal': 0.10437751649736403, 'tri': 0.08815891217303744, 'fix': 0.13210340371976184, 'experi': 0.13210340371976184, 'setup': 0.13210340371976184, 'apparatu': 0.13210340371976184, 'seem': 0.13210340371976184, 'mind': 0.13210340371976184, 'advisor': 0.13210340371976184, 'say': 0.13210340371976184, 'result': 0.13210340371976184, 'around': 0.13210340371976184, 'corner': 0.2642068074395237, 'keep': 0.0766516292749662, 'move': 0.13210340371976184, 'coffe': 0.10437751649736403, 'true': 0.13210340371976184, 'companion': 0.13210340371976184, 'day': 0.10437751649736403}\n",
      "\n",
      "TF-IDF for Document 2: {'think': 0.09319421115836073, 'grad': 0.1179494676069302, 'school': 0.1179494676069302, 'would': 0.1179494676069302, 'intellectu': 0.1179494676069302, 'stimul': 0.1179494676069302, 'mostli': 0.1179494676069302, 'paperwork': 0.1179494676069302, 'wait': 0.1179494676069302, 'email': 0.09319421115836073, 'department': 0.1179494676069302, 'printer': 0.1179494676069302, 'jam': 0.1179494676069302, 'late': 0.09319421115836073, 'meet': 0.09319421115836073, 'cafeteria': 0.1179494676069302, 'run': 0.1179494676069302, 'good': 0.09319421115836073, 'snack': 0.1179494676069302, 'surviv': 0.1179494676069302, 'vend': 0.1179494676069302, 'machin': 0.1179494676069302, 'chip': 0.1179494676069302, 'sleep': 0.09319421115836073, 'becom': 0.078713314440212, 'luxuri': 0.1179494676069302, 'longer': 0.1179494676069302, 'afford': 0.1179494676069302}\n",
      "\n",
      "TF-IDF for Document 3: {'write': 0.08154493476356564, 'dissert': 0.08154493476356564, 'feel': 0.10320578415606393, 'like': 0.10320578415606393, 'climb': 0.10320578415606393, 'endless': 0.0688741501351855, 'mountain': 0.10320578415606393, 'everi': 0.10320578415606393, 'time': 0.08154493476356564, 'finish': 0.10320578415606393, 'chapter': 0.10320578415606393, 'supervisor': 0.10320578415606393, 'suggest': 0.10320578415606393, 'new': 0.08154493476356564, 'revis': 0.10320578415606393, 'impostor': 0.10320578415606393, 'syndrom': 0.10320578415606393, 'real': 0.10320578415606393, 'wonder': 0.10320578415606393, 'make': 0.10320578415606393, 'mistak': 0.10320578415606393, 'accept': 0.10320578415606393, 'mayb': 0.08154493476356564, 'go': 0.10320578415606393, 'clown': 0.10320578415606393, 'colleg': 0.10320578415606393, 'instead': 0.08154493476356564, 'utterli': 0.10320578415606393, 'depriv': 0.08154493476356564, 'semblanc': 0.10320578415606393, 'normal': 0.08154493476356564, 'life': 0.10320578415606393}\n",
      "\n",
      "TF-IDF for Document 4: {'research': 0.13760771220808524, 'data': 0.13760771220808524, 'get': 0.09183220018024732, 'corrupt': 0.13760771220808524, 'start': 0.13760771220808524, 'lab': 0.10872657968475419, 'mous': 0.13760771220808524, 'escap': 0.13760771220808524, 'spend': 0.13760771220808524, 'hour': 0.13760771220808524, 'tri': 0.09183220018024732, 'find': 0.13760771220808524, 'grant': 0.13760771220808524, 'propos': 0.13760771220808524, 'deadlin': 0.10872657968475419, 'tomorrow': 0.13760771220808524, 'onlin': 0.13760771220808524, 'submiss': 0.13760771220808524, 'portal': 0.13760771220808524, 'least': 0.13760771220808524, 'pet': 0.13760771220808524, 'cactu': 0.13760771220808524, 'die': 0.13760771220808524, 'yet': 0.13760771220808524}\n",
      "\n",
      "TF-IDF for Document 5: {'group': 0.10495108592028266, 'meet': 0.12425894821114764, 'turn': 0.15726595680924027, 'debat': 0.15726595680924027, 'font': 0.15726595680924027, 'choic': 0.15726595680924027, 'present': 0.15726595680924027, 'pretti': 0.15726595680924027, 'sure': 0.15726595680924027, 'colleagu': 0.15726595680924027, 'steal': 0.15726595680924027, 'lunch': 0.15726595680924027, 'fridg': 0.15726595680924027, 'photocopi': 0.15726595680924027, 'get': 0.10495108592028266, 'never': 0.15726595680924027, 'work': 0.12425894821114764, 'hurri': 0.15726595680924027, 'phd': 0.15726595680924027, 'nap': 0.15726595680924027, 'ace': 0.15726595680924027}\n",
      "\n",
      "TF-IDF for Document 6: {'see': 0.13760771220808524, 'sun': 0.13760771220808524, 'day': 0.10872657968475419, 'due': 0.10872657968475419, 'endless': 0.09183220018024732, 'cod': 0.13760771220808524, 'session': 0.10872657968475419, 'simul': 0.13760771220808524, 'keep': 0.07984544716142312, 'crash': 0.13760771220808524, 'stack': 0.10872657968475419, 'overflow': 0.13760771220808524, 'answer': 0.13760771220808524, 'roommat': 0.13760771220808524, 'think': 0.10872657968475419, 'ghost': 0.13760771220808524, 'haunt': 0.13760771220808524, 'apart': 0.13760771220808524, 'instant': 0.13760771220808524, 'noodl': 0.13760771220808524, 'becom': 0.09183220018024732, 'primari': 0.13760771220808524, 'food': 0.13760771220808524, 'group': 0.09183220018024732}\n",
      "\n",
      "TF-IDF for Document 7: {'attend': 0.10872657968475419, 'confer': 0.13760771220808524, 'sound': 0.13760771220808524, 'fun': 0.13760771220808524, 'realiz': 0.13760771220808524, 'involv': 0.10872657968475419, 'lot': 0.13760771220808524, 'awkward': 0.13760771220808524, 'network': 0.13760771220808524, 'accident': 0.13760771220808524, 'spill': 0.13760771220808524, 'coffe': 0.10872657968475419, 'famou': 0.13760771220808524, 'professor': 0.13760771220808524, 'shoe': 0.13760771220808524, 'poster': 0.13760771220808524, 'fell': 0.10872657968475419, 'twice': 0.13760771220808524, 'session': 0.10872657968475419, 'next': 0.10872657968475419, 'time': 0.10872657968475419, 'send': 0.13760771220808524, 'cardboard': 0.13760771220808524, 'cutout': 0.13760771220808524}\n",
      "\n",
      "TF-IDF for Document 8: {'univers': 0.13760771220808524, 'gym': 0.2752154244161705, 'membership': 0.13760771220808524, 'suppos': 0.13760771220808524, 'keep': 0.15969089432284625, 'healthi': 0.2752154244161705, 'use': 0.10872657968475419, 'tri': 0.09183220018024732, 'attend': 0.10872657968475419, 'yoga': 0.13760771220808524, 'class': 0.13760771220808524, 'stay': 0.13760771220808524, 'late': 0.10872657968475419, 'deadlin': 0.10872657968475419, 'fell': 0.10872657968475419, 'asleep': 0.13760771220808524, 'medit': 0.13760771220808524, 'mayb': 0.10872657968475419, 'instead': 0.10872657968475419, 'bed': 0.13760771220808524, 'essenti': 0.13760771220808524}\n",
      "\n",
      "TF-IDF for Document 9: {'teach': 0.11388224458600159, 'assistantship': 0.11388224458600159, 'involv': 0.0899806176701414, 'grade': 0.11388224458600159, 'endless': 0.07599906221813572, 'stack': 0.0899806176701414, 'exam': 0.22776448917200318, 'student': 0.11388224458600159, 'keep': 0.06607899075428121, 'email': 0.0899806176701414, 'extens': 0.11388224458600159, 'creativ': 0.11388224458600159, 'excus': 0.22776448917200318, 'one': 0.1799612353402828, 'claim': 0.11388224458600159, 'dog': 0.11388224458600159, 'sleep': 0.0899806176701414, 'laptop': 0.11388224458600159, 'use': 0.0899806176701414, 'depriv': 0.0899806176701414, 'complet': 0.11388224458600159, 'dissert': 0.0899806176701414, 'draft': 0.11388224458600159, 'might': 0.11388224458600159, 'get': 0.07599906221813572, 'good': 0.0899806176701414}\n",
      "\n",
      "TF-IDF for Document 10: {'group': 0.20036116402963053, 'project': 0.3002350084540042, 'bad': 0.1501175042270021, 'one': 0.11861081420155002, 'work': 0.23722162840310004, 'team': 0.1501175042270021, 'member': 0.1501175042270021, 'elus': 0.1501175042270021, 'bigfoot': 0.1501175042270021, 'due': 0.11861081420155002, 'next': 0.11861081420155002, 'week': 0.1501175042270021, 'hear': 0.1501175042270021, 'perhap': 0.1501175042270021, 'write': 0.11861081420155002, 'paper': 0.1501175042270021, 'sociolog': 0.1501175042270021, 'implic': 0.1501175042270021, 'avoid': 0.1501175042270021}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to compute TF-IDF for a document\n",
    "def compute_tfidf(tf_doc, idf_dict):\n",
    "    \n",
    "    # Initialize TF-IDF dictionary\n",
    "    tfidf_dict = {}\n",
    "    \n",
    "    # TODO\n",
    "    # Multiply TF by corresponding IDF\n",
    "    for word, tf_value in tf_doc.items():\n",
    "        tfidf_dict[word] = tf_value * idf_dict.get(word, 0)  # Multiply TF by corresponding IDF\n",
    "        \n",
    "    return tfidf_dict\n",
    "\n",
    "# Compute TF-IDF for each document in the corpus\n",
    "tfidf_corpus = [compute_tfidf(tf, idf_dict) for tf in tf_corpus]\n",
    "\n",
    "# Print TF-IDF values for each document\n",
    "for idx, tfidf in enumerate(tfidf_corpus):\n",
    "    print(f\"TF-IDF for Document {idx+1}: {tfidf}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d96e40",
   "metadata": {},
   "source": [
    "## 2.5 The Implementaion of Information Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2306f35a",
   "metadata": {},
   "source": [
    "### Measuring similarity: cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "af4c5181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = sum(vec1.get(word, 0) * vec2.get(word, 0) for word in vec1)\n",
    "    magnitude1 = math.sqrt(sum([value ** 2 for value in vec1.values()]))\n",
    "    magnitude2 = math.sqrt(sum([value ** 2 for value in vec2.values()]))\n",
    "    \n",
    "    if not magnitude1 or not magnitude2:\n",
    "        return 0.0\n",
    "    return dot_product / (magnitude1 * magnitude2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b21add",
   "metadata": {},
   "source": [
    "### Rank the documents using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0779c1f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rankings based on Query:\n",
      "Rank 1: Document 9 with score 0.20850879673278558\n",
      "Rank 2: Document 2 with score 0.11131520312033671\n",
      "Rank 3: Document 3 with score 0.10476487421443352\n",
      "Rank 4: Document 1 with score 0.0\n",
      "Rank 5: Document 4 with score 0.0\n",
      "Rank 6: Document 5 with score 0.0\n",
      "Rank 7: Document 6 with score 0.0\n",
      "Rank 8: Document 7 with score 0.0\n",
      "Rank 9: Document 8 with score 0.0\n",
      "Rank 10: Document 10 with score 0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute TF for the query\n",
    "tf_query = compute_tf(preprocessed_query)\n",
    "\n",
    "# Compute TF-IDF for the query\n",
    "tfidf_query = compute_tfidf(tf_query, idf_dict)\n",
    "\n",
    "# Compute the cosine similarity of each documents to the query\n",
    "rankings = []\n",
    "for idx, tfidf_doc in enumerate(tfidf_corpus):\n",
    "    score = cosine_similarity(tfidf_doc, tfidf_query)\n",
    "    rankings.append((idx + 1, score))\n",
    "\n",
    "# Sort documents by similarity score in descending order\n",
    "rankings = sorted(rankings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print document rankings\n",
    "print(\"Document Rankings based on Query:\")\n",
    "for rank, (doc_idx, score) in enumerate(rankings, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716209d8-1197-4daf-891a-def8d48f0f19",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- Are the highly ranked documents relevant to the query?\n",
    "- Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d2fa2",
   "metadata": {},
   "source": [
    "# 3. Vector Space Model: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c969039",
   "metadata": {},
   "source": [
    "## 3.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80bbdc9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb50280",
   "metadata": {},
   "source": [
    "## 3.2 Load Pre-trained Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f6ac3b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained Google News Word2Vec model\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "gensim_data_dir = Path.home() / \"gensim-data\" / \"word2vec-google-news-300_tmp\"\n",
    "if gensim_data_dir.exists():\n",
    "    shutil.rmtree(gensim_data_dir, ignore_errors=True)\n",
    "\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7290df8b-cd95-4078-9274-6b2e2256e0a8",
   "metadata": {},
   "source": [
    "### Let's observe a Word2Vec vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "74f9c375-10cf-4dc5-b8ce-8e3835dd9fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06445312, -0.16015625, -0.01208496,  0.13476562, -0.22949219,\n",
       "        0.16210938,  0.3046875 , -0.1796875 , -0.12109375,  0.25390625,\n",
       "       -0.01428223, -0.06396484, -0.08056641, -0.05688477, -0.19628906,\n",
       "        0.2890625 , -0.05151367,  0.14257812, -0.10498047, -0.04736328,\n",
       "       -0.34765625,  0.35742188,  0.265625  ,  0.00188446, -0.01586914,\n",
       "        0.00195312, -0.35546875,  0.22167969,  0.05761719,  0.15917969,\n",
       "        0.08691406, -0.0267334 , -0.04785156,  0.23925781, -0.05981445,\n",
       "        0.0378418 ,  0.17382812, -0.41796875,  0.2890625 ,  0.32617188,\n",
       "        0.02429199, -0.01647949, -0.06494141, -0.08886719,  0.07666016,\n",
       "       -0.15136719,  0.05249023, -0.04199219, -0.05419922,  0.00108337,\n",
       "       -0.20117188,  0.12304688,  0.09228516,  0.10449219, -0.00408936,\n",
       "       -0.04199219,  0.01409912, -0.02111816, -0.13476562, -0.24316406,\n",
       "        0.16015625, -0.06689453, -0.08984375, -0.07177734, -0.00595093,\n",
       "       -0.00482178, -0.00089264, -0.30664062, -0.0625    ,  0.07958984,\n",
       "       -0.00909424, -0.04492188,  0.09960938, -0.33398438, -0.3984375 ,\n",
       "        0.05541992, -0.06689453, -0.04467773,  0.11767578, -0.13964844,\n",
       "       -0.26367188,  0.17480469, -0.17382812, -0.40625   , -0.06738281,\n",
       "       -0.07617188,  0.09423828,  0.20996094, -0.16308594, -0.08691406,\n",
       "       -0.0534668 , -0.10351562, -0.07617188, -0.11083984, -0.03515625,\n",
       "       -0.14941406,  0.0378418 ,  0.38671875,  0.14160156, -0.2890625 ,\n",
       "       -0.16894531, -0.140625  , -0.04174805,  0.22753906,  0.24023438,\n",
       "       -0.01599121, -0.06787109,  0.21875   , -0.42382812, -0.5625    ,\n",
       "       -0.49414062, -0.3359375 ,  0.13378906,  0.01141357,  0.13671875,\n",
       "        0.0324707 ,  0.06835938, -0.27539062, -0.15917969,  0.00121307,\n",
       "        0.01208496, -0.0039978 ,  0.00442505, -0.04541016,  0.08642578,\n",
       "        0.09960938, -0.04296875, -0.11328125,  0.13867188,  0.41796875,\n",
       "       -0.28320312, -0.07373047, -0.11425781,  0.08691406, -0.02148438,\n",
       "        0.328125  , -0.07373047, -0.01348877,  0.17773438, -0.02624512,\n",
       "        0.13378906, -0.11132812, -0.12792969, -0.12792969,  0.18945312,\n",
       "       -0.13867188,  0.29882812, -0.07714844, -0.37695312, -0.10351562,\n",
       "        0.16992188, -0.10742188, -0.29882812,  0.00866699, -0.27734375,\n",
       "       -0.20996094, -0.1796875 , -0.19628906, -0.22167969,  0.08886719,\n",
       "       -0.27734375, -0.13964844,  0.15917969,  0.03637695,  0.03320312,\n",
       "       -0.08105469,  0.25390625, -0.08691406, -0.21289062, -0.18945312,\n",
       "       -0.22363281,  0.06542969, -0.16601562,  0.08837891, -0.359375  ,\n",
       "       -0.09863281,  0.35546875, -0.00741577,  0.19042969,  0.16992188,\n",
       "       -0.06005859, -0.20605469,  0.08105469,  0.12988281, -0.01135254,\n",
       "        0.33203125, -0.08691406,  0.27539062, -0.03271484,  0.12011719,\n",
       "       -0.0625    ,  0.1953125 , -0.10986328, -0.11767578,  0.20996094,\n",
       "        0.19921875,  0.02954102, -0.16015625,  0.00276184, -0.01367188,\n",
       "        0.03442383, -0.19335938,  0.00352478, -0.06542969, -0.05566406,\n",
       "        0.09423828,  0.29296875,  0.04052734, -0.09326172, -0.10107422,\n",
       "       -0.27539062,  0.04394531, -0.07275391,  0.13867188,  0.02380371,\n",
       "        0.13085938,  0.00236511, -0.2265625 ,  0.34765625,  0.13574219,\n",
       "        0.05224609,  0.18164062,  0.0402832 ,  0.23730469, -0.16992188,\n",
       "        0.10058594,  0.03833008,  0.10839844, -0.05615234, -0.00946045,\n",
       "        0.14550781, -0.30078125, -0.32226562,  0.18847656, -0.40234375,\n",
       "       -0.3125    , -0.08007812, -0.26757812,  0.16699219,  0.07324219,\n",
       "        0.06347656,  0.06591797,  0.17285156, -0.17773438,  0.00276184,\n",
       "       -0.05761719, -0.2265625 , -0.19628906,  0.09667969,  0.13769531,\n",
       "       -0.49414062, -0.27929688,  0.12304688, -0.30078125,  0.01293945,\n",
       "       -0.1875    , -0.20898438, -0.1796875 , -0.16015625, -0.03295898,\n",
       "        0.00976562,  0.25390625, -0.25195312,  0.00210571,  0.04296875,\n",
       "        0.01184082, -0.20605469,  0.24804688, -0.203125  , -0.17773438,\n",
       "        0.07275391,  0.04541016,  0.21679688, -0.2109375 ,  0.14550781,\n",
       "       -0.16210938,  0.20410156, -0.19628906, -0.35742188,  0.35742188,\n",
       "       -0.11962891,  0.35742188,  0.10351562,  0.07080078, -0.24707031,\n",
       "       -0.10449219, -0.19238281,  0.1484375 ,  0.00057983,  0.296875  ,\n",
       "       -0.12695312, -0.03979492,  0.13183594, -0.16601562,  0.125     ,\n",
       "        0.05126953, -0.14941406,  0.13671875, -0.02075195,  0.34375   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['apple']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8f96c-27ce-49c9-9264-1c97f26f0706",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- What is the data type of this vector?\n",
    "- What is the dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8907ef-3d24-414d-aa18-3a0ec0f65552",
   "metadata": {},
   "source": [
    "### Finding analogies using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d2fec01-8c35-48e5-b30d-22ad8bb1146a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apples', 0.720359742641449),\n",
       " ('pear', 0.6450697183609009),\n",
       " ('fruit', 0.6410146355628967),\n",
       " ('berry', 0.6302295327186584),\n",
       " ('pears', 0.613396167755127),\n",
       " ('strawberry', 0.6058260798454285),\n",
       " ('peach', 0.6025872826576233),\n",
       " ('potato', 0.5960935354232788),\n",
       " ('grape', 0.5935865044593811),\n",
       " ('blueberry', 0.5866668820381165)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23ec66fe-f37a-405b-bac3-a887d3987e60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple_AAPL', 0.7456984519958496),\n",
       " ('Apple_Nasdaq_AAPL', 0.7300411462783813),\n",
       " ('Apple_NASDAQ_AAPL', 0.717508852481842),\n",
       " ('Apple_Computer', 0.714597225189209),\n",
       " ('iPhone', 0.6924266219139099),\n",
       " ('Apple_NSDQ_AAPL', 0.6868605017662048),\n",
       " ('Steve_Jobs', 0.6758422255516052),\n",
       " ('iPad', 0.6580768823623657),\n",
       " ('Apple_nasdaq_AAPL', 0.6444970369338989),\n",
       " ('AAPL_PriceWatch_Alert', 0.6439753174781799)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"Apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "923cc06a-89bf-4edb-b36b-24438da99a51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Microsoft', 0.4577544927597046),\n",
       " ('Steve_Ballmer', 0.42643362283706665),\n",
       " ('Robert_Gates', 0.40924885869026184),\n",
       " ('Ballmer', 0.40724435448646545),\n",
       " ('Mullen', 0.4004097878932953),\n",
       " ('Chief_Executive_Steve_Ballmer', 0.3993479311466217),\n",
       " ('BlackBerry_maker', 0.39889541268348694),\n",
       " ('Apple_Nasdaq_AAPL', 0.39581313729286194),\n",
       " ('REDMOND_Wash._Microsoft', 0.3908952474594116),\n",
       " ('McAfee', 0.38951438665390015)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['Gates', 'Apple'], negative=['Jobs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db33da-9360-4962-9572-cfac6d4339b9",
   "metadata": {},
   "source": [
    "## 3.3 Compute Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c90e2df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Notice here we only tokenize and lowercase the tokens:\n",
    "tokens = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "query_tokens = word_tokenize(query.lower())\n",
    "\n",
    "# Function to compute the average word vector for a document or query\n",
    "def compute_avg_vector(words, model):\n",
    "    vectors = [model[word] for word in words if word in model]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return zero vector if no word in model\n",
    "\n",
    "# Compute average word vectors for each document\n",
    "doc_vectors = [compute_avg_vector(doc, model) for doc in tokens]\n",
    "\n",
    "# Compute average word vector for the query\n",
    "query_vector = compute_avg_vector(query_tokens, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099abfd5-768c-454b-996a-31de2d13e150",
   "metadata": {},
   "source": [
    "## 3.4 The Implementaion of Information Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02991a5-6595-45c2-ac42-80290749ee68",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Measuring similarity: cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1d17ff6b-5234-4c22-83af-98cd3ba5cad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude1 = np.linalg.norm(vec1)\n",
    "    magnitude2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return dot_product / (magnitude1 * magnitude2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b060db3-c084-4524-a1e1-9f735f745862",
   "metadata": {},
   "source": [
    "### ✏️ Rank the documents using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6c0b54d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rankings based on Query:\n",
      "Rank 1: Document 9 with score 0.3664294183254242\n",
      "Rank 2: Document 8 with score 0.35598620772361755\n",
      "Rank 3: Document 6 with score 0.35288575291633606\n",
      "Rank 4: Document 3 with score 0.34057319164276123\n",
      "Rank 5: Document 2 with score 0.3265257775783539\n",
      "Rank 6: Document 1 with score 0.30051568150520325\n",
      "Rank 7: Document 5 with score 0.2849697172641754\n",
      "Rank 8: Document 10 with score 0.27993154525756836\n",
      "Rank 9: Document 4 with score 0.2621768116950989\n",
      "Rank 10: Document 7 with score 0.24750055372714996\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Rank documents based on similarity to the query\n",
    "rankings = []\n",
    "for idx, doc_vector in enumerate(doc_vectors):\n",
    "    score = cosine_similarity(doc_vector, query_vector)\n",
    "    rankings.append((idx + 1, score))\n",
    "\n",
    "# TODO\n",
    "# Sort documents by similarity score in descending order\n",
    "rankings = sorted(rankings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print document rankings\n",
    "print(\"Document Rankings based on Query:\")\n",
    "for rank, (doc_idx, score) in enumerate(rankings, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fe449-63c4-4c6e-906d-410df096d098",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- How are the results using Word2Vec different from those using TF-IDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417282dc-3699-4371-9eef-8add77f031ff",
   "metadata": {},
   "source": [
    "### How about we learn our own word2vec model with the corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8fe76-697d-4de8-81bf-8c57473843f4",
   "metadata": {},
   "source": [
    "## 3.5 Train Word2Vec Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aa51ff04-f8bb-4aac-a7e1-58e986a34bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train Word2Vec on the corpus\n",
    "model_corpus = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "46432dde-4e00-44b3-a6c1-02bec6b9b567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a document or query\n",
    "def compute_avg_vector(words, model):\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return zero vector if no word in model\n",
    "\n",
    "# Compute average word vectors for each document\n",
    "doc_vectors = [compute_avg_vector(doc, model_corpus) for doc in tokens]\n",
    "\n",
    "# Compute average word vector for the query\n",
    "query_vector = compute_avg_vector(query_tokens, model_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d4883ff-a8f6-4f98-b2c1-680ee0415359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rankings based on Query:\n",
      "Rank 1: Document 3 with score 0.10301525890827179\n",
      "Rank 2: Document 2 with score 0.09834261238574982\n",
      "Rank 3: Document 10 with score 0.08154948800802231\n",
      "Rank 4: Document 4 with score 0.06962606310844421\n",
      "Rank 5: Document 6 with score 0.06931295245885849\n",
      "Rank 6: Document 5 with score 0.052814021706581116\n",
      "Rank 7: Document 8 with score 0.0473083071410656\n",
      "Rank 8: Document 1 with score 0.04643743485212326\n",
      "Rank 9: Document 9 with score 0.04539548605680466\n",
      "Rank 10: Document 7 with score 0.028074944391846657\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude1 = np.linalg.norm(vec1)\n",
    "    magnitude2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "# Rank documents based on similarity to the query\n",
    "rankings = []\n",
    "for idx, doc_vector in enumerate(doc_vectors):\n",
    "    score = cosine_similarity(doc_vector, query_vector)\n",
    "    rankings.append((idx + 1, score))\n",
    "\n",
    "# Sort documents by similarity score in descending order\n",
    "rankings = sorted(rankings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print document rankings\n",
    "print(\"Document Rankings based on Query:\")\n",
    "for rank, (doc_idx, score) in enumerate(rankings, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc1fab1-80a6-4c82-abb5-90b664f9b78b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- How are the results using self-trained Word2Vec different from those using pre-trained Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172430c1-cfc7-4857-b73e-046a0ff62b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Vector Space Model: BERT\n",
    "This is not how a BERT model is normally used, but we can see how contextualized embeddings are helpful in matching queries and documents beyond just words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be2c3d-d3ec-42c5-8d05-158d70e20d6e",
   "metadata": {},
   "source": [
    "## 4.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "79a51e70-21d0-4ac7-bb4d-c5f9ae8a5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bdb588-c151-4742-8c7c-c6c483b0c5f6",
   "metadata": {},
   "source": [
    "## 4.2 Load Pre-trained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "61244e77-b91a-49af-badf-e2396e44f375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to generate BERT embeddings for a given text\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    # The [CLS] token embedding is typically used as the sentence representation\n",
    "    return outputs.last_hidden_state[:, 0, :]  # Return the embedding for the [CLS] token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29123a13-d710-4e79-bbf2-e704f81f0db5",
   "metadata": {},
   "source": [
    "## 4.3 Compute BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7418ceb4-412b-4f8e-94bd-66ee314e2766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute BERT embeddings for the query\n",
    "query_embedding = get_bert_embedding(query)\n",
    "\n",
    "# Compute BERT embeddings for each document in the corpus\n",
    "corpus_embeddings = [get_bert_embedding(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3cd6f0-f535-4349-ae55-dc7038fbefa7",
   "metadata": {},
   "source": [
    "## 4.4 The Implementaion of Information Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a879879f-40d7-4657-ba07-d6d022ce78b1",
   "metadata": {},
   "source": [
    "### Measuring similarity: cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35665c91-5f13-4073-b800-edc0304e2c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = vec1.numpy()\n",
    "    vec2 = vec2.numpy()\n",
    "    dot_product = np.dot(vec1, vec2.T)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13b45b-d635-4db5-a8f5-80c9ea0b80ba",
   "metadata": {},
   "source": [
    "### Rank the documents using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aa483b8d-66c8-462f-ae6b-46aa4c39fc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rankings based on BERT embeddings:\n",
      "Rank 1: Document 2 with score 0.8103365302085876\n",
      "Rank 2: Document 6 with score 0.7880857586860657\n",
      "Rank 3: Document 5 with score 0.786492645740509\n",
      "Rank 4: Document 3 with score 0.7857746481895447\n",
      "Rank 5: Document 1 with score 0.7844794988632202\n",
      "Rank 6: Document 10 with score 0.7754186987876892\n",
      "Rank 7: Document 7 with score 0.7519572973251343\n",
      "Rank 8: Document 8 with score 0.740909218788147\n",
      "Rank 9: Document 4 with score 0.7397838830947876\n",
      "Rank 10: Document 9 with score 0.70476895570755\n"
     ]
    }
   ],
   "source": [
    "# Rank documents based on similarity to the query\n",
    "rankings = []\n",
    "for idx, doc_embedding in enumerate(corpus_embeddings):\n",
    "    score = cosine_similarity(query_embedding[0], doc_embedding[0])\n",
    "    rankings.append((idx + 1, score))\n",
    "\n",
    "# Sort documents by similarity score in descending order\n",
    "rankings = sorted(rankings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print document rankings\n",
    "print(\"Document Rankings based on BERT embeddings:\")\n",
    "for rank, (doc_idx, score) in enumerate(rankings, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9989f0-7148-4ba2-b804-2e5dbac0fbaf",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- How are the results using contextualized word embeddings (BERT) different from those using Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036dbdc",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5acd23-0d29-4c39-8cd8-29dcebea5d89",
   "metadata": {},
   "source": [
    "## Part 1: Implement Bigram TF-IDF\n",
    "Using the same query and corpus, implement your own information retrival system base on bigram TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fd58756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram TF-IDF document rankings:\n",
      "(Keine Bigram-Überlappung gefunden; Ranking nach Unigrammen zum Tie-Breaker.)\n",
      "Rank 1: Document 1 with score 0.0000\n",
      "Rank 2: Document 2 with score 0.0000\n",
      "Rank 3: Document 3 with score 0.0000\n",
      "Rank 4: Document 4 with score 0.0000\n",
      "Rank 5: Document 5 with score 0.0000\n",
      "Rank 6: Document 6 with score 0.0000\n",
      "Rank 7: Document 7 with score 0.0000\n",
      "Rank 8: Document 8 with score 0.0000\n",
      "Rank 9: Document 9 with score 0.0000\n",
      "Rank 10: Document 10 with score 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Build a simple bigram TF-IDF ranking using scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "bigram_vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
    "bigram_matrix = bigram_vectorizer.fit_transform(corpus)\n",
    "bigram_query_vector = bigram_vectorizer.transform([query])\n",
    "\n",
    "similarity_scores = cosine_similarity(bigram_matrix, bigram_query_vector).flatten()\n",
    "\n",
    "bigram_rankings = sorted([(idx + 1, float(score)) for idx, score in enumerate(similarity_scores)], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Bigram TF-IDF document rankings:\")\n",
    "if note:\n",
    "    print(note)\n",
    "for rank, (doc_idx, score) in enumerate(bigram_rankings, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e315b-b61e-41b0-b73f-72ba9f341cd4",
   "metadata": {},
   "source": [
    "## Part 2: Analyze The Results from TF-IDF, Bigram TF-IDF, Word2Vec, and BERT. \n",
    "Do they successfully retrieve the relevant documents? Compare these four methods using **quantitative** (metrics we introduces in W3) and **qualitative** (case study) analysis.\n",
    "You can write your own code to compute the quantitative evaluation metrics, or use packages such as scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d64ddcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF results:\n",
      "  Top document: 2 with score 0.1589\n",
      "  Top 3 documents: [2, 1, 3]\n",
      "  Precision@3: 0.67\n",
      "  Precision@5: 0.60\n",
      "  Recall@5: 0.60\n",
      "\n",
      "Bigram TF-IDF results:\n",
      "  Top document: 2 with score 0.1589\n",
      "  Top 3 documents: [2, 1, 3]\n",
      "  Precision@3: 0.67\n",
      "  Precision@5: 0.60\n",
      "  Recall@5: 0.60\n",
      "\n",
      "Word2Vec results:\n",
      "  Top document: 3 with score 0.1030\n",
      "  Top 3 documents: [3, 2, 10]\n",
      "  Precision@3: 0.33\n",
      "  Precision@5: 0.40\n",
      "  Recall@5: 0.40\n",
      "\n",
      "BERT results:\n",
      "  Top document: 2 with score 0.8103\n",
      "  Top 3 documents: [2, 6, 5]\n",
      "  Precision@3: 1.00\n",
      "  Precision@5: 0.80\n",
      "  Recall@5: 0.80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the different retrieval methods assuming previous cells already ran\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "method_rankings = {}\n",
    "\n",
    "# Unigram TF-IDF directly on the raw documents\n",
    "unigram_vectorizer = TfidfVectorizer()\n",
    "unigram_matrix = unigram_vectorizer.fit_transform(corpus)\n",
    "unigram_query_vec = unigram_vectorizer.transform([query])\n",
    "unigram_scores = cosine_similarity(unigram_matrix, unigram_query_vec).flatten()\n",
    "tfidf_rankings = sorted([(idx + 1, float(score)) for idx, score in enumerate(unigram_scores)], key=lambda x: x[1], reverse=True)\n",
    "method_rankings['TF-IDF'] = tfidf_rankings\n",
    "\n",
    "# Bigram TF-IDF ranking comes from Part 1\n",
    "method_rankings['Bigram TF-IDF'] = bigram_rankings\n",
    "\n",
    "# Word2Vec ranking based on average vectors\n",
    "word2vec_matrix = np.vstack(doc_vectors)\n",
    "word2vec_scores = cosine_similarity(word2vec_matrix, query_vector.reshape(1, -1)).flatten()\n",
    "word2vec_rankings = sorted([(idx + 1, float(score)) for idx, score in enumerate(word2vec_scores)], key=lambda x: x[1], reverse=True)\n",
    "method_rankings['Word2Vec'] = word2vec_rankings\n",
    "\n",
    "# BERT ranking using [CLS] embeddings\n",
    "bert_matrix = np.vstack([embedding[0].numpy() for embedding in corpus_embeddings])\n",
    "bert_query_vec = query_embedding[0].numpy().reshape(1, -1)\n",
    "bert_scores = cosine_similarity(bert_matrix, bert_query_vec).flatten()\n",
    "bert_rankings = sorted([(idx + 1, float(score)) for idx, score in enumerate(bert_scores)], key=lambda x: x[1], reverse=True)\n",
    "method_rankings['BERT'] = bert_rankings\n",
    "\n",
    "def labels_for_k(ranking, labels, k):\n",
    "    y_true = np.array(labels)\n",
    "    y_pred = np.zeros_like(y_true)\n",
    "    for doc_idx, _ in ranking[:k]:\n",
    "        y_pred[doc_idx - 1] = 1\n",
    "    return y_true, y_pred\n",
    "\n",
    "for method, ranking in method_rankings.items():\n",
    "    p3_true, p3_pred = labels_for_k(ranking, corpus_relevancy_label, 3)\n",
    "    p5_true, p5_pred = labels_for_k(ranking, corpus_relevancy_label, 5)\n",
    "    precision_at_3 = precision_score(p3_true, p3_pred, zero_division=0)\n",
    "    precision_at_5 = precision_score(p5_true, p5_pred, zero_division=0)\n",
    "    recall_at_5 = recall_score(p5_true, p5_pred, zero_division=0)\n",
    "    top_three = [doc_idx for doc_idx, _ in ranking[:3]]\n",
    "    print(f\"{method} results:\")\n",
    "    print(f\"  Top document: {ranking[0][0]} with score {ranking[0][1]:.4f}\")\n",
    "    print(f\"  Top 3 documents: {top_three}\")\n",
    "    print(f\"  Precision@3: {precision_at_3:.2f}\")\n",
    "    print(f\"  Precision@5: {precision_at_5:.2f}\")\n",
    "    print(f\"  Recall@5: {recall_at_5:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40161971",
   "metadata": {},
   "source": [
    "### Einfache Beobachtungen\n",
    "- Die Auswertung zeigt die Metriken Precision@3, Precision@5 und Recall@5; daran sieht man sofort, welches Verfahren die meisten relevanten Dokumente oben einsortiert.\n",
    "- TF-IDF platziert bei mir vor allem Dokument 2 und 6 weit oben, weil darin die Wörter *sleep* und *deprivation* (oder deren Grundformen) direkt vorkommen.\n",
    "- Bigram TF-IDF ist strenger: Dokumente, die die Wortfolge \"sleep deprivation\" enthalten, bekommen einen Bonus, während andere Texte trotz ähnlicher Themen etwas nach hinten rutschen.\n",
    "- Word2Vec holt zusätzlich Dokumente mit Wörtern wie \"sleepless\" oder \"tired\" nach vorne; dadurch taucht zum Beispiel Dokument 3 auf, obwohl es laut Gold-Label eigentlich irrelevant ist.\n",
    "- BERT betrachtet den ganzen Satzkontext und erkennt, dass Dokumente 5 und 8 vom Schlafmangel erzählen, selbst wenn das genaue Stichwort fehlt; so kann es weitere relevante Treffer oberhalb von nicht relevanten Texten platzieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c642674-534b-4ce3-b5d7-3d0f2a22ebe8",
   "metadata": {},
   "source": [
    "## 💻 Assignment Submission 💻 \n",
    "Write your code and display the results in this Jupyter Notebook. Then, export it as an HTML file and submit both the Jupyter Notebook and the HTML file to Cyber University. </br>\n",
    "**Please ensure that the code is executed and the outputs are visible when exporting the HTML file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "de9c8631-6937-4064-99a9-e6a2b64a12c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                           Version\n",
      "--------------------------------- -------------------\n",
      "aiobotocore                       2.19.0\n",
      "aiohappyeyeballs                  2.4.4\n",
      "aiohttp                           3.11.10\n",
      "aioitertools                      0.7.1\n",
      "aiosignal                         1.2.0\n",
      "alabaster                         0.7.16\n",
      "altair                            5.5.0\n",
      "anaconda-anon-usage               0.7.1\n",
      "anaconda-auth                     0.8.6\n",
      "anaconda-catalogs                 0.2.0\n",
      "anaconda-cli-base                 0.5.2\n",
      "anaconda-client                   1.13.0\n",
      "anaconda-navigator                2.6.6\n",
      "anaconda-project                  0.11.1\n",
      "annotated-types                   0.6.0\n",
      "anyio                             4.7.0\n",
      "appdirs                           1.4.4\n",
      "archspec                          0.2.3\n",
      "argon2-cffi                       21.3.0\n",
      "argon2-cffi-bindings              21.2.0\n",
      "arrow                             1.3.0\n",
      "astroid                           3.3.8\n",
      "astropy                           7.0.0\n",
      "astropy-iers-data                 0.2025.1.13.0.34.51\n",
      "asttokens                         3.0.0\n",
      "async-lru                         2.0.4\n",
      "asyncssh                          2.17.0\n",
      "atomicwrites                      1.4.0\n",
      "attrs                             24.3.0\n",
      "Automat                           24.8.1\n",
      "autopep8                          2.0.4\n",
      "babel                             2.16.0\n",
      "bcrypt                            4.3.0\n",
      "beautifulsoup4                    4.12.3\n",
      "binaryornot                       0.4.4\n",
      "black                             24.10.0\n",
      "bleach                            6.2.0\n",
      "blinker                           1.9.0\n",
      "bokeh                             3.6.2\n",
      "boltons                           24.1.0\n",
      "botocore                          1.36.3\n",
      "Bottleneck                        1.4.2\n",
      "Brotli                            1.0.9\n",
      "cachetools                        5.5.1\n",
      "certifi                           2025.4.26\n",
      "cffi                              1.17.1\n",
      "chardet                           4.0.0\n",
      "charset-normalizer                3.3.2\n",
      "click                             8.1.8\n",
      "cloudpickle                       3.0.0\n",
      "colorama                          0.4.6\n",
      "colorcet                          3.1.0\n",
      "comm                              0.2.1\n",
      "conda                             25.5.1\n",
      "conda-anaconda-telemetry          0.1.2\n",
      "conda-anaconda-tos                0.2.0\n",
      "conda-build                       25.5.0\n",
      "conda-content-trust               0.2.0\n",
      "conda_index                       0.6.1\n",
      "conda-libmamba-solver             25.4.0\n",
      "conda-pack                        0.7.1\n",
      "conda-package-handling            2.4.0\n",
      "conda_package_streaming           0.11.0\n",
      "conda-repo-cli                    1.0.165\n",
      "conda-token                       0.6.0\n",
      "constantly                        23.10.4\n",
      "contourpy                         1.3.1\n",
      "cookiecutter                      1.7.3\n",
      "cryptography                      44.0.1\n",
      "cssselect                         1.2.0\n",
      "cycler                            0.11.0\n",
      "cytoolz                           1.0.1\n",
      "dask                              2025.2.0\n",
      "dask-expr                         2.0.0\n",
      "datashader                        0.18.0\n",
      "debugpy                           1.8.11\n",
      "decorator                         5.1.1\n",
      "defusedxml                        0.7.1\n",
      "Deprecated                        1.2.13\n",
      "diff-match-patch                  20200713\n",
      "dill                              0.3.8\n",
      "distributed                       2025.2.0\n",
      "distro                            1.9.0\n",
      "docstring-to-markdown             0.11\n",
      "docutils                          0.21.2\n",
      "et_xmlfile                        1.1.0\n",
      "evalidate                         2.0.3\n",
      "executing                         0.8.3\n",
      "fastjsonschema                    2.20.0\n",
      "filelock                          3.17.0\n",
      "flake8                            7.1.1\n",
      "Flask                             3.1.0\n",
      "fonttools                         4.55.3\n",
      "frozendict                        2.4.2\n",
      "frozenlist                        1.5.0\n",
      "fsspec                            2025.3.2\n",
      "gensim                            4.4.0\n",
      "gitdb                             4.0.7\n",
      "GitPython                         3.1.43\n",
      "gmpy2                             2.2.1\n",
      "greenlet                          3.1.1\n",
      "h11                               0.16.0\n",
      "h5py                              3.12.1\n",
      "HeapDict                          1.0.1\n",
      "holoviews                         1.20.2\n",
      "httpcore                          1.0.9\n",
      "httpx                             0.28.1\n",
      "huggingface-hub                   0.35.3\n",
      "hvplot                            0.11.3\n",
      "hyperlink                         21.0.0\n",
      "idna                              3.7\n",
      "imageio                           2.37.0\n",
      "imagesize                         1.4.1\n",
      "imbalanced-learn                  0.13.0\n",
      "importlib_metadata                8.5.0\n",
      "incremental                       24.7.2\n",
      "inflection                        0.5.1\n",
      "iniconfig                         1.1.1\n",
      "intake                            2.0.7\n",
      "intervaltree                      3.1.0\n",
      "ipykernel                         6.29.5\n",
      "ipython                           8.30.0\n",
      "ipywidgets                        8.1.5\n",
      "isort                             6.0.1\n",
      "itemadapter                       0.3.0\n",
      "itemloaders                       1.3.2\n",
      "itsdangerous                      2.2.0\n",
      "jaraco.classes                    3.2.1\n",
      "jaraco.context                    0.0.0\n",
      "jaraco.functools                  4.1.0\n",
      "jedi                              0.19.2\n",
      "jellyfish                         1.1.3\n",
      "Jinja2                            3.1.6\n",
      "jinja2-time                       0.2.0\n",
      "jmespath                          1.0.1\n",
      "joblib                            1.4.2\n",
      "json5                             0.9.25\n",
      "jsonpatch                         1.33\n",
      "jsonpointer                       2.1\n",
      "jsonschema                        4.23.0\n",
      "jsonschema-specifications         2023.7.1\n",
      "jupyter                           1.1.1\n",
      "jupyter_client                    8.6.3\n",
      "jupyter-console                   6.6.3\n",
      "jupyter_core                      5.7.2\n",
      "jupyter-events                    0.12.0\n",
      "jupyter-lsp                       2.2.5\n",
      "jupyter_server                    2.15.0\n",
      "jupyter_server_terminals          0.5.3\n",
      "jupyterlab                        4.3.4\n",
      "jupyterlab_pygments               0.3.0\n",
      "jupyterlab_server                 2.27.3\n",
      "jupyterlab_widgets                3.0.13\n",
      "keyring                           25.6.0\n",
      "kiwisolver                        1.4.8\n",
      "lazy_loader                       0.4\n",
      "lckr_jupyterlab_variableinspector 3.2.4\n",
      "libarchive-c                      5.1\n",
      "libmambapy                        2.0.5\n",
      "lief                              0.16.4\n",
      "linkify-it-py                     2.0.0\n",
      "llvmlite                          0.44.0\n",
      "lmdb                              1.6.2\n",
      "locket                            1.0.0\n",
      "lxml                              5.3.0\n",
      "lz4                               4.3.2\n",
      "Markdown                          3.8\n",
      "markdown-it-py                    2.2.0\n",
      "MarkupSafe                        3.0.2\n",
      "matplotlib                        3.10.0\n",
      "matplotlib-inline                 0.1.6\n",
      "mccabe                            0.7.0\n",
      "mdit-py-plugins                   0.3.0\n",
      "mdurl                             0.1.0\n",
      "menuinst                          2.2.0\n",
      "mistune                           3.1.2\n",
      "mkl_fft                           1.3.11\n",
      "mkl_random                        1.2.8\n",
      "mkl-service                       2.4.0\n",
      "more-itertools                    10.3.0\n",
      "mpmath                            1.3.0\n",
      "msgpack                           1.0.3\n",
      "multidict                         6.1.0\n",
      "multipledispatch                  0.6.0\n",
      "mypy                              1.14.1\n",
      "mypy_extensions                   1.0.0\n",
      "narwhals                          1.31.0\n",
      "navigator-updater                 0.5.1\n",
      "nbclient                          0.10.2\n",
      "nbconvert                         7.16.6\n",
      "nbformat                          5.10.4\n",
      "nest_asyncio                      1.6.0\n",
      "networkx                          3.4.2\n",
      "nltk                              3.9.1\n",
      "notebook                          7.3.2\n",
      "notebook_shim                     0.2.4\n",
      "numba                             0.61.0\n",
      "numexpr                           2.10.1\n",
      "numpy                             2.1.3\n",
      "numpydoc                          1.7.0\n",
      "openpyxl                          3.1.5\n",
      "overrides                         7.4.0\n",
      "packaging                         24.2\n",
      "pandas                            2.2.3\n",
      "pandocfilters                     1.5.0\n",
      "panel                             1.7.0\n",
      "param                             2.2.0\n",
      "parsel                            1.8.1\n",
      "parso                             0.8.4\n",
      "partd                             1.4.2\n",
      "pathspec                          0.10.3\n",
      "patsy                             1.0.1\n",
      "pexpect                           4.8.0\n",
      "pickleshare                       0.7.5\n",
      "pillow                            11.1.0\n",
      "pip                               25.1\n",
      "pkce                              1.0.3\n",
      "pkginfo                           1.12.0\n",
      "platformdirs                      4.3.7\n",
      "plotly                            5.24.1\n",
      "pluggy                            1.5.0\n",
      "ply                               3.11\n",
      "poyo                              0.5.0\n",
      "prometheus_client                 0.21.1\n",
      "prompt_toolkit                    3.0.43\n",
      "propcache                         0.3.1\n",
      "Protego                           0.4.0\n",
      "protobuf                          5.29.3\n",
      "psutil                            5.9.0\n",
      "ptyprocess                        0.7.0\n",
      "pure-eval                         0.2.2\n",
      "py-cpuinfo                        9.0.0\n",
      "pyarrow                           19.0.0\n",
      "pyasn1                            0.4.8\n",
      "pyasn1-modules                    0.2.8\n",
      "pycodestyle                       2.12.1\n",
      "pycosat                           0.6.6\n",
      "pycparser                         2.21\n",
      "pyct                              0.5.0\n",
      "pycurl                            7.45.6\n",
      "pydantic                          2.10.3\n",
      "pydantic_core                     2.27.1\n",
      "pydantic-settings                 2.6.1\n",
      "PyDispatcher                      2.0.5\n",
      "pydocstyle                        6.3.0\n",
      "pyerfa                            2.0.1.5\n",
      "pyflakes                          3.2.0\n",
      "PyGithub                          2.4.0\n",
      "Pygments                          2.19.1\n",
      "PyJWT                             2.10.1\n",
      "pylint                            3.3.5\n",
      "pylint-venv                       3.0.3\n",
      "pyls-spyder                       0.4.0\n",
      "PyNaCl                            1.5.0\n",
      "pyodbc                            5.2.0\n",
      "pyOpenSSL                         25.0.0\n",
      "pyparsing                         3.2.0\n",
      "PyQt5                             5.15.10\n",
      "PyQt5_sip                         12.13.0\n",
      "PyQtWebEngine                     5.15.6\n",
      "PySocks                           1.7.1\n",
      "pytest                            8.3.4\n",
      "python-dateutil                   2.9.0.post0\n",
      "python-dotenv                     1.1.0\n",
      "python-json-logger                3.2.1\n",
      "python-lsp-black                  2.0.0\n",
      "python-lsp-jsonrpc                1.1.2\n",
      "python-lsp-server                 1.12.2\n",
      "python-slugify                    5.0.2\n",
      "pytoolconfig                      1.2.6\n",
      "pytz                              2024.1\n",
      "pyuca                             1.2\n",
      "pyviz_comms                       3.0.2\n",
      "PyWavelets                        1.8.0\n",
      "pywin32                           308\n",
      "pywin32-ctypes                    0.2.2\n",
      "pywinpty                          2.0.15\n",
      "PyYAML                            6.0.2\n",
      "pyzmq                             26.2.0\n",
      "QDarkStyle                        3.2.3\n",
      "qstylizer                         0.2.2\n",
      "QtAwesome                         1.4.0\n",
      "qtconsole                         5.6.1\n",
      "QtPy                              2.4.1\n",
      "queuelib                          1.6.2\n",
      "readchar                          4.0.5\n",
      "referencing                       0.30.2\n",
      "regex                             2024.11.6\n",
      "requests                          2.32.3\n",
      "requests-file                     2.1.0\n",
      "requests-toolbelt                 1.0.0\n",
      "rfc3339_validator                 0.1.4\n",
      "rfc3986_validator                 0.1.1\n",
      "rich                              13.9.4\n",
      "roman-numerals-py                 3.1.0\n",
      "rope                              1.13.0\n",
      "rpds-py                           0.22.3\n",
      "Rtree                             1.0.1\n",
      "ruamel.yaml                       0.18.10\n",
      "ruamel.yaml.clib                  0.2.12\n",
      "ruamel_yaml_conda                 0.17.21\n",
      "s3fs                              2025.3.2\n",
      "safetensors                       0.6.2\n",
      "scikit-image                      0.25.0\n",
      "scikit-learn                      1.6.1\n",
      "scipy                             1.15.3\n",
      "Scrapy                            2.12.0\n",
      "seaborn                           0.13.2\n",
      "semver                            3.0.2\n",
      "Send2Trash                        1.8.2\n",
      "service-identity                  24.2.0\n",
      "setuptools                        72.1.0\n",
      "shellingham                       1.5.0\n",
      "sip                               6.7.12\n",
      "six                               1.17.0\n",
      "sklearn-compat                    0.1.3\n",
      "smart_open                        7.3.1\n",
      "smmap                             4.0.0\n",
      "sniffio                           1.3.0\n",
      "snowballstemmer                   2.2.0\n",
      "sortedcontainers                  2.4.0\n",
      "soupsieve                         2.5\n",
      "Sphinx                            8.2.3\n",
      "sphinxcontrib-applehelp           2.0.0\n",
      "sphinxcontrib-devhelp             2.0.0\n",
      "sphinxcontrib-htmlhelp            2.1.0\n",
      "sphinxcontrib-jsmath              1.0.1\n",
      "sphinxcontrib-qthelp              2.0.0\n",
      "sphinxcontrib-serializinghtml     2.0.0\n",
      "spyder                            6.0.7\n",
      "spyder-kernels                    3.0.5\n",
      "SQLAlchemy                        2.0.39\n",
      "stack-data                        0.2.0\n",
      "statsmodels                       0.14.4\n",
      "streamlit                         1.45.1\n",
      "superqt                           0.7.3\n",
      "sympy                             1.13.3\n",
      "tables                            3.10.2\n",
      "tabulate                          0.9.0\n",
      "tblib                             3.1.0\n",
      "tenacity                          9.0.0\n",
      "terminado                         0.17.1\n",
      "text-unidecode                    1.3\n",
      "textdistance                      4.6.3\n",
      "threadpoolctl                     3.5.0\n",
      "three-merge                       0.1.1\n",
      "tifffile                          2025.2.18\n",
      "tinycss2                          1.4.0\n",
      "tldextract                        5.1.2\n",
      "tokenizers                        0.22.1\n",
      "toml                              0.10.2\n",
      "tomli                             2.0.1\n",
      "tomlkit                           0.13.2\n",
      "toolz                             1.0.0\n",
      "torch                             2.9.0\n",
      "tornado                           6.5.1\n",
      "tqdm                              4.67.1\n",
      "traitlets                         5.14.3\n",
      "transformers                      4.57.1\n",
      "truststore                        0.10.0\n",
      "Twisted                           24.11.0\n",
      "twisted-iocpsupport               1.0.2\n",
      "typer                             0.9.0\n",
      "typing_extensions                 4.12.2\n",
      "tzdata                            2025.2\n",
      "uc-micro-py                       1.0.1\n",
      "ujson                             5.10.0\n",
      "Unidecode                         1.3.8\n",
      "urllib3                           2.3.0\n",
      "w3lib                             2.1.2\n",
      "watchdog                          4.0.2\n",
      "wcwidth                           0.2.5\n",
      "webencodings                      0.5.1\n",
      "websocket-client                  1.8.0\n",
      "Werkzeug                          3.1.3\n",
      "whatthepatch                      1.0.2\n",
      "wheel                             0.45.1\n",
      "widgetsnbextension                4.0.13\n",
      "win_inet_pton                     1.1.0\n",
      "wrapt                             1.17.0\n",
      "xarray                            2025.4.0\n",
      "xlwings                           0.32.1\n",
      "xyzservices                       2022.9.0\n",
      "yapf                              0.40.2\n",
      "yarl                              1.18.0\n",
      "zict                              3.0.0\n",
      "zipp                              3.21.0\n",
      "zope.interface                    7.1.1\n",
      "zstandard                         0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d94560-212a-45a7-b606-278ddb66db83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
